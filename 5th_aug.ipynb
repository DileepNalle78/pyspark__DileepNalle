{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/5th_aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Scala Env."
      ],
      "metadata": {
        "id": "c6Z58K9N1_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78Oxr-PY1pA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ddfa65-56c8-4abe-a692-2635870c409d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyJiL9W-2lXu",
        "outputId": "9185ea02-4bab-4d44-e111-500929ffe95c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Hello World in JAVA"
      ],
      "metadata": {
        "id": "P86TPUCo6-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CreateDataFrame.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Encoders;\n",
        "import java.util.Arrays;\n",
        "import java.util.List;\n",
        "\n",
        "public class CreateDataFrame {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"CreateDataFrameExample\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Sample data\n",
        "        List<String> data = Arrays.asList(\"Java\", \"Python\", \"Scala\");\n",
        "\n",
        "        // Create DataFrame from a list\n",
        "        Dataset<String> df = spark.createDataset(data, Encoders.STRING());\n",
        "\n",
        "        // Show the DataFrame content\n",
        "        df.show();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_aQP7Q28De",
        "outputId": "abbb34af-8252-4119-9ed3-3e43138c8a3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CreateDataFrame.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!javac -cp \"$spark_home/jars/*\" CreateDataFrame.java"
      ],
      "metadata": {
        "id": "sIaNIoC56kLN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!java -cp \"$spark_home/jars/*:.\" CreateDataFrame"
      ],
      "metadata": {
        "id": "iS5Z3wOblVCw",
        "outputId": "8aa53b72-1a64-4fc1-f24e-a4938e7bb49b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/05 09:35:11 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/05 09:35:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/05 09:35:12 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 09:35:12 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/05 09:35:12 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 09:35:12 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/05 09:35:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/05 09:35:12 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/05 09:35:12 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/05 09:35:12 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/05 09:35:12 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/05 09:35:12 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/05 09:35:12 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/05 09:35:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/05 09:35:13 INFO Utils: Successfully started service 'sparkDriver' on port 44979.\n",
            "25/08/05 09:35:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/05 09:35:13 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/05 09:35:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/05 09:35:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/05 09:35:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/05 09:35:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21d3f285-c0a6-4ba0-8aa4-7f1c6df90514\n",
            "25/08/05 09:35:13 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/05 09:35:13 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/05 09:35:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/05 09:35:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/05 09:35:14 INFO Executor: Starting executor ID driver on host 7de832a941f5\n",
            "25/08/05 09:35:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/05 09:35:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43557.\n",
            "25/08/05 09:35:14 INFO NettyBlockTransferService: Server created on 7de832a941f5:43557\n",
            "25/08/05 09:35:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/05 09:35:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7de832a941f5, 43557, None)\n",
            "25/08/05 09:35:14 INFO BlockManagerMasterEndpoint: Registering block manager 7de832a941f5:43557 with 1767.6 MiB RAM, BlockManagerId(driver, 7de832a941f5, 43557, None)\n",
            "25/08/05 09:35:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7de832a941f5, 43557, None)\n",
            "25/08/05 09:35:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7de832a941f5, 43557, None)\n",
            "25/08/05 09:35:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/05 09:35:18 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/05 09:35:21 INFO CodeGenerator: Code generated in 521.325717 ms\n",
            "25/08/05 09:35:24 INFO CodeGenerator: Code generated in 35.219969 ms\n",
            "25/08/05 09:35:24 INFO CodeGenerator: Code generated in 32.565873 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/05 09:35:24 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/05 09:35:24 INFO SparkUI: Stopped Spark web UI at http://7de832a941f5:4040\n",
            "25/08/05 09:35:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/05 09:35:24 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/05 09:35:24 INFO BlockManager: BlockManager stopped\n",
            "25/08/05 09:35:24 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/05 09:35:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/05 09:35:24 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/05 09:35:24 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/05 09:35:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea252da8-897d-4d62-a361-035ca8c6fc36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set varible\n",
        "\n",
        "!SPARK_HOME=/content/spark-3.3.2-bin-hadoop3\n",
        "!JARS=$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\n"
      ],
      "metadata": {
        "id": "M4ClcALbla3J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $SPARK_HOME\n",
        "!echo $JARS"
      ],
      "metadata": {
        "id": "7-PmEhD51yxA",
        "outputId": "99b6deae-2b11-4db3-9fd7-96b46466cfd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.4.1-bin-hadoop3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \"$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame.java"
      ],
      "metadata": {
        "id": "i9urYc1e1_CI",
        "outputId": "19c23fb2-3ba0-4522-b7a3-370511c97a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/05 10:48:55 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/05 10:48:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/05 10:48:56 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 10:48:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/05 10:48:56 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 10:48:56 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/05 10:48:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/05 10:48:56 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/05 10:48:56 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/05 10:48:56 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/05 10:48:56 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/05 10:48:56 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/05 10:48:56 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/05 10:48:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/05 10:48:57 INFO Utils: Successfully started service 'sparkDriver' on port 44601.\n",
            "25/08/05 10:48:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/05 10:48:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/05 10:48:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/05 10:48:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/05 10:48:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/05 10:48:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ad8be341-44d9-4de0-b330-14e9e9083219\n",
            "25/08/05 10:48:57 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/05 10:48:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/05 10:48:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/05 10:48:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/05 10:48:58 INFO Executor: Starting executor ID driver on host 7de832a941f5\n",
            "25/08/05 10:48:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/05 10:48:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43065.\n",
            "25/08/05 10:48:58 INFO NettyBlockTransferService: Server created on 7de832a941f5:43065\n",
            "25/08/05 10:48:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/05 10:48:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7de832a941f5, 43065, None)\n",
            "25/08/05 10:48:58 INFO BlockManagerMasterEndpoint: Registering block manager 7de832a941f5:43065 with 1767.6 MiB RAM, BlockManagerId(driver, 7de832a941f5, 43065, None)\n",
            "25/08/05 10:48:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7de832a941f5, 43065, None)\n",
            "25/08/05 10:48:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7de832a941f5, 43065, None)\n",
            "25/08/05 10:49:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/05 10:49:02 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/05 10:49:06 INFO CodeGenerator: Code generated in 861.473194 ms\n",
            "25/08/05 10:49:09 INFO CodeGenerator: Code generated in 48.843133 ms\n",
            "25/08/05 10:49:09 INFO CodeGenerator: Code generated in 60.575116 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/05 10:49:09 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/05 10:49:09 INFO SparkUI: Stopped Spark web UI at http://7de832a941f5:4040\n",
            "25/08/05 10:49:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/05 10:49:09 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/05 10:49:09 INFO BlockManager: BlockManager stopped\n",
            "25/08/05 10:49:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/05 10:49:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/05 10:49:09 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/05 10:49:09 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/05 10:49:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee51bc9c-6de0-4229-af22-1176079acb3e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame"
      ],
      "metadata": {
        "id": "y69uEWm92J3S",
        "outputId": "ac24b519-c2f1-461b-b197-cabcf70382a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/05 10:50:20 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/05 10:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/05 10:50:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 10:50:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/05 10:50:21 INFO ResourceUtils: ==============================================================\n",
            "25/08/05 10:50:21 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/05 10:50:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/05 10:50:21 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/05 10:50:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/05 10:50:21 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/05 10:50:21 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/05 10:50:21 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/05 10:50:21 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/05 10:50:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/05 10:50:22 INFO Utils: Successfully started service 'sparkDriver' on port 35635.\n",
            "25/08/05 10:50:23 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/05 10:50:23 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/05 10:50:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/05 10:50:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/05 10:50:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/05 10:50:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cf405abe-f813-4b8d-838f-3074c1f21815\n",
            "25/08/05 10:50:23 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/05 10:50:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/05 10:50:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/05 10:50:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/05 10:50:24 INFO Executor: Starting executor ID driver on host 7de832a941f5\n",
            "25/08/05 10:50:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/05 10:50:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41287.\n",
            "25/08/05 10:50:25 INFO NettyBlockTransferService: Server created on 7de832a941f5:41287\n",
            "25/08/05 10:50:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/05 10:50:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7de832a941f5, 41287, None)\n",
            "25/08/05 10:50:25 INFO BlockManagerMasterEndpoint: Registering block manager 7de832a941f5:41287 with 1767.6 MiB RAM, BlockManagerId(driver, 7de832a941f5, 41287, None)\n",
            "25/08/05 10:50:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7de832a941f5, 41287, None)\n",
            "25/08/05 10:50:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7de832a941f5, 41287, None)\n",
            "25/08/05 10:50:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/05 10:50:28 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/05 10:50:30 INFO CodeGenerator: Code generated in 571.840332 ms\n",
            "25/08/05 10:50:33 INFO CodeGenerator: Code generated in 36.631889 ms\n",
            "25/08/05 10:50:33 INFO CodeGenerator: Code generated in 46.089842 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/05 10:50:33 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/05 10:50:33 INFO SparkUI: Stopped Spark web UI at http://7de832a941f5:4040\n",
            "25/08/05 10:50:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/05 10:50:33 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/05 10:50:33 INFO BlockManager: BlockManager stopped\n",
            "25/08/05 10:50:33 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/05 10:50:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/05 10:50:33 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/05 10:50:33 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/05 10:50:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5d2e565-eb1e-4c9e-ac24-54b8a67c8500\n"
          ]
        }
      ]
    }
  ]
}