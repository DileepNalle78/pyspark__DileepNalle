{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/adv_scala.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!wget -q https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!dpkg -i scala-2.12.18.deb\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "!curl -Lo coursier https://git.io/coursier-cli && chmod +x coursier\n",
        "!./coursier launch --fork almond --scala 2.12.10 -- --install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPbKNUUB0QZQ",
        "outputId": "009e336e-ac22-4911-f88b-3467ee2b389e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... 130340 files and directories currently installed.)\n",
            "Preparing to unpack scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) over (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 42577  100 42577    0     0  99924      0 --:--:-- --:--:-- --:--:-- 99924\n",
            "Error: /root/.local/share/jupyter/kernels/scala already exists, pass --force to force erasing it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/lib/jvm/java-11-openjdk-amd64\n",
        "!ls /content/spark-3.4.1-bin-hadoop3\n",
        "!ls /content/scala-2.12.18.deb\n",
        "!echo $JAVA_HOME\n",
        "!echo $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "821ogDBB7oD3",
        "outputId": "34bb9d91-3785-4b0c-bdd9-bfb37ecb1c58"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  conf  docs  include  jmods  legal\tlib  man  release\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n",
            "/content/scala-2.12.18.deb\n",
            "/usr/lib/jvm/java-11-openjdk-amd64\n",
            "/content/spark-3.4.1-bin-hadoop3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Colab Scala Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nB2cFWTE8IGc",
        "outputId": "1ecc4d0b-fcf9-4535-a070-9df955f9892e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ae5c11e7650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://99cb23bccadf:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab Scala Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile HelloWorld_hi.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "\n",
        "object HelloWorld_hi {\n",
        "  def main(args: Array[String]): Unit = {\n",
        "    val spark = SparkSession.builder()\n",
        "      .appName(\"ScalaAdvancedTransformations\")\n",
        "      .master(\"local[*]\")\n",
        "      .config(\"spark.driver.memory\", \"1g\")\n",
        "      .getOrCreate()\n",
        "\n",
        "    println(\"Spark session created successfully.\")\n",
        "    spark.stop()\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXlGFigZ8SV3",
        "outputId": "d33729b5-fa94-432b-e0ec-df74c8a093c0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting HelloWorld_hi.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac HelloWorld_hi.scala"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJTWrwoBjWc",
        "outputId": "90cab015-2fad-4669-f169-7dbc049dc821"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HelloWorld_hi.scala:1: error: object apache is not a member of package org\n",
            "import org.apache.spark.sql.SparkSession\n",
            "           ^\n",
            "HelloWorld_hi.scala:5: error: not found: value SparkSession\n",
            "    val spark = SparkSession.builder()\n",
            "                ^\n",
            "two errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scala HelloWorld_hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QQsuniSDZOj",
        "outputId": "d7877d1e-bf82-4b88-901a-ce42a0cd8cf7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No such file or class on classpath: HelloWorld_hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld_hi.scala"
      ],
      "metadata": {
        "id": "O1dFseiL8YPW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld_hi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PECjUzYi8hXa",
        "outputId": "20f62de0-f48d-474b-c10a-e51d7912101f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 06:39:30 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 06:39:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 06:39:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:39:31 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 06:39:31 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:39:31 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 06:39:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 06:39:31 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 06:39:31 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 06:39:31 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 06:39:31 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 06:39:31 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 06:39:31 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 06:39:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 06:39:32 INFO Utils: Successfully started service 'sparkDriver' on port 40367.\n",
            "25/08/01 06:39:32 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 06:39:32 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 06:39:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 06:39:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 06:39:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 06:39:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c797ba35-5388-4f8f-ac4d-65b81028b8ef\n",
            "25/08/01 06:39:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 06:39:32 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 06:39:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 06:39:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 06:39:33 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 06:39:33 INFO Executor: Starting executor ID driver on host 99cb23bccadf\n",
            "25/08/01 06:39:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 06:39:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36217.\n",
            "25/08/01 06:39:33 INFO NettyBlockTransferService: Server created on 99cb23bccadf:36217\n",
            "25/08/01 06:39:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 06:39:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 99cb23bccadf, 36217, None)\n",
            "25/08/01 06:39:34 INFO BlockManagerMasterEndpoint: Registering block manager 99cb23bccadf:36217 with 434.4 MiB RAM, BlockManagerId(driver, 99cb23bccadf, 36217, None)\n",
            "25/08/01 06:39:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 99cb23bccadf, 36217, None)\n",
            "25/08/01 06:39:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 99cb23bccadf, 36217, None)\n",
            "Spark session created successfully.\n",
            "25/08/01 06:39:35 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 06:39:35 INFO SparkUI: Stopped Spark web UI at http://99cb23bccadf:4041\n",
            "25/08/01 06:39:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 06:39:35 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 06:39:35 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 06:39:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 06:39:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 06:39:35 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 06:39:35 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 06:39:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-47f95ff4-2a61-43ca-9a36-84426f9e4abc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cccf48ae",
        "outputId": "193f727d-5831-45d7-cfe9-1982a20f0ac3"
      },
      "source": [
        "%%writefile HelloWorld.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "object HelloWorld {\n",
        "    def main(args: Array[String]): Unit = {\n",
        "        val spark = SparkSession.builder()\n",
        "          .appName(\"ScalaAdvancedTransformations\")\n",
        "          .master(\"local[*]\")\n",
        "          .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "          .getOrCreate()\n",
        "        val sc = spark.sparkContext\n",
        "\n",
        "        ///////////////////////////////////////////create an accumulator variable\n",
        "        val errorCount = sc.longAccumulator(\"Error Count\")\n",
        "\n",
        "        // Sample rdd with some log line\n",
        "        val data = sc.parallelize(Seq(\"INFO Start\", \"ERROR Failure1\", \"ERROR Failure2\", \"INFO End\"))\n",
        "\n",
        "        //Use the accumulator\n",
        "        data.foreach(line => {\n",
        "          if (line.contains(\"ERROR\")) {\n",
        "            errorCount.add(1)\n",
        "          }\n",
        "        })\n",
        "\n",
        "        println(\"Total errors: \" + errorCount.value)\n",
        "       spark.stop()\n",
        "    }\n",
        "}"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting HelloWorld.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld.scala"
      ],
      "metadata": {
        "id": "v9v60RxoGhHp"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" HelloWorld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QTlxZmGOWpy",
        "outputId": "35a5e912-d94d-42bb-d41d-47fa2c486ee1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 06:37:07 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 06:37:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 06:37:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:37:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 06:37:07 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:37:07 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 06:37:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 06:37:07 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 06:37:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 06:37:07 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 06:37:07 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 06:37:07 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 06:37:07 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 06:37:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 06:37:08 INFO Utils: Successfully started service 'sparkDriver' on port 36417.\n",
            "25/08/01 06:37:08 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 06:37:08 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 06:37:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 06:37:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 06:37:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 06:37:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9bfb0147-e24b-46b9-8c96-8a5cc0d16c58\n",
            "25/08/01 06:37:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 06:37:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 06:37:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 06:37:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 06:37:08 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 06:37:08 INFO Executor: Starting executor ID driver on host 99cb23bccadf\n",
            "25/08/01 06:37:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 06:37:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42953.\n",
            "25/08/01 06:37:08 INFO NettyBlockTransferService: Server created on 99cb23bccadf:42953\n",
            "25/08/01 06:37:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 06:37:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 99cb23bccadf, 42953, None)\n",
            "25/08/01 06:37:08 INFO BlockManagerMasterEndpoint: Registering block manager 99cb23bccadf:42953 with 434.4 MiB RAM, BlockManagerId(driver, 99cb23bccadf, 42953, None)\n",
            "25/08/01 06:37:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 99cb23bccadf, 42953, None)\n",
            "25/08/01 06:37:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 99cb23bccadf, 42953, None)\n",
            "25/08/01 06:37:09 INFO SparkContext: Starting job: foreach at HelloWorld.scala:18\n",
            "25/08/01 06:37:09 INFO DAGScheduler: Got job 0 (foreach at HelloWorld.scala:18) with 2 output partitions\n",
            "25/08/01 06:37:09 INFO DAGScheduler: Final stage: ResultStage 0 (foreach at HelloWorld.scala:18)\n",
            "25/08/01 06:37:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 06:37:09 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 06:37:09 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloWorld.scala:15), which has no missing parents\n",
            "25/08/01 06:37:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.5 KiB, free 434.4 MiB)\n",
            "25/08/01 06:37:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.0 KiB, free 434.4 MiB)\n",
            "25/08/01 06:37:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 99cb23bccadf:42953 (size: 2.0 KiB, free: 434.4 MiB)\n",
            "25/08/01 06:37:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 06:37:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at HelloWorld.scala:15) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 06:37:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "25/08/01 06:37:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7389 bytes) \n",
            "25/08/01 06:37:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (99cb23bccadf, executor driver, partition 1, PROCESS_LOCAL, 7387 bytes) \n",
            "25/08/01 06:37:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "25/08/01 06:37:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 06:37:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1004 bytes result sent to driver\n",
            "25/08/01 06:37:10 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1004 bytes result sent to driver\n",
            "25/08/01 06:37:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 442 ms on 99cb23bccadf (executor driver) (1/2)\n",
            "25/08/01 06:37:10 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 250 ms on 99cb23bccadf (executor driver) (2/2)\n",
            "25/08/01 06:37:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 06:37:10 INFO DAGScheduler: ResultStage 0 (foreach at HelloWorld.scala:18) finished in 0.924 s\n",
            "25/08/01 06:37:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 06:37:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 06:37:10 INFO DAGScheduler: Job 0 finished: foreach at HelloWorld.scala:18, took 1.001102 s\n",
            "Total errors: 2\n",
            "25/08/01 06:37:10 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 06:37:10 INFO SparkUI: Stopped Spark web UI at http://99cb23bccadf:4041\n",
            "25/08/01 06:37:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 06:37:10 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 06:37:10 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 06:37:10 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 06:37:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 06:37:11 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 06:37:11 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 06:37:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1b0d774-0983-431f-a8bc-07f42957e6dd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile broadcast.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "object broadcast {\n",
        "    def main(args: Array[String]): Unit = {\n",
        "        val spark = SparkSession.builder()\n",
        "          .appName(\"ScalaAdvancedTransformations\")\n",
        "          .master(\"local[*]\")\n",
        "          .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "          .getOrCreate()\n",
        "          val sc = spark.sparkContext\n",
        "          // Create a lookup map to broadcast\n",
        "          val countryCodes = Map(\"USA\" -> \"US\", \"India\" -> \"IN\", \"China\" -> \"CN\")\n",
        "          // Create a broadcast variable\n",
        "          val countryCodesBroadcast = sc.broadcast(countryCodes)\n",
        "          // Sample RDD with country names\n",
        "          val countries = sc.parallelize(Seq(\"USA\", \"India\", \"China\", \"Japan\"))\n",
        "          // Map using the broadcast variable\n",
        "          val countryCodesMapped = countries.map(country => (country, countryCodesBroadcast.value.getOrElse(country, \"Unknown\")))\n",
        "          // Collect and print the result\n",
        "          countryCodesMapped.collect().foreach(println)\n",
        "       spark.stop()\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l88OBdx4WTRq",
        "outputId": "8fc58d5d-3825-471a-a23f-d66e607c39d8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing broadcast.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" broadcast.scala"
      ],
      "metadata": {
        "id": "cOCNC4gnZzfD"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" broadcast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS_4NlK4Zz2P",
        "outputId": "18734496-a131-4c30-b8df-f4e273a93a77"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 06:54:15 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 06:54:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 06:54:15 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:54:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 06:54:15 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 06:54:15 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 06:54:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 06:54:15 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 06:54:15 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 06:54:15 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 06:54:15 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 06:54:15 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 06:54:15 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 06:54:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 06:54:16 INFO Utils: Successfully started service 'sparkDriver' on port 38301.\n",
            "25/08/01 06:54:16 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 06:54:16 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 06:54:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 06:54:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 06:54:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 06:54:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-87851f22-5535-4b65-9a33-65c874df2b58\n",
            "25/08/01 06:54:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 06:54:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 06:54:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 06:54:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 06:54:17 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 06:54:17 INFO Executor: Starting executor ID driver on host 99cb23bccadf\n",
            "25/08/01 06:54:17 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 06:54:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45953.\n",
            "25/08/01 06:54:17 INFO NettyBlockTransferService: Server created on 99cb23bccadf:45953\n",
            "25/08/01 06:54:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 06:54:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 99cb23bccadf, 45953, None)\n",
            "25/08/01 06:54:17 INFO BlockManagerMasterEndpoint: Registering block manager 99cb23bccadf:45953 with 434.4 MiB RAM, BlockManagerId(driver, 99cb23bccadf, 45953, None)\n",
            "25/08/01 06:54:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 99cb23bccadf, 45953, None)\n",
            "25/08/01 06:54:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 99cb23bccadf, 45953, None)\n",
            "25/08/01 06:54:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 352.0 B, free 434.4 MiB)\n",
            "25/08/01 06:54:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 201.0 B, free 434.4 MiB)\n",
            "25/08/01 06:54:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 99cb23bccadf:45953 (size: 201.0 B, free: 434.4 MiB)\n",
            "25/08/01 06:54:18 INFO SparkContext: Created broadcast 0 from broadcast at broadcast.scala:13\n",
            "25/08/01 06:54:18 INFO SparkContext: Starting job: collect at broadcast.scala:19\n",
            "25/08/01 06:54:18 INFO DAGScheduler: Got job 0 (collect at broadcast.scala:19) with 2 output partitions\n",
            "25/08/01 06:54:18 INFO DAGScheduler: Final stage: ResultStage 0 (collect at broadcast.scala:19)\n",
            "25/08/01 06:54:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 06:54:18 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 06:54:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at broadcast.scala:17), which has no missing parents\n",
            "25/08/01 06:54:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.3 KiB, free 434.4 MiB)\n",
            "25/08/01 06:54:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 434.4 MiB)\n",
            "25/08/01 06:54:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 99cb23bccadf:45953 (size: 2.5 KiB, free: 434.4 MiB)\n",
            "25/08/01 06:54:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 06:54:19 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at broadcast.scala:17) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 06:54:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "25/08/01 06:54:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7373 bytes) \n",
            "25/08/01 06:54:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (99cb23bccadf, executor driver, partition 1, PROCESS_LOCAL, 7375 bytes) \n",
            "25/08/01 06:54:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "25/08/01 06:54:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 06:54:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 983 bytes result sent to driver\n",
            "25/08/01 06:54:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 990 bytes result sent to driver\n",
            "25/08/01 06:54:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 402 ms on 99cb23bccadf (executor driver) (1/2)\n",
            "25/08/01 06:54:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 269 ms on 99cb23bccadf (executor driver) (2/2)\n",
            "25/08/01 06:54:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 06:54:19 INFO DAGScheduler: ResultStage 0 (collect at broadcast.scala:19) finished in 0.629 s\n",
            "25/08/01 06:54:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 06:54:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 06:54:19 INFO DAGScheduler: Job 0 finished: collect at broadcast.scala:19, took 0.750929 s\n",
            "(USA,US)\n",
            "(India,IN)\n",
            "(China,CN)\n",
            "(Japan,Unknown)\n",
            "25/08/01 06:54:19 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 06:54:19 INFO SparkUI: Stopped Spark web UI at http://99cb23bccadf:4041\n",
            "25/08/01 06:54:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 06:54:19 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 06:54:19 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 06:54:19 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 06:54:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 06:54:19 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 06:54:19 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 06:54:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b0ba9be-858b-4c2b-a494-ede269f890db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "object data {\n",
        "    def main(args: Array[String]): Unit = {\n",
        "        val spark = SparkSession.builder()\n",
        "          .appName(\"ScalaAdvancedTransformations\")\n",
        "          .master(\"local[*]\")\n",
        "          .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "          .getOrCreate()\n",
        "        val sc = spark.sparkContext\n",
        "        val data = sc.parallelize(1 to 1000000) // Removed underscore\n",
        "\n",
        "        val evenNumbers = data.filter(x => x % 2 == 0).cache() // Fixed syntax\n",
        "\n",
        "        println(\"count: \" + evenNumbers.count())\n",
        "\n",
        "        println(\"sum: \" + evenNumbers.sum())\n",
        "        spark.stop()\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4U0MAktaStN",
        "outputId": "bf7f3c1b-6eb4-4629-f1bc-3804d734c2b9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" data.scala"
      ],
      "metadata": {
        "id": "r2WQdMHube4W"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe9RDbL9bfBE",
        "outputId": "278b100c-87cd-46b4-fbd1-a42c486b9d45"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 07:01:42 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 07:01:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 07:01:42 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:01:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 07:01:42 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:01:42 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 07:01:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 07:01:42 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 07:01:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 07:01:42 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 07:01:42 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 07:01:42 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 07:01:42 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 07:01:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 07:01:43 INFO Utils: Successfully started service 'sparkDriver' on port 37543.\n",
            "25/08/01 07:01:43 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 07:01:43 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 07:01:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 07:01:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 07:01:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 07:01:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f85fd369-e1ad-4c28-a2d6-3e03499c0f27\n",
            "25/08/01 07:01:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 07:01:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 07:01:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 07:01:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 07:01:43 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 07:01:44 INFO Executor: Starting executor ID driver on host 99cb23bccadf\n",
            "25/08/01 07:01:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 07:01:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34839.\n",
            "25/08/01 07:01:44 INFO NettyBlockTransferService: Server created on 99cb23bccadf:34839\n",
            "25/08/01 07:01:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 07:01:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 99cb23bccadf, 34839, None)\n",
            "25/08/01 07:01:44 INFO BlockManagerMasterEndpoint: Registering block manager 99cb23bccadf:34839 with 434.4 MiB RAM, BlockManagerId(driver, 99cb23bccadf, 34839, None)\n",
            "25/08/01 07:01:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 99cb23bccadf, 34839, None)\n",
            "25/08/01 07:01:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 99cb23bccadf, 34839, None)\n",
            "25/08/01 07:01:45 INFO SparkContext: Starting job: count at data.scala:14\n",
            "25/08/01 07:01:45 INFO DAGScheduler: Got job 0 (count at data.scala:14) with 2 output partitions\n",
            "25/08/01 07:01:45 INFO DAGScheduler: Final stage: ResultStage 0 (count at data.scala:14)\n",
            "25/08/01 07:01:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:01:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:01:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at filter at data.scala:12), which has no missing parents\n",
            "25/08/01 07:01:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.7 KiB, free 434.4 MiB)\n",
            "25/08/01 07:01:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 434.4 MiB)\n",
            "25/08/01 07:01:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 99cb23bccadf:34839 (size: 2.2 KiB, free: 434.4 MiB)\n",
            "25/08/01 07:01:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:01:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at filter at data.scala:12) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 07:01:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
            "25/08/01 07:01:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:01:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (99cb23bccadf, executor driver, partition 1, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:01:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 07:01:46 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
            "25/08/01 07:01:46 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 976.6 KiB, free 433.4 MiB)\n",
            "25/08/01 07:01:46 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 976.6 KiB, free 432.5 MiB)\n",
            "25/08/01 07:01:46 INFO BlockManagerInfo: Added rdd_1_0 in memory on 99cb23bccadf:34839 (size: 976.6 KiB, free: 433.4 MiB)\n",
            "25/08/01 07:01:46 INFO BlockManagerInfo: Added rdd_1_1 in memory on 99cb23bccadf:34839 (size: 976.6 KiB, free: 432.5 MiB)\n",
            "25/08/01 07:01:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 930 bytes result sent to driver\n",
            "25/08/01 07:01:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 973 bytes result sent to driver\n",
            "25/08/01 07:01:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 623 ms on 99cb23bccadf (executor driver) (1/2)\n",
            "25/08/01 07:01:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 562 ms on 99cb23bccadf (executor driver) (2/2)\n",
            "25/08/01 07:01:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:01:46 INFO DAGScheduler: ResultStage 0 (count at data.scala:14) finished in 1.005 s\n",
            "25/08/01 07:01:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:01:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 07:01:46 INFO DAGScheduler: Job 0 finished: count at data.scala:14, took 1.121371 s\n",
            "count: 500000\n",
            "25/08/01 07:01:47 INFO SparkContext: Starting job: sum at data.scala:16\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Got job 1 (sum at data.scala:16) with 2 output partitions\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Final stage: ResultStage 1 (sum at data.scala:16)\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[2] at numericRDDToDoubleRDDFunctions at data.scala:16), which has no missing parents\n",
            "25/08/01 07:01:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.6 KiB, free 432.5 MiB)\n",
            "25/08/01 07:01:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 432.5 MiB)\n",
            "25/08/01 07:01:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 99cb23bccadf:34839 (size: 2.6 KiB, free: 432.5 MiB)\n",
            "25/08/01 07:01:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at numericRDDToDoubleRDDFunctions at data.scala:16) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 07:01:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "25/08/01 07:01:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:01:47 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (99cb23bccadf, executor driver, partition 1, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:01:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
            "25/08/01 07:01:47 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
            "25/08/01 07:01:47 INFO BlockManager: Found block rdd_1_1 locally\n",
            "25/08/01 07:01:47 INFO BlockManager: Found block rdd_1_0 locally\n",
            "25/08/01 07:01:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1018 bytes result sent to driver\n",
            "25/08/01 07:01:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1018 bytes result sent to driver\n",
            "25/08/01 07:01:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 111 ms on 99cb23bccadf (executor driver) (1/2)\n",
            "25/08/01 07:01:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 113 ms on 99cb23bccadf (executor driver) (2/2)\n",
            "25/08/01 07:01:47 INFO DAGScheduler: ResultStage 1 (sum at data.scala:16) finished in 0.138 s\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:01:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:01:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/01 07:01:47 INFO DAGScheduler: Job 1 finished: sum at data.scala:16, took 0.158353 s\n",
            "sum: 2.500005E11\n",
            "25/08/01 07:01:47 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 07:01:47 INFO SparkUI: Stopped Spark web UI at http://99cb23bccadf:4041\n",
            "25/08/01 07:01:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 07:01:47 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 07:01:47 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 07:01:47 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 07:01:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 07:01:47 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 07:01:47 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 07:01:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca646e9a-2ae2-4468-850a-91b1e580b767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile memory.scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.storage.StorageLevel // Import StorageLevel\n",
        "\n",
        "object memory {\n",
        "    def main(args: Array[String]): Unit = {\n",
        "        val spark = SparkSession.builder()\n",
        "          .appName(\"ScalaAdvancedTransformations\")\n",
        "          .master(\"local[*]\")\n",
        "          .config(\"spark.driver.memory\", \"1g\")  // Increase memory to 1 GB\n",
        "          .getOrCreate()\n",
        "        val sc = spark.sparkContext\n",
        "\n",
        "        val bigData = sc.parallelize(1 to 1000000)\n",
        "\n",
        "        val squares = bigData.map(x => x * x).persist(StorageLevel.MEMORY_AND_DISK) // Corrected StorageLevel\n",
        "\n",
        "        println(\"First 5: \" + squares.take(5).mkString(\", \"))\n",
        "\n",
        "        println(\"Count: \" + squares.count())\n",
        "\n",
        "\n",
        "        spark.stop()\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQNEBIXncDtH",
        "outputId": "8ca2a07f-c1ac-4611-aa00-81b7a46909e4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting memory.scala\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scalac -classpath \"$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" memory.scala"
      ],
      "metadata": {
        "id": "GXteLkUfcEQ8"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scala -J-Xmx1g -classpath \".:$(echo /content/spark-3.4.1-bin-hadoop3/jars/*.jar | tr ' ' ':')\" memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6NAoSUpcEXb",
        "outputId": "dd1c4fd1-fd28-4631-f1fd-44c53c362bdb"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/01 07:10:57 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/01 07:10:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/01 07:10:58 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:10:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/01 07:10:58 INFO ResourceUtils: ==============================================================\n",
            "25/08/01 07:10:58 INFO SparkContext: Submitted application: ScalaAdvancedTransformations\n",
            "25/08/01 07:10:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/01 07:10:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/01 07:10:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/01 07:10:58 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/01 07:10:58 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/01 07:10:58 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/01 07:10:58 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/01 07:10:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/01 07:10:59 INFO Utils: Successfully started service 'sparkDriver' on port 34991.\n",
            "25/08/01 07:10:59 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/01 07:10:59 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/01 07:10:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/01 07:10:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/01 07:10:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/01 07:10:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dacbceb9-5c60-4df9-9c34-1332cd8e5b11\n",
            "25/08/01 07:10:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/01 07:10:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/01 07:11:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/01 07:11:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/01 07:11:00 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/01 07:11:00 INFO Executor: Starting executor ID driver on host 99cb23bccadf\n",
            "25/08/01 07:11:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/01 07:11:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33025.\n",
            "25/08/01 07:11:00 INFO NettyBlockTransferService: Server created on 99cb23bccadf:33025\n",
            "25/08/01 07:11:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/01 07:11:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 99cb23bccadf, 33025, None)\n",
            "25/08/01 07:11:01 INFO BlockManagerMasterEndpoint: Registering block manager 99cb23bccadf:33025 with 434.4 MiB RAM, BlockManagerId(driver, 99cb23bccadf, 33025, None)\n",
            "25/08/01 07:11:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 99cb23bccadf, 33025, None)\n",
            "25/08/01 07:11:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 99cb23bccadf, 33025, None)\n",
            "25/08/01 07:11:02 INFO SparkContext: Starting job: take at memory.scala:17\n",
            "25/08/01 07:11:02 INFO DAGScheduler: Got job 0 (take at memory.scala:17) with 1 output partitions\n",
            "25/08/01 07:11:02 INFO DAGScheduler: Final stage: ResultStage 0 (take at memory.scala:17)\n",
            "25/08/01 07:11:02 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:11:02 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:11:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at memory.scala:15), which has no missing parents\n",
            "25/08/01 07:11:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.8 KiB, free 434.4 MiB)\n",
            "25/08/01 07:11:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 434.4 MiB)\n",
            "25/08/01 07:11:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 99cb23bccadf:33025 (size: 2.2 KiB, free: 434.4 MiB)\n",
            "25/08/01 07:11:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at memory.scala:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/01 07:11:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/01 07:11:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/01 07:11:03 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 1953.1 KiB, free 432.5 MiB)\n",
            "25/08/01 07:11:03 INFO BlockManagerInfo: Added rdd_1_0 in memory on 99cb23bccadf:33025 (size: 1953.1 KiB, free: 432.5 MiB)\n",
            "25/08/01 07:11:03 INFO Executor: 1 block locks were not released by task 0.0 in stage 0.0 (TID 0)\n",
            "[rdd_1_0]\n",
            "25/08/01 07:11:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 938 bytes result sent to driver\n",
            "25/08/01 07:11:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 640 ms on 99cb23bccadf (executor driver) (1/1)\n",
            "25/08/01 07:11:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:11:03 INFO DAGScheduler: ResultStage 0 (take at memory.scala:17) finished in 1.069 s\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:11:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Job 0 finished: take at memory.scala:17, took 1.145995 s\n",
            "First 5: 1, 4, 9, 16, 25\n",
            "25/08/01 07:11:03 INFO SparkContext: Starting job: count at memory.scala:19\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Got job 1 (count at memory.scala:19) with 2 output partitions\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Final stage: ResultStage 1 (count at memory.scala:19)\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[1] at map at memory.scala:15), which has no missing parents\n",
            "25/08/01 07:11:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.7 KiB, free 432.5 MiB)\n",
            "25/08/01 07:11:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KiB, free 432.5 MiB)\n",
            "25/08/01 07:11:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 99cb23bccadf:33025 (size: 2.2 KiB, free: 432.5 MiB)\n",
            "25/08/01 07:11:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
            "25/08/01 07:11:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[1] at map at memory.scala:15) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/01 07:11:03 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "25/08/01 07:11:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (99cb23bccadf, executor driver, partition 0, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:04 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (99cb23bccadf, executor driver, partition 1, PROCESS_LOCAL, 7488 bytes) \n",
            "25/08/01 07:11:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/01 07:11:04 INFO BlockManager: Found block rdd_1_0 locally\n",
            "25/08/01 07:11:04 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "25/08/01 07:11:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1016 bytes result sent to driver\n",
            "25/08/01 07:11:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 287 ms on 99cb23bccadf (executor driver) (1/2)\n",
            "25/08/01 07:11:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 99cb23bccadf:33025 in memory (size: 2.2 KiB, free: 432.5 MiB)\n",
            "25/08/01 07:11:04 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 1953.1 KiB, free 430.6 MiB)\n",
            "25/08/01 07:11:04 INFO BlockManagerInfo: Added rdd_1_1 in memory on 99cb23bccadf:33025 (size: 1953.1 KiB, free: 430.6 MiB)\n",
            "25/08/01 07:11:04 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 930 bytes result sent to driver\n",
            "25/08/01 07:11:04 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 913 ms on 99cb23bccadf (executor driver) (2/2)\n",
            "25/08/01 07:11:04 INFO DAGScheduler: ResultStage 1 (count at memory.scala:19) finished in 0.941 s\n",
            "25/08/01 07:11:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/01 07:11:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/01 07:11:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/01 07:11:04 INFO DAGScheduler: Job 1 finished: count at memory.scala:19, took 0.964790 s\n",
            "Count: 1000000\n",
            "25/08/01 07:11:04 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/01 07:11:04 INFO SparkUI: Stopped Spark web UI at http://99cb23bccadf:4041\n",
            "25/08/01 07:11:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/01 07:11:05 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/01 07:11:05 INFO BlockManager: BlockManager stopped\n",
            "25/08/01 07:11:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/01 07:11:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/01 07:11:05 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/01 07:11:05 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/01 07:11:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a54d2b5-0237-414b-bafa-9dc768bc7ef8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-T4aB9_eJlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xz_850r6eKE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dk9UH_KSeKZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}