{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6l06o0P0q_sy",
        "Pdks7cnOtJ0N",
        "G3HGEg7nN5CN",
        "pJHJcDwioVYb"
      ],
      "authorship_tag": "ABX9TyPFnQCWwkSenLyg57OJCuI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/sales.csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINUX BASIC\n",
        "\n"
      ],
      "metadata": {
        "id": "6l06o0P0q_sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tuaBrsBpsJu",
        "outputId": "7dadcd60-fcf5-4aeb-b4e7-6a0ca973685b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgDvLNuXqFAf",
        "outputId": "04a08de4-55d2-4854-db0b-d51314718e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 6d3647f0c437 6.1.123+ #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frGDZ2CAqP11",
        "outputId": "939fbe66-dba8-45fe-925e-5ba4d99b3ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3wwzlOUqgLT",
        "outputId": "ce870f83-16e8-412e-e007-2fd243ec2e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Basics"
      ],
      "metadata": {
        "id": "Pdks7cnOtJ0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHmBGEZqjHB",
        "outputId": "4add7874-0fd3-47ea-c7f9-b08f0897760e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2k8unOuFqE",
        "outputId": "b37bf375-cb4d-4d53-b6fa-21e9b3cda924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "KcvOfQ2yuJw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
      ],
      "metadata": {
        "id": "kU96jmpouaog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create DataFrame\n",
        "data = [(\"HEllo\",\"world\")]\n",
        "columns = [\"world1\",\"world2\"]\n",
        "\n",
        "df = spark.createDataFrame"
      ],
      "metadata": {
        "id": "kRQKHS3OvGMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark"
      ],
      "metadata": {
        "id": "_C91JB-JwEbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data frame"
      ],
      "metadata": {
        "id": "TQ8A_cqzw0ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Name\",\"Department\",\"Salary\"]\n",
        "data = [\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Jane\", \"Finance\", 4000),\n",
        "    (\"Mike\", \"Sales\", 3500),\n",
        "    (\"Alice\", \"Finance\", 3800),\n",
        "    (\"Bob\", \"IT\", 4500)\n",
        "]\n",
        "\n",
        "df=spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDVItNmMwG7p",
        "outputId": "8965c74c-a356-48c1-b3a9-53bb8931128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter: employees with salary > 3500\n",
        "df_filtered = df.filter(df.Salary > 3500)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2qJQet_xtL2",
        "outputId": "db0a44ee-0834-4ba6-a744-6877e331d78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| Jane|   Finance|  4000|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupBy(\"Department\").avg(\"salary\")\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4u6sPCiyDdF",
        "outputId": "fd802356-95e1-44d6-a2ba-4e331fa39f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new comumn: salary with new bones (10%)\n",
        "from pyspark.sql.functions import  col\n",
        "exp=col(\"Salary\") * 1.1\n",
        "df_with_bonus = df.withColumn(\"Salary_10%_Bonus\",exp)\n",
        "df_with_bonus.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL8hFjFOzpEq",
        "outputId": "dce3c884-6352-4eb0-90e2-ff86b1654525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+------------------+\n",
            "| Name|Department|Salary|  Salary_10%_Bonus|\n",
            "+-----+----------+------+------------------+\n",
            "| John|     Sales|  3000|3300.0000000000005|\n",
            "| Jane|   Finance|  4000|            4400.0|\n",
            "| Mike|     Sales|  3500|3850.0000000000005|\n",
            "|Alice|   Finance|  3800|            4180.0|\n",
            "|  Bob|        IT|  4500|            4950.0|\n",
            "+-----+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,upper, lower, concat_ws,length,when\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnFuUJPV1I07",
        "outputId": "26bb23cb-11a5-4866-a190-ff25c8c2f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_upper = df.withColumn(\"Name_upper\",upper(col(\"Name\")))\n",
        "df_lower = df.withColumn(\"Name_lower\", lower(col(\"Name\")))\n",
        "df_upper.show()\n",
        "df_lower.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbRaESgY2Uzk",
        "outputId": "d4d8aa80-c1e9-440d-fe5a-7174e73c2b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_upper|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      JOHN|\n",
            "| Jane|   Finance|  4000|      JANE|\n",
            "| Mike|     Sales|  3500|      MIKE|\n",
            "|Alice|   Finance|  3800|     ALICE|\n",
            "|  Bob|        IT|  4500|       BOB|\n",
            "+-----+----------+------+----------+\n",
            "\n",
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_lower|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      john|\n",
            "| Jane|   Finance|  4000|      jane|\n",
            "| Mike|     Sales|  3500|      mike|\n",
            "|Alice|   Finance|  3800|     alice|\n",
            "|  Bob|        IT|  4500|       bob|\n",
            "+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_concat = df.withColumn(\"Full_length_Name\",concat_ws(\" \",col(\"Name\"),col(\"Department\")))\n",
        "df_concat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQeJZZeT3Q0S",
        "outputId": "f7276e27-dca8-4a71-b026-0198df27ec03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+----------------+\n",
            "| Name|Department|Salary|Full_length_Name|\n",
            "+-----+----------+------+----------------+\n",
            "| John|     Sales|  3000|      John Sales|\n",
            "| Jane|   Finance|  4000|    Jane Finance|\n",
            "| Mike|     Sales|  3500|      Mike Sales|\n",
            "|Alice|   Finance|  3800|   Alice Finance|\n",
            "|  Bob|        IT|  4500|          Bob IT|\n",
            "+-----+----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# String leangth of Names in New DF\n",
        "df_length = df.withColumn(\"Name_length\",length(col(\"Name\")))\n",
        "df_length.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Idva0z_3XOK",
        "outputId": "b429794e-7e7f-4abc-e6c2-485c9875d73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+\n",
            "| Name|Department|Salary|Name_length|\n",
            "+-----+----------+------+-----------+\n",
            "| John|     Sales|  3000|          4|\n",
            "| Jane|   Finance|  4000|          4|\n",
            "| Mike|     Sales|  3500|          4|\n",
            "|Alice|   Finance|  3800|          5|\n",
            "|  Bob|        IT|  4500|          3|\n",
            "+-----+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conditional column (salary Category)\n",
        "df_conditional = df.withColumn(\n",
        "    \"Salary_Category\",\n",
        "    when(col(\"Salary\") >= 4000, \"High\")\n",
        "    .when(col(\"Salary\") >= 3500, \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "df_conditional.show()\n"
      ],
      "metadata": {
        "id": "g3SiP6aY33ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the column (Salary --> Base_Salary)\n",
        "df_renamed = df.withColumnRenamed(\"Salary\",\"Base_salary\")\n",
        "df_renamed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eggHzJgk5Amy",
        "outputId": "eb6cc9c1-97f9-42a2-a151-a2bc16630a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----------+\n",
            "| Name|Department|Base_salary|\n",
            "+-----+----------+-----------+\n",
            "| John|     Sales|       3000|\n",
            "| Jane|   Finance|       4000|\n",
            "| Mike|     Sales|       3500|\n",
            "|Alice|   Finance|       3800|\n",
            "|  Bob|        IT|       4500|\n",
            "+-----+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlVZodkHLKh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advance"
      ],
      "metadata": {
        "id": "G3HGEg7nN5CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark =SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
        "columns = [\"Name\",\"Department\",\"Salary\"]\n",
        "data = [\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Jane\", \"Finance\", 4000),\n",
        "    (\"Mike\", \"Sales\", 3500),\n",
        "    (\"Alice\", \"Finance\", 3800),\n",
        "    (\"Bob\", \"IT\", 4500)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iH22Se56Jug",
        "outputId": "36dd02ce-c751-4d66-9395-89728fc9df8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg({\"Salary\":\"avg\"}).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFvOY1PIOPHI",
        "outputId": "60470295-9716-47ce-ee2b-e11d695cb67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").avg(\"Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAPqHIMwPLCn",
        "outputId": "39d422f5-1a1f-4258-9345-a6bff005af58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg({\"Salary\":\"avg\", \"Salary\":\"min\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9DCIGdZPla6",
        "outputId": "084faa4d-ee13-46f6-bdce-61d9572fe63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|min(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       3000|\n",
            "|   Finance|       3800|\n",
            "|        IT|       4500|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.groupBy(\"Department\") \\\n",
        "   .agg(F.avg(\"Salary\").alias(\"Average_Salary\"),\n",
        "        F.max(\"Salary\").alias(\"Max_Salary\"),\n",
        "        F.min(\"Salary\").alias(\"Min_Salary\")) \\\n",
        "   .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wag3ZpT7P-qW",
        "outputId": "b0f8aaab-351c-454f-d37a-1346879867a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+----------+----------+\n",
            "|Department|Average_Salary|Max_Salary|Min_Salary|\n",
            "+----------+--------------+----------+----------+\n",
            "|     Sales|        3250.0|      3500|      3000|\n",
            "|   Finance|        3900.0|      4000|      3800|\n",
            "|        IT|        4500.0|      4500|      4500|\n",
            "+----------+--------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for department info\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\")\n",
        "]\n",
        "dept_columns = [\"Department\", \"Location\"]\n",
        "\n",
        "dept_df=spark.createDataFrame(dept_data, dept_columns)\n",
        "dept_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-ax8hAJR3NS",
        "outputId": "6de2ceb9-fffb-45bc-c5bb-7adb0676a213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for department info\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\")\n",
        "]\n",
        "dept_columns = [\"Department\", \"Location\"]\n",
        "\n",
        "dept_df=spark.createDataFrame(dept_data, dept_columns)\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGMD86-cTwdS",
        "outputId": "cea8dd98-8a3d-453c-ab81-d088ff1ba314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "joined_df = df.join(dept_df, on=\"Department\", how=\"inner\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKa960E3TGTL",
        "outputId": "a1ea3c2a-ad26-4d12-f2c2-eae9b945cee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+----------+\n",
            "|Department| Name|Salary|  Location|\n",
            "+----------+-----+------+----------+\n",
            "|   Finance| Jane|  4000|Building B|\n",
            "|   Finance|Alice|  3800|Building B|\n",
            "|        IT|  Bob|  4500|Building C|\n",
            "|     Sales| John|  3000|Building A|\n",
            "|     Sales| Mike|  3500|Building A|\n",
            "+----------+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXLBxOjjT8ti",
        "outputId": "af0261dc-dc6a-4cb9-8795-b39a4e0cc699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Employee DataFrame\n",
        "emp_data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"HR\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Sam\", \"Support\", 3200)\n",
        "]\n",
        "emp_cols = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_cols)\n",
        "\n",
        "# Department DataFrame\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\"),\n",
        "    (\"Admin\", \"Building D\")\n",
        "]\n",
        "dept_cols = [\"Department\", \"Location\"]\n",
        "dept_df = spark.createDataFrame(dept_data, dept_cols)\n",
        "\n",
        "# Display both\n",
        "emp_df.show()\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KT6v2E4UGzU",
        "outputId": "95671345-a97b-460f-b7c2-ea29cec0c943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1| John|     Sales|  3000|\n",
            "|    2| Jane|   Finance|  4000|\n",
            "|    3| Mike|     Sales|  3500|\n",
            "|    4|Alice|        HR|  3800|\n",
            "|    5|  Bob|        IT|  4500|\n",
            "|    6|  Sam|   Support|  3200|\n",
            "+-----+-----+----------+------+\n",
            "\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "|     Admin|Building D|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMURZs9oVSA-",
        "outputId": "64f734a0-285f-4775-9152-a0da3a7c9982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+------+----------+\n",
            "|Department|EmpID|Name|Salary|  Location|\n",
            "+----------+-----+----+------+----------+\n",
            "|   Finance|    2|Jane|  4000|Building B|\n",
            "|        IT|    5| Bob|  4500|Building C|\n",
            "|     Sales|    1|John|  3000|Building A|\n",
            "|     Sales|    3|Mike|  3500|Building A|\n",
            "+----------+-----+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# left join\n",
        "emp_df.join(dept_df, on=\"Department\", how=\"left\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyWA9eNGVfUk",
        "outputId": "f2a75c4a-aa58-49cb-a104-667f8c2a12bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+----------+\n",
            "|Department|EmpID| Name|Salary|  Location|\n",
            "+----------+-----+-----+------+----------+\n",
            "|     Sales|    1| John|  3000|Building A|\n",
            "|     Sales|    3| Mike|  3500|Building A|\n",
            "|   Finance|    2| Jane|  4000|Building B|\n",
            "|        HR|    4|Alice|  3800|      NULL|\n",
            "|        IT|    5|  Bob|  4500|Building C|\n",
            "|   Support|    6|  Sam|  3200|      NULL|\n",
            "+----------+-----+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd5tYkmkVrs6",
        "outputId": "63678785-9822-4d6a-966b-f0700a486ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+------+----------+\n",
            "|Department|EmpID|Name|Salary|  Location|\n",
            "+----------+-----+----+------+----------+\n",
            "|     Sales|    3|Mike|  3500|Building A|\n",
            "|     Sales|    1|John|  3000|Building A|\n",
            "|   Finance|    2|Jane|  4000|Building B|\n",
            "|     Admin| NULL|NULL|  NULL|Building D|\n",
            "|        IT|    5| Bob|  4500|Building C|\n",
            "+----------+-----+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"full\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd6lHzGIVzd-",
        "outputId": "95409f65-ca23-490d-c0fa-508a1b4f78de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+----------+\n",
            "|Department|EmpID| Name|Salary|  Location|\n",
            "+----------+-----+-----+------+----------+\n",
            "|     Admin| NULL| NULL|  NULL|Building D|\n",
            "|   Finance|    2| Jane|  4000|Building B|\n",
            "|        HR|    4|Alice|  3800|      NULL|\n",
            "|        IT|    5|  Bob|  4500|Building C|\n",
            "|     Sales|    1| John|  3000|Building A|\n",
            "|     Sales|    3| Mike|  3500|Building A|\n",
            "|   Support|    6|  Sam|  3200|      NULL|\n",
            "+----------+-----+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SALES.csv"
      ],
      "metadata": {
        "id": "pJHJcDwioVYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"sale_date\", StringType(), True),  # or TimestampType()\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Create data\n",
        "data = [\n",
        "    (1001, 101, 501, \"2025-07-10 08:23:00\", 3, 25.5),\n",
        "    (1002, 102, 502, \"2025-07-11 09:45:00\", 2, 15.0),\n",
        "    (1003, 103, 503, \"2025-07-12 10:15:00\", 1, 30.0),\n",
        "    (1004, 101, 504, \"2025-07-13 12:20:00\", 5, 25.5),\n",
        "    (1005, 105, 505, \"2025-07-14 14:35:00\", 10, 45.0),\n",
        "    (1006, 102, 506, \"2025-07-15 16:00:00\", 4, 15.0),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df = df.withColumn(\"sale_date\", F.to_timestamp(\"sale_date\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa1ZzS7cWIpR",
        "outputId": "8f9f43b5-3621-4e07-8cd1-fb66638c4a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|customer_id|          sale_date|quantity|price|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|   1001|       101|        501|2025-07-10 08:23:00|       3| 25.5|\n",
            "|   1002|       102|        502|2025-07-11 09:45:00|       2| 15.0|\n",
            "|   1003|       103|        503|2025-07-12 10:15:00|       1| 30.0|\n",
            "|   1004|       101|        504|2025-07-13 12:20:00|       5| 25.5|\n",
            "|   1005|       105|        505|2025-07-14 14:35:00|      10| 45.0|\n",
            "|   1006|       102|        506|2025-07-15 16:00:00|       4| 15.0|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Define schema for products\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Create product data\n",
        "products_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_products = spark.createDataFrame(products_data, schema=product_schema)\n",
        "df_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2UKYhzqZtn0",
        "outputId": "4169b7f1-95da-43d6-9dca-ccd8de97833d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|     category|\n",
            "+----------+------------+-------------+\n",
            "|       101|    Widget A|      Gadgets|\n",
            "|       102|    Widget B|      Gadgets|\n",
            "|       103|    Widget C|  Electronics|\n",
            "|       104|    Widget D|  Electronics|\n",
            "|       105|    Widget E|Home & Living|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "\n",
        "# Define schema\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"join_date\", StringType(), True),  # or use TimestampType\n",
        "])\n",
        "\n",
        "# Create data\n",
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 09:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_customers = spark.createDataFrame(customer_data, schema=customer_schema) \\\n",
        "    .withColumn(\"join_date\", F.to_timestamp(\"join_date\"))\n",
        "\n",
        "df_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE4dVCTBaJ45",
        "outputId": "1f1a501e-073b-48df-f1ab-3b3344baeb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------------+-------------------+\n",
            "|customer_id|customer_name|              email|          join_date|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "|        501|        Alice|  alice@example.com|2025-05-20 10:10:00|\n",
            "|        502|          Bob|    bob@example.com|2025-06-15 14:00:00|\n",
            "|        503|      Charlie|charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|        David|  david@example.com|2025-07-01 12:25:00|\n",
            "|        505|         Emma|   emma@example.com|2025-07-10 15:30:00|\n",
            "|        506|        Frank|  frank@example.com|2025-03-23 17:00:00|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df contains sales data with: product_id, quantity, price\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\")) \\\n",
        "  .groupBy(\"product_id\") \\\n",
        "  .agg(F.sum(\"revenue\").alias(\"total_revenue\")) \\\n",
        "  .orderBy(F.desc(\"total_revenue\")) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCQFKQG-ao5T",
        "outputId": "62ec255e-0f63-4888-ac6d-19be2c447e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|total_revenue|\n",
            "+----------+-------------+\n",
            "|       105|        450.0|\n",
            "|       101|        204.0|\n",
            "|       102|         90.0|\n",
            "|       103|         30.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Option 1: Use customer_id only\n",
        "df.groupBy(\"customer_id\") \\\n",
        "  .agg(F.sum(\"quantity\").alias(\"total_quantity\")) \\\n",
        "  .orderBy(F.desc(\"total_quantity\")) \\\n",
        "  .show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn2Qay-YbHdk",
        "outputId": "5f542244-7f44-41e3-ee92-2a2e058c5c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|total_quantity|\n",
            "+-----------+--------------+\n",
            "|        505|            10|\n",
            "|        504|             5|\n",
            "|        506|             4|\n",
            "|        501|             3|\n",
            "|        502|             2|\n",
            "|        503|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Define schema for products\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Create product data\n",
        "products_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_products = spark.createDataFrame(products_data, schema=product_schema)\n",
        "\n",
        "# Define schema for customer data\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"join_date\", StringType(), True),  # or use TimestampType\n",
        "])\n",
        "\n",
        "# Create data\n",
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 09:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_customers = spark.createDataFrame(customer_data, schema=customer_schema) \\\n",
        "    .withColumn(\"join_date\", F.to_timestamp(\"join_date\"))\n",
        "\n",
        "# Rename df[\"Product\"] to \"product_id\" so it matches df_products\n",
        "df_renamed = df.withColumnRenamed(\"Product\", \"product_id\")\n",
        "\n",
        "# Then join\n",
        "df_joined = df_renamed.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "id": "_Wvk9iZLcXny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c7cb69ee-308f-4cd3-df79-0d6674aa5088"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------------+--------+\n",
            "|product_id|Month|Sales|product_name|category|\n",
            "+----------+-----+-----+------------+--------+\n",
            "+----------+-----+-----+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_full.groupBy(\"customer_name\") \\\n",
        "       .agg(F.sum(\"quantity\").alias(\"total_quantity\")) \\\n",
        "       .orderBy(F.desc(\"total_quantity\")) \\\n",
        "       .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SYG2RI0cgbV",
        "outputId": "7abae3dc-0981-4723-ff7f-1cdc9fcd2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|customer_name|total_quantity|\n",
            "+-------------+--------------+\n",
            "|         Emma|            10|\n",
            "|        David|             5|\n",
            "|        Frank|             4|\n",
            "|        Alice|             3|\n",
            "|          Bob|             2|\n",
            "|      Charlie|             1|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Make sure df_full includes: customer_id, customer_name, quantity, price\n",
        "df_full.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\")) \\\n",
        "       .groupBy(\"customer_name\") \\\n",
        "       .agg(F.avg(\"revenue\").alias(\"average_revenue\")) \\\n",
        "       .orderBy(F.desc(\"average_revenue\")) \\\n",
        "       .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hvkrKsrb2xb",
        "outputId": "5a878710-4584-4bb4-a76b-8a505a3ddf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+\n",
            "|customer_name|average_revenue|\n",
            "+-------------+---------------+\n",
            "|         Emma|          450.0|\n",
            "|        David|          127.5|\n",
            "|        Alice|           76.5|\n",
            "|        Frank|           60.0|\n",
            "|      Charlie|           30.0|\n",
            "|          Bob|           30.0|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Add a 'revenue' column\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Extract year and month from 'sale_date'\n",
        "df_monthly = df_with_revenue.withColumn(\"year_month\", F.date_format(\"sale_date\", \"yyyy-MM\"))\n",
        "\n",
        "# Step 3: Group by 'year_month' and sum revenue\n",
        "df_monthly.groupBy(\"year_month\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_revenue\")) \\\n",
        "    .orderBy(\"year_month\") \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOvUu8becu7H",
        "outputId": "cb545e6c-b979-4837-c8aa-71c82e23aab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|year_month|total_revenue|\n",
            "+----------+-------------+\n",
            "|   2025-07|        774.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_sales_by_category = df.join(df_products, on=\"product_id\", how=\"inner\") \\\n",
        "    .groupBy(\"category\") \\\n",
        "    .agg(F.count(\"*\").alias(\"total_sales\")) \\\n",
        "    .orderBy(F.desc(\"total_sales\"))\n",
        "\n",
        "df_sales_by_category.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUgfpq_rdPB5",
        "outputId": "903ceefd-86b7-4f0a-ed8b-70eaa602c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|     category|total_sales|\n",
            "+-------------+-----------+\n",
            "|      Gadgets|          4|\n",
            "|  Electronics|          1|\n",
            "|Home & Living|          1|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Calculate revenue per row\n",
        "df_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with products to get product names\n",
        "df_with_names = df_revenue.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Group by product and sum revenue\n",
        "df_top_products = df_with_names.groupBy(\"product_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\")) \\\n",
        "    .limit(3)\n",
        "\n",
        "# Step 4: Show result\n",
        "df_top_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2J-vJURdY4H",
        "outputId": "42c16366-80c1-4f07-f420-f81e9df33fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|product_name|total_revenue|\n",
            "+------------+-------------+\n",
            "|    Widget E|        450.0|\n",
            "|    Widget A|        204.0|\n",
            "|    Widget B|         90.0|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joins\n",
        "\n",
        "# Join sales_data with product_data on product_id\n",
        "df_sales_with_products = df.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Select the relevant columns\n",
        "df_sales_with_products.select(\n",
        "    \"sale_id\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJLuvvA0dzJI",
        "outputId": "a4e24265-82f9-4ad3-cbde-30087d27bf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category     |customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets      |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1004   |101       |Widget A    |Gadgets      |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1002   |102       |Widget B    |Gadgets      |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1006   |102       |Widget B    |Gadgets      |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|1003   |103       |Widget C    |Electronics  |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|1005   |105       |Widget E    |Home & Living|505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join sales with customer data on customer_id\n",
        "df_sales_with_customers = df.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Select columns for detailed sales info with customer name and email\n",
        "df_sales_with_customers.select(\n",
        "    \"sale_id\",\n",
        "    \"customer_id\",\n",
        "    \"customer_name\",\n",
        "    \"email\",\n",
        "    \"sale_date\",\n",
        "    \"product_id\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZ70a8OlJo9",
        "outputId": "ee3918c1-006c-42ee-bdd3-b36db499e475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "|sale_id|customer_id|customer_name|email              |sale_date          |product_id|quantity|price|\n",
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "|1001   |501        |Alice        |alice@example.com  |2025-07-10 08:23:00|101       |3       |25.5 |\n",
            "|1002   |502        |Bob          |bob@example.com    |2025-07-11 09:45:00|102       |2       |15.0 |\n",
            "|1003   |503        |Charlie      |charlie@example.com|2025-07-12 10:15:00|103       |1       |30.0 |\n",
            "|1004   |504        |David        |david@example.com  |2025-07-13 12:20:00|101       |5       |25.5 |\n",
            "|1005   |505        |Emma         |emma@example.com   |2025-07-14 14:35:00|105       |10      |45.0 |\n",
            "|1006   |506        |Frank        |frank@example.com  |2025-07-15 16:00:00|102       |4       |15.0 |\n",
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Join sales and product data\n",
        "df_sales_with_category = df.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 2: Filter for category 'Gadgets'\n",
        "df_gadgets_sales = df_sales_with_category.filter(F.col(\"category\") == \"Gadgets\")\n",
        "\n",
        "# Step 3: Show selected fields\n",
        "df_gadgets_sales.select(\n",
        "    \"sale_id\", \"product_id\", \"product_name\", \"category\",\n",
        "    \"customer_id\", \"sale_date\", \"quantity\", \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ec9cypllXat",
        "outputId": "0465921b-4694-4fea-c366-38131e6750e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category|customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1004   |101       |Widget A    |Gadgets |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1002   |102       |Widget B    |Gadgets |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1006   |102       |Widget B    |Gadgets |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LEFT JOIN between sales and products\n",
        "df_sales_left_join = df.join(df_products, on=\"product_id\", how=\"left\")\n",
        "\n",
        "# Show key fields, including nulls if product info is missing\n",
        "df_sales_left_join.select(\n",
        "    \"sale_id\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96aU-KIflrrr",
        "outputId": "490bd3f2-c530-4a5e-acd2-943f503d6e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category     |customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets      |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1003   |103       |Widget C    |Electronics  |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|1002   |102       |Widget B    |Gadgets      |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1004   |101       |Widget A    |Gadgets      |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1006   |102       |Widget B    |Gadgets      |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|1005   |105       |Widget E    |Home & Living|505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Alias the sales data for self-join\n",
        "sales1 = df.alias(\"s1\")\n",
        "sales2 = df.alias(\"s2\")\n",
        "\n",
        "# Perform the self-join on product_id where sale_dates are different\n",
        "df_self_joined = sales1.join(\n",
        "    sales2,\n",
        "    (col(\"s1.product_id\") == col(\"s2.product_id\")) &\n",
        "    (col(\"s1.sale_date\") < col(\"s2.sale_date\")),  # avoid same and duplicate pairs\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Select relevant comparison fields\n",
        "df_self_joined.select(\n",
        "    col(\"s1.product_id\"),\n",
        "    col(\"s1.sale_id\").alias(\"sale_id_1\"),\n",
        "    col(\"s1.sale_date\").alias(\"sale_date_1\"),\n",
        "    col(\"s1.quantity\").alias(\"quantity_1\"),\n",
        "    col(\"s2.sale_id\").alias(\"sale_id_2\"),\n",
        "    col(\"s2.sale_date\").alias(\"sale_date_2\"),\n",
        "    col(\"s2.quantity\").alias(\"quantity_2\")\n",
        ").orderBy(\"product_id\", \"sale_date_1\").show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tin10qcRl0jW",
        "outputId": "b8234231-2b97-460a-d581-03f3e0f9953d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "|product_id|sale_id_1|sale_date_1        |quantity_1|sale_id_2|sale_date_2        |quantity_2|\n",
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "|101       |1001     |2025-07-10 08:23:00|3         |1004     |2025-07-13 12:20:00|5         |\n",
            "|102       |1002     |2025-07-11 09:45:00|2         |1006     |2025-07-15 16:00:00|4         |\n",
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform full outer join on product_id\n",
        "df_full_outer = df.join(df_products, on=\"product_id\", how=\"outer\")\n",
        "\n",
        "# Select relevant fields\n",
        "df_full_outer.select(\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"sale_id\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").orderBy(\"product_id\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thZtbCgMmXY2",
        "outputId": "e41855db-ba36-41db-815e-4512fc26b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "|product_id|product_name|category     |sale_id|customer_id|sale_date          |quantity|price|\n",
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "|101       |Widget A    |Gadgets      |1001   |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|101       |Widget A    |Gadgets      |1004   |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|102       |Widget B    |Gadgets      |1002   |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|102       |Widget B    |Gadgets      |1006   |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|103       |Widget C    |Electronics  |1003   |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|104       |Widget D    |Electronics  |NULL   |NULL       |NULL               |NULL    |NULL |\n",
            "|105       |Widget E    |Home & Living|1005   |505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Join sales with customers\n",
        "df_sales_customers = df.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 2: Join the above result with products\n",
        "df_complete_sales = df_sales_customers.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Select and display relevant columns\n",
        "df_complete_sales.select(\n",
        "    \"sale_id\",\n",
        "    \"sale_date\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"customer_name\",\n",
        "    \"email\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").orderBy(\"sale_date\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPFVpO3LmfPH",
        "outputId": "3dfbeb7c-1293-41b5-9a62-6f79153dd7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "|sale_id|sale_date          |product_id|product_name|category     |customer_id|customer_name|email              |quantity|price|\n",
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "|1001   |2025-07-10 08:23:00|101       |Widget A    |Gadgets      |501        |Alice        |alice@example.com  |3       |25.5 |\n",
            "|1002   |2025-07-11 09:45:00|102       |Widget B    |Gadgets      |502        |Bob          |bob@example.com    |2       |15.0 |\n",
            "|1003   |2025-07-12 10:15:00|103       |Widget C    |Electronics  |503        |Charlie      |charlie@example.com|1       |30.0 |\n",
            "|1004   |2025-07-13 12:20:00|101       |Widget A    |Gadgets      |504        |David        |david@example.com  |5       |25.5 |\n",
            "|1005   |2025-07-14 14:35:00|105       |Widget E    |Home & Living|505        |Emma         |emma@example.com   |10      |45.0 |\n",
            "|1006   |2025-07-15 16:00:00|102       |Widget B    |Gadgets      |506        |Frank        |frank@example.com  |4       |15.0 |\n",
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Additional Transformations:\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Ensure sale_date is in timestamp format (if it's a string)\n",
        "df_filtered = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Filter rows between July 10 and July 15, 2025 (inclusive)\n",
        "df_sales_range = df_filtered.filter(\n",
        "    col(\"sale_date\").between(\"2025-07-10\", \"2025-07-15\")\n",
        ")\n",
        "\n",
        "# Show result\n",
        "df_sales_range.select(\"sale_id\", \"sale_date\", \"product_id\", \"quantity\", \"price\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKtViCPimwOJ",
        "outputId": "4a332c6f-4a8e-4160-a346-8e221e021c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+--------+-----+\n",
            "|sale_id|sale_date          |product_id|quantity|price|\n",
            "+-------+-------------------+----------+--------+-----+\n",
            "|1001   |2025-07-10 08:23:00|101       |3       |25.5 |\n",
            "|1002   |2025-07-11 09:45:00|102       |2       |15.0 |\n",
            "|1003   |2025-07-12 10:15:00|103       |1       |30.0 |\n",
            "|1004   |2025-07-13 12:20:00|101       |5       |25.5 |\n",
            "|1005   |2025-07-14 14:35:00|105       |10      |45.0 |\n",
            "+-------+-------------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Calculate revenue per transaction\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with customer data to get names\n",
        "df_with_customers = df_with_revenue.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Aggregate total revenue per customer\n",
        "df_customer_spending = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_spent\"))\n",
        "\n",
        "# Step 4: Order and limit to top 5\n",
        "df_top_5_customers = df_customer_spending.orderBy(F.desc(\"total_spent\")).limit(5)\n",
        "\n",
        "# Step 5: Show result\n",
        "df_top_5_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aomJECxunKLt",
        "outputId": "f5603e2a-f0cf-4cba-a27d-657c008a2fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+\n",
            "|customer_id|customer_name|total_spent|\n",
            "+-----------+-------------+-----------+\n",
            "|        505|         Emma|      450.0|\n",
            "|        504|        David|      127.5|\n",
            "|        501|        Alice|       76.5|\n",
            "|        506|        Frank|       60.0|\n",
            "|        502|          Bob|       30.0|\n",
            "+-----------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Step 1: Compute revenue per transaction\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with customer data\n",
        "df_with_customers = df_with_revenue.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Total spending per customer\n",
        "df_total_spending = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_spent\"))\n",
        "\n",
        "# Step 4: Categorize into spending groups\n",
        "df_segmented = df_total_spending.withColumn(\n",
        "    \"spending_category\",\n",
        "    when(F.col(\"total_spent\") <= 100, \"Low Spender\")\n",
        "    .when((F.col(\"total_spent\") > 100) & (F.col(\"total_spent\") <= 300), \"Medium Spender\")\n",
        "    .otherwise(\"High Spender\")\n",
        ")\n",
        "\n",
        "# Step 5: Show the result\n",
        "df_segmented.orderBy(F.desc(\"total_spent\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0DV_Qu8nWzV",
        "outputId": "22c7ea9c-47b4-4e44-b5cf-8b35da9bc74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------------+\n",
            "|customer_id|customer_name|total_spent|spending_category|\n",
            "+-----------+-------------+-----------+-----------------+\n",
            "|        505|         Emma|      450.0|     High Spender|\n",
            "|        504|        David|      127.5|   Medium Spender|\n",
            "|        501|        Alice|       76.5|      Low Spender|\n",
            "|        506|        Frank|       60.0|      Low Spender|\n",
            "|        502|          Bob|       30.0|      Low Spender|\n",
            "|        503|      Charlie|       30.0|      Low Spender|\n",
            "+-----------+-------------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, to_timestamp\n",
        "\n",
        "# Ensure sale_date is timestamp\n",
        "df_sales = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Join with customer names (optional, for readability)\n",
        "df_with_customers = df_sales.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Group by customer and find first and last purchase dates\n",
        "df_purchase_dates = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(\n",
        "        min(\"sale_date\").alias(\"first_purchase\"),\n",
        "        max(\"sale_date\").alias(\"last_purchase\")\n",
        "    ) \\\n",
        "    .orderBy(\"customer_id\")\n",
        "\n",
        "# Show result\n",
        "df_purchase_dates.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzN8WuuwnZKc",
        "outputId": "4fdd05e7-e8dd-49ab-dbb1-3c6ccd5be82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------------+-------------------+\n",
            "|customer_id|customer_name|first_purchase     |last_purchase      |\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "|501        |Alice        |2025-07-10 08:23:00|2025-07-10 08:23:00|\n",
            "|502        |Bob          |2025-07-11 09:45:00|2025-07-11 09:45:00|\n",
            "|503        |Charlie      |2025-07-12 10:15:00|2025-07-12 10:15:00|\n",
            "|504        |David        |2025-07-13 12:20:00|2025-07-13 12:20:00|\n",
            "|505        |Emma         |2025-07-14 14:35:00|2025-07-14 14:35:00|\n",
            "|506        |Frank        |2025-07-15 16:00:00|2025-07-15 16:00:00|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, to_timestamp, current_date, datediff\n",
        "\n",
        "# Ensure sale_date is timestamp\n",
        "df_sales = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Step 1: Get last purchase date per customer\n",
        "df_last_purchase = df_sales.groupBy(\"customer_id\") \\\n",
        "    .agg(max(\"sale_date\").alias(\"last_purchase\"))\n",
        "\n",
        "# Step 2: Join with customer info\n",
        "df_customer_last = df_last_purchase.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Filter customers where last purchase was more than 30 days ago\n",
        "df_inactive_customers = df_customer_last.filter(\n",
        "    datediff(current_date(), \"last_purchase\") > 30\n",
        ")\n",
        "\n",
        "# Step 4: Show result\n",
        "df_inactive_customers.select(\"customer_id\", \"customer_name\", \"email\", \"last_purchase\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bNuzSONnmTQ",
        "outputId": "b5fea272-e19b-4808-810f-8e25a7d23196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----+-------------+\n",
            "|customer_id|customer_name|email|last_purchase|\n",
            "+-----------+-------------+-----+-------------+\n",
            "+-----------+-------------+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA SETS"
      ],
      "metadata": {
        "id": "UOosWDy_r5VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FullCustomerDataset\").getOrCreate()\n",
        "\n",
        "# Define schema for customer data\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"join_date\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Full customer dataset\n",
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 9:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\"),\n",
        "    (507, \"Grace\", \"grace@example.com\", \"2025-05-01 13:20:00\"),\n",
        "    (508, \"Henry\", \"henry@example.com\", \"2025-06-07 10:10:00\"),\n",
        "    (509, \"Isabel\", \"isabel@example.com\", \"2025-05-25 16:30:00\"),\n",
        "    (510, \"Jack\", \"jack@example.com\", \"2025-04-12 11:55:00\"),\n",
        "    (511, \"Kate\", \"kate@example.com\", \"2025-06-18 14:10:00\"),\n",
        "    (512, \"Liam\", \"liam@example.com\", \"2025-07-05 8:45:00\"),\n",
        "    (513, \"Mona\", \"mona@example.com\", \"2025-03-30 15:25:00\"),\n",
        "    (514, \"Nina\", \"nina@example.com\", \"2025-04-19 10:35:00\"),\n",
        "    (515, \"Oscar\", \"oscar@example.com\", \"2025-05-14 9:15:00\"),\n",
        "    (516, \"Paul\", \"paul@example.com\", \"2025-06-03 12:50:00\"),\n",
        "    (517, \"Quinn\", \"quinn@example.com\", \"2025-07-13 13:10:00\"),\n",
        "    (518, \"Rita\", \"rita@example.com\", \"2025-07-20 11:00:00\"),\n",
        "    (519, \"Sara\", \"sara@example.com\", \"2025-06-30 17:40:00\"),\n",
        "    (520, \"Tom\", \"tom@example.com\", \"2025-05-18 14:20:00\"),\n",
        "    (521, \"Ursula\", \"ursula@example.com\", \"2025-04-01 10:50:00\"),\n",
        "    (522, \"Victor\", \"victor@example.com\", \"2025-07-22 16:30:00\"),\n",
        "    (523, \"Wendy\", \"wendy@example.com\", \"2025-06-09 13:00:00\"),\n",
        "    (524, \"Xander\", \"xander@example.com\", \"2025-05-30 18:05:00\"),\n",
        "    (525, \"Yvonne\", \"yvonne@example.com\", \"2025-03-28 14:15:00\"),\n",
        "    (526, \"Zach\", \"zach@example.com\", \"2025-04-23 11:30:00\"),\n",
        "    (527, \"Amos\", \"amos@example.com\", \"2025-07-07 9:25:00\"),\n",
        "    (528, \"Bella\", \"bella@example.com\", \"2025-05-02 12:00:00\"),\n",
        "    (529, \"Clara\", \"clara@example.com\", \"2025-06-25 15:10:00\"),\n",
        "    (530, \"Dan\", \"d@danmail.com\", \"2025-07-14 14:15:00\"),\n",
        "    (531, \"Ellie\", \"ellie@example.com\", \"2025-06-01 8:30:00\"),\n",
        "    (532, \"Freddy\", \"freddy@example.com\", \"2025-07-03 12:45:00\"),\n",
        "    (533, \"Grace\", \"grace@anothermail.com\", \"2025-05-15 13:30:00\"),\n",
        "    (534, \"Harry\", \"harry@example.com\", \"2025-04-10 17:25:00\"),\n",
        "    (535, \"Ivy\", \"ivy@example.com\", \"2025-03-18 16:00:00\"),\n",
        "    (536, \"John\", \"john@example.com\", \"2025-07-19 10:50:00\"),\n",
        "    (537, \"Ken\", \"ken@example.com\", \"2025-05-12 9:35:00\"),\n",
        "    (538, \"Lena\", \"lena@example.com\", \"2025-06-21 13:55:00\"),\n",
        "    (539, \"Monica\", \"monica@example.com\", \"2025-03-27 18:40:00\"),\n",
        "    (540, \"Nash\", \"nash@example.com\", \"2025-04-15 14:25:00\"),\n",
        "    (541, \"Olivia\", \"olivia@example.com\", \"2025-07-06 17:10:00\"),\n",
        "    (542, \"Peter\", \"peter@example.com\", \"2025-05-23 14:00:00\"),\n",
        "    (543, \"Quincy\", \"quincy@example.com\", \"2025-06-05 12:10:00\"),\n",
        "    (544, \"Rob\", \"rob@example.com\", \"2025-03-22 13:35:00\"),\n",
        "    (545, \"Sophia\", \"sophia@example.com\", \"2025-07-09 10:20:00\"),\n",
        "    (546, \"Theo\", \"theo@example.com\", \"2025-04-02 12:30:00\"),\n",
        "    (547, \"Una\", \"una@example.com\", \"2025-07-15 16:55:00\"),\n",
        "    (548, \"Victor\", \"victor1@example.com\", \"2025-05-06 15:40:00\"),\n",
        "    (549, \"Wesley\", \"wesley@example.com\", \"2025-06-12 11:50:00\"),\n",
        "    (550, \"Xena\", \"xena@example.com\", \"2025-07-17 8:55:00\"),\n",
        "    (551, \"Yara\", \"yara@example.com\", \"2025-03-29 10:45:00\"),\n",
        "    (552, \"Zoe\", \"zoe@example.com\", \"2025-05-05 9:20:00\"),\n",
        "    (553, \"Alice2\", \"alice2@example.com\", \"2025-07-23 14:30:00\"),\n",
        "    (554, \"Bob2\", \"bob2@example.com\", \"2025-06-13 11:25:00\"),\n",
        "    (555, \"Charlie2\", \"charlie2@example.com\", \"2025-04-20 9:55:00\"),\n",
        "    (556, \"David2\", \"david2@example.com\", \"2025-07-11 10:00:00\")\n",
        "]\n",
        "\n",
        "# Create DataFrame and convert join_date to timestamp\n",
        "df_all_customers = spark.createDataFrame(customer_data, schema=customer_schema) \\\n",
        "    .withColumn(\"join_date\", to_timestamp(\"join_date\"))\n",
        "\n",
        "# Display first few records\n",
        "df_all_customers.show(60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1yjGEEur4gv",
        "outputId": "799a1311-f908-4f8d-ed8d-331508ccce8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------------------+\n",
            "|customer_id|customer_name|               email|          join_date|\n",
            "+-----------+-------------+--------------------+-------------------+\n",
            "|        501|        Alice|   alice@example.com|2025-05-20 10:10:00|\n",
            "|        502|          Bob|     bob@example.com|2025-06-15 14:00:00|\n",
            "|        503|      Charlie| charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|        David|   david@example.com|2025-07-01 12:25:00|\n",
            "|        505|         Emma|    emma@example.com|2025-07-10 15:30:00|\n",
            "|        506|        Frank|   frank@example.com|2025-03-23 17:00:00|\n",
            "|        507|        Grace|   grace@example.com|2025-05-01 13:20:00|\n",
            "|        508|        Henry|   henry@example.com|2025-06-07 10:10:00|\n",
            "|        509|       Isabel|  isabel@example.com|2025-05-25 16:30:00|\n",
            "|        510|         Jack|    jack@example.com|2025-04-12 11:55:00|\n",
            "|        511|         Kate|    kate@example.com|2025-06-18 14:10:00|\n",
            "|        512|         Liam|    liam@example.com|2025-07-05 08:45:00|\n",
            "|        513|         Mona|    mona@example.com|2025-03-30 15:25:00|\n",
            "|        514|         Nina|    nina@example.com|2025-04-19 10:35:00|\n",
            "|        515|        Oscar|   oscar@example.com|2025-05-14 09:15:00|\n",
            "|        516|         Paul|    paul@example.com|2025-06-03 12:50:00|\n",
            "|        517|        Quinn|   quinn@example.com|2025-07-13 13:10:00|\n",
            "|        518|         Rita|    rita@example.com|2025-07-20 11:00:00|\n",
            "|        519|         Sara|    sara@example.com|2025-06-30 17:40:00|\n",
            "|        520|          Tom|     tom@example.com|2025-05-18 14:20:00|\n",
            "|        521|       Ursula|  ursula@example.com|2025-04-01 10:50:00|\n",
            "|        522|       Victor|  victor@example.com|2025-07-22 16:30:00|\n",
            "|        523|        Wendy|   wendy@example.com|2025-06-09 13:00:00|\n",
            "|        524|       Xander|  xander@example.com|2025-05-30 18:05:00|\n",
            "|        525|       Yvonne|  yvonne@example.com|2025-03-28 14:15:00|\n",
            "|        526|         Zach|    zach@example.com|2025-04-23 11:30:00|\n",
            "|        527|         Amos|    amos@example.com|2025-07-07 09:25:00|\n",
            "|        528|        Bella|   bella@example.com|2025-05-02 12:00:00|\n",
            "|        529|        Clara|   clara@example.com|2025-06-25 15:10:00|\n",
            "|        530|          Dan|       d@danmail.com|2025-07-14 14:15:00|\n",
            "|        531|        Ellie|   ellie@example.com|2025-06-01 08:30:00|\n",
            "|        532|       Freddy|  freddy@example.com|2025-07-03 12:45:00|\n",
            "|        533|        Grace|grace@anothermail...|2025-05-15 13:30:00|\n",
            "|        534|        Harry|   harry@example.com|2025-04-10 17:25:00|\n",
            "|        535|          Ivy|     ivy@example.com|2025-03-18 16:00:00|\n",
            "|        536|         John|    john@example.com|2025-07-19 10:50:00|\n",
            "|        537|          Ken|     ken@example.com|2025-05-12 09:35:00|\n",
            "|        538|         Lena|    lena@example.com|2025-06-21 13:55:00|\n",
            "|        539|       Monica|  monica@example.com|2025-03-27 18:40:00|\n",
            "|        540|         Nash|    nash@example.com|2025-04-15 14:25:00|\n",
            "|        541|       Olivia|  olivia@example.com|2025-07-06 17:10:00|\n",
            "|        542|        Peter|   peter@example.com|2025-05-23 14:00:00|\n",
            "|        543|       Quincy|  quincy@example.com|2025-06-05 12:10:00|\n",
            "|        544|          Rob|     rob@example.com|2025-03-22 13:35:00|\n",
            "|        545|       Sophia|  sophia@example.com|2025-07-09 10:20:00|\n",
            "|        546|         Theo|    theo@example.com|2025-04-02 12:30:00|\n",
            "|        547|          Una|     una@example.com|2025-07-15 16:55:00|\n",
            "|        548|       Victor| victor1@example.com|2025-05-06 15:40:00|\n",
            "|        549|       Wesley|  wesley@example.com|2025-06-12 11:50:00|\n",
            "|        550|         Xena|    xena@example.com|2025-07-17 08:55:00|\n",
            "|        551|         Yara|    yara@example.com|2025-03-29 10:45:00|\n",
            "|        552|          Zoe|     zoe@example.com|2025-05-05 09:20:00|\n",
            "|        553|       Alice2|  alice2@example.com|2025-07-23 14:30:00|\n",
            "|        554|         Bob2|    bob2@example.com|2025-06-13 11:25:00|\n",
            "|        555|     Charlie2|charlie2@example.com|2025-04-20 09:55:00|\n",
            "|        556|       David2|  david2@example.com|2025-07-11 10:00:00|\n",
            "+-----------+-------------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start SparkSession (if not already running)\n",
        "spark = SparkSession.builder.appName(\"ProductDataset\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Product data (truncated here  use full list in your environment)\n",
        "product_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\"),\n",
        "    (106, \"Widget F\", \"Home & Living\"),\n",
        "    (107, \"Widget G\", \"Furniture\"),\n",
        "    (108, \"Widget H\", \"Gadgets\"),\n",
        "    (109, \"Widget I\", \"Furniture\"),\n",
        "    (110, \"Widget J\", \"Electronics\"),\n",
        "    (111, \"Widget K\", \"Home & Living\"),\n",
        "    (112, \"Widget L\", \"Electronics\"),\n",
        "    (113, \"Widget M\", \"Gadgets\"),\n",
        "    (114, \"Widget N\", \"Furniture\"),\n",
        "    (115, \"Widget O\", \"Gadgets\"),\n",
        "    (116, \"Widget P\", \"Home & Living\"),\n",
        "    (117, \"Widget Q\", \"Electronics\"),\n",
        "    (118, \"Widget R\", \"Furniture\"),\n",
        "    (119, \"Widget S\", \"Gadgets\"),\n",
        "    (120, \"Widget T\", \"Furniture\"),\n",
        "    (121, \"Widget U\", \"Home & Living\"),\n",
        "    (122, \"Widget V\", \"Electronics\"),\n",
        "    (123, \"Widget W\", \"Furniture\"),\n",
        "    (124, \"Widget X\", \"Home & Living\"),\n",
        "    (125, \"Widget Y\", \"Gadgets\"),\n",
        "    (126, \"Widget Z\", \"Electronics\"),\n",
        "    (127, \"Widget AA\", \"Furniture\"),\n",
        "    (128, \"Widget AB\", \"Gadgets\"),\n",
        "    (129, \"Widget AC\", \"Home & Living\"),\n",
        "    (130, \"Widget AD\", \"Furniture\"),\n",
        "    (131, \"Widget AE\", \"Gadgets\"),\n",
        "    (132, \"Widget AF\", \"Electronics\"),\n",
        "    (133, \"Widget AG\", \"Furniture\"),\n",
        "    (134, \"Widget AH\", \"Home & Living\"),\n",
        "    (135, \"Widget AI\", \"Electronics\"),\n",
        "    (136, \"Widget AJ\", \"Furniture\"),\n",
        "    (137, \"Widget AK\", \"Gadgets\"),\n",
        "    (138, \"Widget AL\", \"Home & Living\"),\n",
        "    (139, \"Widget AM\", \"Electronics\"),\n",
        "    (140, \"Widget AN\", \"Furniture\"),\n",
        "    (141, \"Widget AO\", \"Gadgets\"),\n",
        "    (142, \"Widget AP\", \"Electronics\"),\n",
        "    (143, \"Widget AQ\", \"Home & Living\"),\n",
        "    (144, \"Widget AR\", \"Gadgets\"),\n",
        "    (145, \"Widget AS\", \"Furniture\"),\n",
        "    (146, \"Widget AT\", \"Home & Living\"),\n",
        "    (147, \"Widget AU\", \"Electronics\"),\n",
        "    (148, \"Widget AV\", \"Furniture\"),\n",
        "    (149, \"Widget AW\", \"Gadgets\"),\n",
        "    (150, \"Widget AX\", \"Home & Living\"),\n",
        "    (151, \"Widget AY\", \"Electronics\"),\n",
        "    (152, \"Widget AZ\", \"Furniture\"),\n",
        "    (153, \"Widget BA\", \"Gadgets\"),\n",
        "    (154, \"Widget BB\", \"Electronics\"),\n",
        "    (155, \"Widget BC\", \"Home & Living\"),\n",
        "    (156, \"Widget BD\", \"Furniture\"),\n",
        "    (157, \"Widget BE\", \"Gadgets\"),\n",
        "    (158, \"Widget BF\", \"Electronics\"),\n",
        "    (159, \"Widget BG\", \"Home & Living\"),\n",
        "    (160, \"Widget BH\", \"Furniture\"),\n",
        "    (161, \"Widget BI\", \"Gadgets\"),\n",
        "    (162, \"Widget BJ\", \"Electronics\"),\n",
        "    (163, \"Widget BK\", \"Home & Living\"),\n",
        "    (164, \"Widget BL\", \"Furniture\"),\n",
        "    (165, \"Widget BM\", \"Gadgets\"),\n",
        "    (166, \"Widget BN\", \"Electronics\"),\n",
        "    (167, \"Widget BO\", \"Furniture\"),\n",
        "    (168, \"Widget BP\", \"Home & Living\"),\n",
        "    (169, \"Widget BQ\", \"Gadgets\"),\n",
        "    (170, \"Widget BR\", \"Electronics\")\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_products = spark.createDataFrame(product_data, schema=product_schema)\n",
        "\n",
        "# Show some records\n",
        "df_products.show(70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffk_Qg5ytdLv",
        "outputId": "5dd0a067-e1c0-4829-8073-b82608510117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|     category|\n",
            "+----------+------------+-------------+\n",
            "|       101|    Widget A|      Gadgets|\n",
            "|       102|    Widget B|      Gadgets|\n",
            "|       103|    Widget C|  Electronics|\n",
            "|       104|    Widget D|  Electronics|\n",
            "|       105|    Widget E|Home & Living|\n",
            "|       106|    Widget F|Home & Living|\n",
            "|       107|    Widget G|    Furniture|\n",
            "|       108|    Widget H|      Gadgets|\n",
            "|       109|    Widget I|    Furniture|\n",
            "|       110|    Widget J|  Electronics|\n",
            "|       111|    Widget K|Home & Living|\n",
            "|       112|    Widget L|  Electronics|\n",
            "|       113|    Widget M|      Gadgets|\n",
            "|       114|    Widget N|    Furniture|\n",
            "|       115|    Widget O|      Gadgets|\n",
            "|       116|    Widget P|Home & Living|\n",
            "|       117|    Widget Q|  Electronics|\n",
            "|       118|    Widget R|    Furniture|\n",
            "|       119|    Widget S|      Gadgets|\n",
            "|       120|    Widget T|    Furniture|\n",
            "|       121|    Widget U|Home & Living|\n",
            "|       122|    Widget V|  Electronics|\n",
            "|       123|    Widget W|    Furniture|\n",
            "|       124|    Widget X|Home & Living|\n",
            "|       125|    Widget Y|      Gadgets|\n",
            "|       126|    Widget Z|  Electronics|\n",
            "|       127|   Widget AA|    Furniture|\n",
            "|       128|   Widget AB|      Gadgets|\n",
            "|       129|   Widget AC|Home & Living|\n",
            "|       130|   Widget AD|    Furniture|\n",
            "|       131|   Widget AE|      Gadgets|\n",
            "|       132|   Widget AF|  Electronics|\n",
            "|       133|   Widget AG|    Furniture|\n",
            "|       134|   Widget AH|Home & Living|\n",
            "|       135|   Widget AI|  Electronics|\n",
            "|       136|   Widget AJ|    Furniture|\n",
            "|       137|   Widget AK|      Gadgets|\n",
            "|       138|   Widget AL|Home & Living|\n",
            "|       139|   Widget AM|  Electronics|\n",
            "|       140|   Widget AN|    Furniture|\n",
            "|       141|   Widget AO|      Gadgets|\n",
            "|       142|   Widget AP|  Electronics|\n",
            "|       143|   Widget AQ|Home & Living|\n",
            "|       144|   Widget AR|      Gadgets|\n",
            "|       145|   Widget AS|    Furniture|\n",
            "|       146|   Widget AT|Home & Living|\n",
            "|       147|   Widget AU|  Electronics|\n",
            "|       148|   Widget AV|    Furniture|\n",
            "|       149|   Widget AW|      Gadgets|\n",
            "|       150|   Widget AX|Home & Living|\n",
            "|       151|   Widget AY|  Electronics|\n",
            "|       152|   Widget AZ|    Furniture|\n",
            "|       153|   Widget BA|      Gadgets|\n",
            "|       154|   Widget BB|  Electronics|\n",
            "|       155|   Widget BC|Home & Living|\n",
            "|       156|   Widget BD|    Furniture|\n",
            "|       157|   Widget BE|      Gadgets|\n",
            "|       158|   Widget BF|  Electronics|\n",
            "|       159|   Widget BG|Home & Living|\n",
            "|       160|   Widget BH|    Furniture|\n",
            "|       161|   Widget BI|      Gadgets|\n",
            "|       162|   Widget BJ|  Electronics|\n",
            "|       163|   Widget BK|Home & Living|\n",
            "|       164|   Widget BL|    Furniture|\n",
            "|       165|   Widget BM|      Gadgets|\n",
            "|       166|   Widget BN|  Electronics|\n",
            "|       167|   Widget BO|    Furniture|\n",
            "|       168|   Widget BP|Home & Living|\n",
            "|       169|   Widget BQ|      Gadgets|\n",
            "|       170|   Widget BR|  Electronics|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary components\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Define schema for full sales data\n",
        "sales_schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"sale_date\", StringType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Full 70-entry sales dataset\n",
        "sales_data_full = [\n",
        "    (1001, 101, 501, \"2025-07-10 8:23:00\", 3, 25.5), (1002, 102, 502, \"2025-07-11 9:45:00\", 2, 15.0),\n",
        "    (1003, 103, 503, \"2025-07-12 10:15:00\", 1, 30.0), (1004, 101, 504, \"2025-07-13 12:20:00\", 5, 25.5),\n",
        "    (1005, 105, 505, \"2025-07-14 14:35:00\", 10, 45.0), (1006, 102, 506, \"2025-07-15 16:00:00\", 4, 15.0),\n",
        "    (1007, 106, 507, \"2025-07-16 17:10:00\", 3, 40.0), (1008, 107, 508, \"2025-07-17 18:25:00\", 2, 60.0),\n",
        "    (1009, 108, 509, \"2025-07-18 19:30:00\", 7, 25.0), (1010, 109, 510, \"2025-07-19 20:45:00\", 6, 50.0),\n",
        "    (1011, 110, 511, \"2025-07-20 21:55:00\", 4, 12.5), (1012, 111, 512, \"2025-07-21 22:30:00\", 9, 10.0),\n",
        "    (1013, 112, 513, \"2025-07-22 23:40:00\", 8, 22.5), (1014, 113, 514, \"2025-07-23 9:15:00\", 3, 17.0),\n",
        "    (1015, 114, 515, \"2025-07-24 10:35:00\", 2, 45.5), (1016, 115, 516, \"2025-07-25 11:45:00\", 4, 32.0),\n",
        "    (1017, 116, 517, \"2025-07-26 13:00:00\", 5, 28.0), (1018, 117, 518, \"2025-07-27 14:15:00\", 1, 50.0),\n",
        "    (1019, 118, 519, \"2025-07-28 15:30:00\", 6, 18.5), (1020, 119, 520, \"2025-07-29 16:45:00\", 7, 40.0),\n",
        "    (1021, 120, 521, \"2025-07-30 17:50:00\", 3, 30.5), (1022, 121, 522, \"2025-07-31 18:20:00\", 2, 22.0),\n",
        "    (1023, 122, 523, \"2025-08-01 19:00:00\", 4, 25.0), (1024, 123, 524, \"2025-08-02 20:10:00\", 9, 15.5),\n",
        "    (1025, 124, 525, \"2025-08-03 21:00:00\", 5, 27.5), (1026, 125, 526, \"2025-08-04 22:25:00\", 8, 33.0),\n",
        "    (1027, 126, 527, \"2025-08-05 23:30:00\", 6, 40.0), (1028, 127, 528, \"2025-08-06 9:50:00\", 7, 10.0),\n",
        "    (1029, 128, 529, \"2025-08-07 10:10:00\", 4, 22.5), (1030, 129, 530, \"2025-08-08 11:00:00\", 2, 38.0),\n",
        "    (1031, 130, 531, \"2025-08-09 12:20:00\", 5, 50.0), (1032, 131, 532, \"2025-08-10 13:30:00\", 8, 29.5),\n",
        "    (1033, 132, 533, \"2025-08-11 14:40:00\", 3, 44.0), (1034, 133, 534, \"2025-08-12 15:00:00\", 7, 16.5),\n",
        "    (1035, 134, 535, \"2025-08-13 16:00:00\", 9, 20.0), (1036, 135, 536, \"2025-08-14 17:10:00\", 2, 28.5),\n",
        "    (1037, 136, 537, \"2025-08-15 18:30:00\", 4, 32.0), (1038, 137, 538, \"2025-08-16 19:00:00\", 5, 23.0),\n",
        "    (1039, 138, 539, \"2025-08-17 20:15:00\", 6, 35.0), (1040, 139, 540, \"2025-08-18 21:25:00\", 3, 18.5),\n",
        "    (1041, 140, 541, \"2025-08-19 22:00:00\", 2, 40.0), (1042, 141, 542, \"2025-08-20 23:10:00\", 8, 21.5),\n",
        "    (1043, 142, 543, \"2025-08-21 9:15:00\", 4, 47.5), (1044, 143, 544, \"2025-08-22 10:30:00\", 7, 12.0),\n",
        "    (1045, 144, 545, \"2025-08-23 11:40:00\", 3, 30.5), (1046, 145, 546, \"2025-08-24 12:55:00\", 6, 15.0),\n",
        "    (1047, 146, 547, \"2025-08-25 13:10:00\", 5, 36.0), (1048, 147, 548, \"2025-08-26 14:25:00\", 9, 22.0),\n",
        "    (1049, 148, 549, \"2025-08-27 15:40:00\", 2, 29.0), (1050, 149, 550, \"2025-08-28 16:50:00\", 1, 50.0),\n",
        "    (1051, 101, 551, \"2025-08-29 17:55:00\", 3, 25.5), (1052, 102, 552, \"2025-08-30 18:45:00\", 4, 15.0),\n",
        "    (1053, 103, 553, \"2025-08-31 19:30:00\", 7, 30.0), (1054, 104, 554, \"2025-09-01 20:40:00\", 5, 40.0),\n",
        "    (1055, 105, 555, \"2025-09-02 21:10:00\", 2, 45.0), (1056, 106, 556, \"2025-09-03 22:30:00\", 8, 50.0),\n",
        "    (1057, 107, 557, \"2025-09-04 23:50:00\", 3, 20.5), (1058, 108, 558, \"2025-09-05 9:30:00\", 1, 60.0),\n",
        "    (1059, 109, 559, \"2025-09-06 10:40:00\", 6, 18.0), (1060, 110, 560, \"2025-09-07 11:50:00\", 5, 32.0),\n",
        "    (1061, 111, 561, \"2025-09-08 13:00:00\", 9, 22.5), (1062, 112, 562, \"2025-09-09 14:10:00\", 2, 25.5),\n",
        "    (1063, 113, 563, \"2025-09-10 15:25:00\", 7, 38.0), (1064, 114, 564, \"2025-09-11 16:35:00\", 3, 27.5),\n",
        "    (1065, 115, 565, \"2025-09-12 17:45:00\", 8, 44.5), (1066, 116, 566, \"2025-09-13 18:55:00\", 6, 20.0),\n",
        "    (1067, 117, 567, \"2025-09-14 20:00:00\", 2, 32.0), (1068, 118, 568, \"2025-09-15 21:10:00\", 5, 27.5),\n",
        "    (1069, 119, 569, \"2025-09-16 22:15:00\", 4, 50.0), (1070, 120, 570, \"2025-09-17 23:00:00\", 3, 40.0)\n",
        "]\n",
        "\n",
        "# Create DataFrame and convert sale_date to timestamp\n",
        "df_sales_full = spark.createDataFrame(sales_data_full, schema=sales_schema) \\\n",
        "    .withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Display first 10 records to confirm\n",
        "df_sales_full.show(70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l_BwomdxTYG",
        "outputId": "0c679c33-5a32-46c8-c8b7-2493cfe14f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|customer_id|          sale_date|quantity|price|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|   1001|       101|        501|2025-07-10 08:23:00|       3| 25.5|\n",
            "|   1002|       102|        502|2025-07-11 09:45:00|       2| 15.0|\n",
            "|   1003|       103|        503|2025-07-12 10:15:00|       1| 30.0|\n",
            "|   1004|       101|        504|2025-07-13 12:20:00|       5| 25.5|\n",
            "|   1005|       105|        505|2025-07-14 14:35:00|      10| 45.0|\n",
            "|   1006|       102|        506|2025-07-15 16:00:00|       4| 15.0|\n",
            "|   1007|       106|        507|2025-07-16 17:10:00|       3| 40.0|\n",
            "|   1008|       107|        508|2025-07-17 18:25:00|       2| 60.0|\n",
            "|   1009|       108|        509|2025-07-18 19:30:00|       7| 25.0|\n",
            "|   1010|       109|        510|2025-07-19 20:45:00|       6| 50.0|\n",
            "|   1011|       110|        511|2025-07-20 21:55:00|       4| 12.5|\n",
            "|   1012|       111|        512|2025-07-21 22:30:00|       9| 10.0|\n",
            "|   1013|       112|        513|2025-07-22 23:40:00|       8| 22.5|\n",
            "|   1014|       113|        514|2025-07-23 09:15:00|       3| 17.0|\n",
            "|   1015|       114|        515|2025-07-24 10:35:00|       2| 45.5|\n",
            "|   1016|       115|        516|2025-07-25 11:45:00|       4| 32.0|\n",
            "|   1017|       116|        517|2025-07-26 13:00:00|       5| 28.0|\n",
            "|   1018|       117|        518|2025-07-27 14:15:00|       1| 50.0|\n",
            "|   1019|       118|        519|2025-07-28 15:30:00|       6| 18.5|\n",
            "|   1020|       119|        520|2025-07-29 16:45:00|       7| 40.0|\n",
            "|   1021|       120|        521|2025-07-30 17:50:00|       3| 30.5|\n",
            "|   1022|       121|        522|2025-07-31 18:20:00|       2| 22.0|\n",
            "|   1023|       122|        523|2025-08-01 19:00:00|       4| 25.0|\n",
            "|   1024|       123|        524|2025-08-02 20:10:00|       9| 15.5|\n",
            "|   1025|       124|        525|2025-08-03 21:00:00|       5| 27.5|\n",
            "|   1026|       125|        526|2025-08-04 22:25:00|       8| 33.0|\n",
            "|   1027|       126|        527|2025-08-05 23:30:00|       6| 40.0|\n",
            "|   1028|       127|        528|2025-08-06 09:50:00|       7| 10.0|\n",
            "|   1029|       128|        529|2025-08-07 10:10:00|       4| 22.5|\n",
            "|   1030|       129|        530|2025-08-08 11:00:00|       2| 38.0|\n",
            "|   1031|       130|        531|2025-08-09 12:20:00|       5| 50.0|\n",
            "|   1032|       131|        532|2025-08-10 13:30:00|       8| 29.5|\n",
            "|   1033|       132|        533|2025-08-11 14:40:00|       3| 44.0|\n",
            "|   1034|       133|        534|2025-08-12 15:00:00|       7| 16.5|\n",
            "|   1035|       134|        535|2025-08-13 16:00:00|       9| 20.0|\n",
            "|   1036|       135|        536|2025-08-14 17:10:00|       2| 28.5|\n",
            "|   1037|       136|        537|2025-08-15 18:30:00|       4| 32.0|\n",
            "|   1038|       137|        538|2025-08-16 19:00:00|       5| 23.0|\n",
            "|   1039|       138|        539|2025-08-17 20:15:00|       6| 35.0|\n",
            "|   1040|       139|        540|2025-08-18 21:25:00|       3| 18.5|\n",
            "|   1041|       140|        541|2025-08-19 22:00:00|       2| 40.0|\n",
            "|   1042|       141|        542|2025-08-20 23:10:00|       8| 21.5|\n",
            "|   1043|       142|        543|2025-08-21 09:15:00|       4| 47.5|\n",
            "|   1044|       143|        544|2025-08-22 10:30:00|       7| 12.0|\n",
            "|   1045|       144|        545|2025-08-23 11:40:00|       3| 30.5|\n",
            "|   1046|       145|        546|2025-08-24 12:55:00|       6| 15.0|\n",
            "|   1047|       146|        547|2025-08-25 13:10:00|       5| 36.0|\n",
            "|   1048|       147|        548|2025-08-26 14:25:00|       9| 22.0|\n",
            "|   1049|       148|        549|2025-08-27 15:40:00|       2| 29.0|\n",
            "|   1050|       149|        550|2025-08-28 16:50:00|       1| 50.0|\n",
            "|   1051|       101|        551|2025-08-29 17:55:00|       3| 25.5|\n",
            "|   1052|       102|        552|2025-08-30 18:45:00|       4| 15.0|\n",
            "|   1053|       103|        553|2025-08-31 19:30:00|       7| 30.0|\n",
            "|   1054|       104|        554|2025-09-01 20:40:00|       5| 40.0|\n",
            "|   1055|       105|        555|2025-09-02 21:10:00|       2| 45.0|\n",
            "|   1056|       106|        556|2025-09-03 22:30:00|       8| 50.0|\n",
            "|   1057|       107|        557|2025-09-04 23:50:00|       3| 20.5|\n",
            "|   1058|       108|        558|2025-09-05 09:30:00|       1| 60.0|\n",
            "|   1059|       109|        559|2025-09-06 10:40:00|       6| 18.0|\n",
            "|   1060|       110|        560|2025-09-07 11:50:00|       5| 32.0|\n",
            "|   1061|       111|        561|2025-09-08 13:00:00|       9| 22.5|\n",
            "|   1062|       112|        562|2025-09-09 14:10:00|       2| 25.5|\n",
            "|   1063|       113|        563|2025-09-10 15:25:00|       7| 38.0|\n",
            "|   1064|       114|        564|2025-09-11 16:35:00|       3| 27.5|\n",
            "|   1065|       115|        565|2025-09-12 17:45:00|       8| 44.5|\n",
            "|   1066|       116|        566|2025-09-13 18:55:00|       6| 20.0|\n",
            "|   1067|       117|        567|2025-09-14 20:00:00|       2| 32.0|\n",
            "|   1068|       118|        568|2025-09-15 21:10:00|       5| 27.5|\n",
            "|   1069|       119|        569|2025-09-16 22:15:00|       4| 50.0|\n",
            "|   1070|       120|        570|2025-09-17 23:00:00|       3| 40.0|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WINDOWS FUNCTION  "
      ],
      "metadata": {
        "id": "X3j0tvmybGrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsYh5QhnbFgg",
        "outputId": "8e2d0af1-1509-45e8-adc0-941ebdec12fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window specification\n",
        "window_spec = Window.partitionBy(\"address.city\").orderBy(\"salary\")\n",
        "df.withColumn(\"rank\", rank().over(window_spec)).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "RDYhkCxsypye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35262962-99ed-47d5-ed28-f1cb455003b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "|id |name   |salary|subjects                |address                    |rank|\n",
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |1   |\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|1   |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|2   |\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |1   |\n",
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number, rank, dense_rank, max, sum, avg\n",
        "\n",
        "# Employee Data\n",
        "data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"Finance\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Tom\", \"Sales\", 3700),\n",
        "    (7, \"Jerry\", \"Finance\", 4200),\n",
        "    (8, \"Sam\", \"IT\", 4700),\n",
        "    (9, \"Steve\", \"Sales\", 3100),\n",
        "    (10, \"Rachel\", \"IT\", 4600)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0D97ukKbp6m",
        "outputId": "f3419a7c-ea13-4f21-f493-32678aa865d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+\n",
            "|EmpID|  Name|Department|Salary|\n",
            "+-----+------+----------+------+\n",
            "|    1|  John|     Sales|  3000|\n",
            "|    2|  Jane|   Finance|  4000|\n",
            "|    3|  Mike|     Sales|  3500|\n",
            "|    4| Alice|   Finance|  3800|\n",
            "|    5|   Bob|        IT|  4500|\n",
            "|    6|   Tom|     Sales|  3700|\n",
            "|    7| Jerry|   Finance|  4200|\n",
            "|    8|   Sam|        IT|  4700|\n",
            "|    9| Steve|     Sales|  3100|\n",
            "|   10|Rachel|        IT|  4600|\n",
            "+-----+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WindowSpec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "df.withColumn(\"Rank\", rank().over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQH69D_3dEgW",
        "outputId": "606b3660-e317-4707-ba1b-2f0e5c981be5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+----+\n",
            "|EmpID|  Name|Department|Salary|Rank|\n",
            "+-----+------+----------+------+----+\n",
            "|    7| Jerry|   Finance|  4200|   1|\n",
            "|    2|  Jane|   Finance|  4000|   2|\n",
            "|    4| Alice|   Finance|  3800|   3|\n",
            "|    8|   Sam|        IT|  4700|   1|\n",
            "|   10|Rachel|        IT|  4600|   2|\n",
            "|    5|   Bob|        IT|  4500|   3|\n",
            "|    6|   Tom|     Sales|  3700|   1|\n",
            "|    3|  Mike|     Sales|  3500|   2|\n",
            "|    9| Steve|     Sales|  3100|   3|\n",
            "|    1|  John|     Sales|  3000|   4|\n",
            "+-----+------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WindowSpec = Window.partitionBy(\"Department\")\n",
        "df.withColumn(\"MaxSalary\", max(col(\"Salary\")).over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeGIv4VDd1xx",
        "outputId": "0d9f869b-3345-42cb-fe83-9e31654d444d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+---------+\n",
            "|EmpID|  Name|Department|Salary|MaxSalary|\n",
            "+-----+------+----------+------+---------+\n",
            "|    2|  Jane|   Finance|  4000|     4200|\n",
            "|    4| Alice|   Finance|  3800|     4200|\n",
            "|    7| Jerry|   Finance|  4200|     4200|\n",
            "|    5|   Bob|        IT|  4500|     4700|\n",
            "|    8|   Sam|        IT|  4700|     4700|\n",
            "|   10|Rachel|        IT|  4600|     4700|\n",
            "|    1|  John|     Sales|  3000|     3700|\n",
            "|    3|  Mike|     Sales|  3500|     3700|\n",
            "|    6|   Tom|     Sales|  3700|     3700|\n",
            "|    9| Steve|     Sales|  3100|     3700|\n",
            "+-----+------+----------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6038a314",
        "outputId": "5226f196-f860-48b1-d0da-4e2f7ef8d36c"
      },
      "source": [
        "df.groupBy(\"Department\").agg(max(\"Salary\").alias(\"Max Salary\")).show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|Max Salary|\n",
            "+----------+----------+\n",
            "|     Sales|      3700|\n",
            "|   Finance|      4200|\n",
            "|        IT|      4700|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"Total Salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5qcbLHDzTRO",
        "outputId": "0ea20964-a364-4b04-84d4-c2d3170b3c53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+\n",
            "|Department|Total Salary|\n",
            "+----------+------------+\n",
            "|     Sales|       13300|\n",
            "|   Finance|       12000|\n",
            "|        IT|       13800|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WindowSpec = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "df.withColumn(\"Cumulative Salary\", sum(\"Salary\").over(WindowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQVWFe-1zuaF",
        "outputId": "ec5d2547-cb1e-4eef-9859-b00f683486db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+-----------------+\n",
            "|EmpID|  Name|Department|Salary|Cumulative Salary|\n",
            "+-----+------+----------+------+-----------------+\n",
            "|    4| Alice|   Finance|  3800|             3800|\n",
            "|    2|  Jane|   Finance|  4000|             7800|\n",
            "|    7| Jerry|   Finance|  4200|            12000|\n",
            "|    5|   Bob|        IT|  4500|             4500|\n",
            "|   10|Rachel|        IT|  4600|             9100|\n",
            "|    8|   Sam|        IT|  4700|            13800|\n",
            "|    1|  John|     Sales|  3000|             3000|\n",
            "|    9| Steve|     Sales|  3100|             6100|\n",
            "|    3|  Mike|     Sales|  3500|             9600|\n",
            "|    6|   Tom|     Sales|  3700|            13300|\n",
            "+-----+------+----------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n"
      ],
      "metadata": {
        "id": "aGzTz-AP5KHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
        "window_current = Window.rowsBetween(0,0)\n",
        "df.withColumn(\"Salary_currentoniy\", sum(\"Salary\").over(window_current)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6fW_lRu57bD",
        "outputId": "3fe0a0fa-55b6-486d-8b79-f9c12dd152b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+----------+------+------------------+\n",
            "|EmpID|  Name|Department|Salary|Salary_currentoniy|\n",
            "+-----+------+----------+------+------------------+\n",
            "|    1|  John|     Sales|  3000|              3000|\n",
            "|    2|  Jane|   Finance|  4000|              4000|\n",
            "|    3|  Mike|     Sales|  3500|              3500|\n",
            "|    4| Alice|   Finance|  3800|              3800|\n",
            "|    5|   Bob|        IT|  4500|              4500|\n",
            "|    6|   Tom|     Sales|  3700|              3700|\n",
            "|    7| Jerry|   Finance|  4200|              4200|\n",
            "|    8|   Sam|        IT|  4700|              4700|\n",
            "|    9| Steve|     Sales|  3100|              3100|\n",
            "|   10|Rachel|        IT|  4600|              4600|\n",
            "+-----+------+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mat1nAev7e10",
        "outputId": "7194ba64-6cb4-4e9c-bd22-34a8352166ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(IntegerType())\n",
        "def subject_count(subjects):\n",
        "  return len(subjects)\n",
        "\n",
        "df.withColumn(\"subject_count\", subject_count(df[\"subjects\"])).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ffa3Vay7n5m",
        "outputId": "82c66b9b-7190-4b1e-8fa9-fd71fefb1b1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "|id |name   |salary|subjects                |address                    |subject_count|\n",
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|2            |\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |1            |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|3            |\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |1            |\n",
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(\"integer\")\n",
        "def subject_count(subjects):\n",
        "  return subjects.apply(len)\n",
        "\n",
        "df.withColumn(\"subject_count\", subject_count(df[\"subjects\"])).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4pUk0AT_pTI",
        "outputId": "674f3d1d-1ad2-4908-9559-f6d02e749db1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "|id |name   |salary|subjects                |address                    |subject_count|\n",
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|2            |\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |1            |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|3            |\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |1            |\n",
            "+---+-------+------+------------------------+---------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark Use Case Activities\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col, rank, avg, udf, pandas_udf\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "\n",
        "employees_data = [\n",
        "    (1, \"Alice\", \"HR\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"IT\", 4500),\n",
        "    (5, \"Eve\", \"Finance\", 5000),\n",
        "    (6, \"Frank\", \"Finance\", 4800),\n",
        "]\n",
        "\n",
        "employees_df = spark.createDataFrame(employees_data, [\"id\", \"name\", \"department\", \"salary\"])\n",
        "employees_df.show()\n",
        "\n",
        "departments_data = [\n",
        "    (\"HR\", \"New York\"),\n",
        "    (\"IT\", \"San Francisco\"),\n",
        "    (\"Finance\", \"Chicago\"),\n",
        "]\n",
        "\n",
        "departments_df = spark.createDataFrame(departments_data, [\"department\", \"location\"])\n",
        "departments_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYIb63q4AAju",
        "outputId": "ba6d0649-9e1f-44c4-8794-e464a99737b6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| id| name|department|salary|\n",
            "+---+-----+----------+------+\n",
            "|  1|Alice|        HR|  3000|\n",
            "|  2|  Bob|        IT|  4000|\n",
            "|  3|Cathy|        HR|  3500|\n",
            "|  4|David|        IT|  4500|\n",
            "|  5|  Eve|   Finance|  5000|\n",
            "|  6|Frank|   Finance|  4800|\n",
            "+---+-----+----------+------+\n",
            "\n",
            "+----------+-------------+\n",
            "|department|     location|\n",
            "+----------+-------------+\n",
            "|        HR|     New York|\n",
            "|        IT|San Francisco|\n",
            "|   Finance|      Chicago|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define a window partitioned by department and ordered by salary descending\n",
        "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Add rank column to employees DataFrame\n",
        "ranked_df = employees_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Show the ranked result\n",
        "ranked_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHOZA8RlB7aY",
        "outputId": "20b1f9e3-0af7-4bd9-ec4e-ab180eeacfe9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----+\n",
            "| id| name|department|salary|rank|\n",
            "+---+-----+----------+------+----+\n",
            "|  5|  Eve|   Finance|  5000|   1|\n",
            "|  6|Frank|   Finance|  4800|   2|\n",
            "|  3|Cathy|        HR|  3500|   1|\n",
            "|  1|Alice|        HR|  3000|   2|\n",
            "|  4|David|        IT|  4500|   1|\n",
            "|  2|  Bob|        IT|  4000|   2|\n",
            "+---+-----+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "from pyspark.sql.window import Window\n",
        "Window.partitionBy(\"department\")\n",
        "employees_df.withColumn(\"rank\", rank().over(window_spec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlhQD-sNFLYb",
        "outputId": "e2508add-72eb-44d7-c314-578c4a555d2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----+\n",
            "| id| name|department|salary|rank|\n",
            "+---+-----+----------+------+----+\n",
            "|  5|  Eve|   Finance|  5000|   1|\n",
            "|  6|Frank|   Finance|  4800|   2|\n",
            "|  3|Cathy|        HR|  3500|   1|\n",
            "|  1|Alice|        HR|  3000|   2|\n",
            "|  4|David|        IT|  4500|   1|\n",
            "|  2|  Bob|        IT|  4000|   2|\n",
            "+---+-----+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define the window spec partitioned by department\n",
        "window_spec_avg = Window.partitionBy(\"department\")\n",
        "\n",
        "# Add average salary column to employees DataFrame\n",
        "avg_salary_df = employees_df.withColumn(\"avg_salary\", avg(\"salary\").over(window_spec_avg))\n",
        "\n",
        "# Show result\n",
        "avg_salary_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivQDRTdoCwSp",
        "outputId": "98fac5ed-08f6-4207-894c-f09ff8f1ca2f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------+\n",
            "| id| name|department|salary|avg_salary|\n",
            "+---+-----+----------+------+----------+\n",
            "|  5|  Eve|   Finance|  5000|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|    4900.0|\n",
            "|  1|Alice|        HR|  3000|    3250.0|\n",
            "|  3|Cathy|        HR|  3500|    3250.0|\n",
            "|  2|  Bob|        IT|  4000|    4250.0|\n",
            "|  4|David|        IT|  4500|    4250.0|\n",
            "+---+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.window import Window\n",
        "Window.partitionBy(\"department\")\n",
        "employees_df.withColumn(\"avg_salary\", avg(\"salary\").over(window_spec_avg)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKAlb30jDmCj",
        "outputId": "714edc17-40e5-40fe-ce77-481b51ac7f35"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+----------+\n",
            "| id| name|department|salary|avg_salary|\n",
            "+---+-----+----------+------+----------+\n",
            "|  5|  Eve|   Finance|  5000|    4900.0|\n",
            "|  6|Frank|   Finance|  4800|    4900.0|\n",
            "|  1|Alice|        HR|  3000|    3250.0|\n",
            "|  3|Cathy|        HR|  3500|    3250.0|\n",
            "|  2|  Bob|        IT|  4000|    4250.0|\n",
            "|  4|David|        IT|  4500|    4250.0|\n",
            "+---+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# complex data set"
      ],
      "metadata": {
        "id": "0RFzDweAhZHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (\"John\", [\"Python\", \"Java\"]),\n",
        "    (\"Jane\", [\"SQL\", \"R\", \"Scala\"]),\n",
        "    (\"Mike\", [])\n",
        "]\n",
        "columns = [\"Name\", \"Skills\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z32xqvwAhV8e",
        "outputId": "00287b4c-54ee-4651-ac33-fd3f3aded29c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|Name|Skills         |\n",
            "+----+---------------+\n",
            "|John|[Python, Java] |\n",
            "|Jane|[SQL, R, Scala]|\n",
            "|Mike|[]             |\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_exploded = df.withColumn(\"Skills_Exploded\", explode(\"Skills\"))\n",
        "df_exploded.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5l59B2kLFP",
        "outputId": "d44a421f-f3bf-4aa8-f6d0-4a8cde6eb17b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+---------------+\n",
            "|Name|Skills         |Skills_Exploded|\n",
            "+----+---------------+---------------+\n",
            "|John|[Python, Java] |Python         |\n",
            "|John|[Python, Java] |Java           |\n",
            "|Jane|[SQL, R, Scala]|SQL            |\n",
            "|Jane|[SQL, R, Scala]|R              |\n",
            "|Jane|[SQL, R, Scala]|Scala          |\n",
            "+----+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")"
      ],
      "metadata": {
        "id": "0ikC0veOkWK3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = spark.sql(\"\"\"\n",
        "    select name, skill\n",
        "    from people\n",
        "    lateral view explode(skills) as skill\n",
        "\"\"\")\n",
        "a.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfh1aXYdlhVf",
        "outputId": "9d354804-e9c7-406e-f8a9-e866229022ef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|name|skill |\n",
            "+----+------+\n",
            "|John|Python|\n",
            "|John|Java  |\n",
            "|Jane|SQL   |\n",
            "|Jane|R     |\n",
            "|Jane|Scala |\n",
            "+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ProductA\", \"Jan\", 100),\n",
        "    (\"ProductA\", \"Feb\", 150),\n",
        "    (\"ProductA\", \"Mar\", 120),\n",
        "    (\"ProductB\", \"Jan\", 200),\n",
        "    (\"ProductB\", \"Feb\", 230),\n",
        "    (\"ProductB\", \"Mar\", 210),\n",
        "]\n",
        "columns = [\"Product\", \"Month\", \"Sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7wqtaJdmtOx",
        "outputId": "cb89692c-ceae-43a3-8b0a-a0b1f0e71700"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df = df.groupBy(\"Product\").pivot(\"Month\").sum(\"Sales\")\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjbmxynTybts",
        "outputId": "b67af871-7e4b-43fe-fdd0-671b226e1ea8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+---+\n",
            "| Product|Feb|Jan|Mar|\n",
            "+--------+---+---+---+\n",
            "|ProductB|230|200|210|\n",
            "|ProductA|150|100|120|\n",
            "+--------+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Product\").pivot(\"Month\").sum(\"sales\")\n",
        "\n",
        "unpivot_df = pivot_df.selectExpr(\n",
        "    \"Product\",\n",
        "    \"stack(3, 'Jan', Jan, 'Feb', Feb, 'Mar', Mar) as (Month, Sales)\"\n",
        ")\n",
        "unpivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDgf0JUyyhwC",
        "outputId": "643231f2-d8e8-4a2a-9cf8-fdb73f01ba06"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|ProductB|  Jan|  200|\n",
            "|ProductB|  Feb|  230|\n",
            "|ProductB|  Mar|  210|\n",
            "|ProductA|  Jan|  100|\n",
            "|ProductA|  Feb|  150|\n",
            "|ProductA|  Mar|  120|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  RDD"
      ],
      "metadata": {
        "id": "pg7DeB4c_xgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Stop any existing SparkContext before creating a new one\n",
        "try:\n",
        "    sc.stop()\n",
        "except Exception as e:\n",
        "    print(f\"No existing SparkContext to stop or an error occurred: {e}\")\n",
        "    pass # No existing SparkContext\n",
        "\n",
        "\n",
        "conf = SparkConf().setAppName(\"mysparkapp\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "rdd = sc.parallelize({1, 2, 3, 4, 5})\n",
        "\n",
        "rdd_mapped = rdd.map(lambda x: (x, x**2))\n",
        "\n",
        "print(rdd_mapped.collect())\n",
        "\n",
        "# Apply the filter to the second element of the tuple (the squared value)\n",
        "rdd_filtered = rdd_mapped.filter(lambda x: x[1] % 2 == 0)\n",
        "print(rdd_filtered.collect())\n",
        "\n",
        "\n",
        "rdd_redused = rdd_filtered.reduceByKey(lambda x, y: x + y)\n",
        "print(rdd_filtered.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHvb1E0HZiB9",
        "outputId": "a066eca5-8f37-4ef0-890f-a99446fe944b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
            "[(2, 4), (4, 16)]\n",
            "[(2, 4), (4, 16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "#  Stop any existing SparkContext\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "rdd_mapped = rdd.map(lambda x: x*2)\n",
        "print(rdd_mapped.collect())\n",
        "\n",
        "rdd_filtered = rdd.filter(lambda x: x%2 == 0)\n",
        "print(rdd_filtered.collect())\n",
        "\n",
        "rdd_redused = rdd_filtered.reduceByKey(lambda x, y: x + y)\n",
        "print(rdd_filtered.collect())\n",
        "\n",
        "rdd = sc.parallelize([4, 5, 6, ])\n",
        "\n",
        "rdd_flat = rdd.flatMap(lambda x: range(x, x + 3))\n",
        "print(rdd_flat.collect())\n",
        "\n",
        "rdd = sc.parallelize([\"hellow\", \"world\"])\n",
        "\n",
        "rdd_flat = rdd.flatMap(lambda x: list(x))\n",
        "print(rdd_flat.collect())\n",
        "\n",
        "rdd = sc.parallelize([[1,2], [3,4], [5]])\n",
        "rdd_flat = rdd.flatMap(lambda x: x)\n",
        "print(rdd_flat.collect())\n",
        "\n",
        "\n",
        "sc.stop()  #  Stop after work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLPFSBk5pC8S",
        "outputId": "86a733cf-8db2-4c62-c5eb-8e0c9d173635"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10]\n",
            "[2, 4]\n",
            "[2, 4]\n",
            "[4, 5, 6, 5, 6, 7, 6, 7, 8]\n",
            "['h', 'e', 'l', 'l', 'o', 'w', 'w', 'o', 'r', 'l', 'd']\n",
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "#  Stop any existing SparkContext\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# input: 1,2,3,4,5\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "rdd_group = rdd.groupBy(lambda x:\"even\" if x % 2 == 0 else \"odd\")\n",
        "print([(key, list(value)) for (key, value) in rdd_group.collect()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mtP9V53yOwN",
        "outputId": "f8f052d5-0215-46e2-d090-1eb81ec43985"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('even', [2, 4]), ('odd', [1, 3, 5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "rdd_group = rdd.groupBy(lambda x:\"even\" if x % 2 == 0 else \"odd\")\n",
        "print([(key, list(value)) for (key, value) in rdd_group.collect()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQQz1NV1tXOM",
        "outputId": "aec1fda0-3e39-45ca-8e0c-e16db1802788"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('even', [2, 4]), ('odd', [1, 3, 5])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"John\", \"HR\", 5000),\n",
        "    (2, \"Jane\", \"IT\", 8000),\n",
        "    (3, \"Mike\", \"IT\", 6000),\n",
        "    (4, \"Sara\", \"Finance\", 7000),\n",
        "    (5, \"David\", \"HR\", 5500)\n",
        "]\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNRILRr26DQz",
        "outputId": "887439e3-2915-4056-b602-550ba1e04bb1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Name\", \"salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99UM5s_C70RQ",
        "outputId": "d1845412-9b77-48b1-b7a8-2ed2311f5149"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|salary|\n",
            "+-----+------+\n",
            "| John|  5000|\n",
            "| Jane|  8000|\n",
            "| Mike|  6000|\n",
            "| Sara|  7000|\n",
            "|David|  5500|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"salary\"] > 6000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg0MWI7q7-KP",
        "outputId": "71862da5-2c6d-41e8-9186-0a0fd26a325b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+------+\n",
            "| ID|Name|Department|Salary|\n",
            "+---+----+----------+------+\n",
            "|  2|Jane|        IT|  8000|\n",
            "|  4|Sara|   Finance|  7000|\n",
            "+---+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"Bonus\", df[\"salary\"] * 0.1)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSOXon7h8Q1u",
        "outputId": "d696cc55-5112-49f0-9e31-b4b0cbfa4b4a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+-----+\n",
            "|EmpID| Name|Department|Salary|Bonus|\n",
            "+-----+-----+----------+------+-----+\n",
            "|    1|Alice|     Sales|  3000|300.0|\n",
            "|    2|  Bob|        IT|  4000|400.0|\n",
            "|    3|Cathy|        HR|  3500|350.0|\n",
            "|    4|David|     Sales|  4500|450.0|\n",
            "|    5|  Eva|        IT|  4200|420.0|\n",
            "+-----+-----+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"Bonus\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc42fSL98wN5",
        "outputId": "58a6c986-071b-48cd-f2ae-0aabed52130d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed(\"salary\", \"salary_after_tax\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLkTDgy780ar",
        "outputId": "a5996aa7-bbe1-4a25-a0ac-e3409035e642"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+----------------+\n",
            "| ID| Name|Department|salary_after_tax|\n",
            "+---+-----+----------+----------------+\n",
            "|  1| John|        HR|            5000|\n",
            "|  2| Jane|        IT|            8000|\n",
            "|  3| Mike|        IT|            6000|\n",
            "|  4| Sara|   Finance|            7000|\n",
            "|  5|David|        HR|            5500|\n",
            "+---+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df.groupBy(\"Department\").agg(avg(\"salary_after_tax\").alias(\"Avg_Salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVgtbzxi9Enp",
        "outputId": "3f1966d1-f1f8-4017-eb98-787281085501"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|Avg_Salary|\n",
            "+----------+----------+\n",
            "|        HR|    5250.0|\n",
            "|        IT|    7000.0|\n",
            "|   Finance|    7000.0|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df[\"salary_after_tax\"].desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhhLfv-Y9kCr",
        "outputId": "8356adfa-5789-4002-9aa4-9e6d5ee403cb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+----------------+\n",
            "| ID| Name|Department|salary_after_tax|\n",
            "+---+-----+----------+----------------+\n",
            "|  2| Jane|        IT|            8000|\n",
            "|  4| Sara|   Finance|            7000|\n",
            "|  3| Mike|        IT|            6000|\n",
            "|  5|David|        HR|            5500|\n",
            "|  1| John|        HR|            5000|\n",
            "+---+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df[\"salary_after_tax\"].asc()).show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lowCXrbs9uUH",
        "outputId": "8b5951c4-f476-437f-e1de-2a9a624dc90c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+----------------+\n",
            "| ID| Name|Department|salary_after_tax|\n",
            "+---+-----+----------+----------------+\n",
            "|  1| John|        HR|            5000|\n",
            "|  5|David|        HR|            5500|\n",
            "|  3| Mike|        IT|            6000|\n",
            "+---+-----+----------+----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = df.collect()\n",
        "\n",
        "for row in data_list:\n",
        "    print((row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSaHAk4J947W",
        "outputId": "534b084f-8cdf-4171-f68f-69a275697321"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(ID=1, Name='John', Department='HR', salary_after_tax=5000)\n",
            "Row(ID=2, Name='Jane', Department='IT', salary_after_tax=8000)\n",
            "Row(ID=3, Name='Mike', Department='IT', salary_after_tax=6000)\n",
            "Row(ID=4, Name='Sara', Department='Finance', salary_after_tax=7000)\n",
            "Row(ID=5, Name='David', Department='HR', salary_after_tax=5500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA SET"
      ],
      "metadata": {
        "id": "laMKljdV-vvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col # Import col for DataFrame expressions\n",
        "\n",
        "rdd = sc.parallelize([\n",
        "    Row(name=\"Alice\", age=25,),\n",
        "    Row(name=\"Bob\", age=30, )])\n",
        "data = spark.createDataFrame(rdd)\n",
        "\n",
        "# Filter using DataFrame syntax (using col)\n",
        "filtered_data = data.filter(col(\"age\") > 25)\n",
        "\n",
        "# Show the filtered data\n",
        "filtered_data.select(\"name\")\n",
        "selected_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXFX9maS-IYD",
        "outputId": "b8721ddc-8a45-4564-970d-ce8e8535ab78"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Bob|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "#  Stop any existing SparkContext\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "\n",
        "from pyspark.sql import Row\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Initialize SparkConf and SparkContext\n",
        "# conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "# sc = SparkContext(conf=conf)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "rdd = sc.parallelize([Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30)])\n",
        "\n",
        "# Correct way in PySpark\n",
        "dataset = spark.createDataFrame(rdd)\n",
        "\n",
        "# Apply filter and select\n",
        "filtered = dataset.filter(dataset.age > 25).select(\"name\")\n",
        "filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "Y1LOD2PaBLiR",
        "outputId": "3a71f03d-2b46-4348-a1c9-061e62cff691"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-76-2048627527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Correct way in PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Apply filter and select\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             )\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_jconf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"JavaObject\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;34m\"\"\"Accessor for the JVM SQL-specific configurations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"SparkSession\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Initialize SparkConf and SparkContext\n",
        "# conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "# sc = SparkContext(conf=conf)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "sc = spark.SparkContext\n",
        "\n",
        "rdd = sc.parallelize([Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30)])\n",
        "dataset = spark.createDataset(rdd)\n",
        "dataset.filter(lambda x: x.age > 25)\n",
        "dataset.select(\"name\")\n",
        "selectd_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_lDa6bipBlQO",
        "outputId": "ff7ccfc8-d12b-4e9f-d84a-1324f56c9a15"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SparkSession' object has no attribute 'SparkContext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-65-2173669778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrameOperations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Alice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'SparkContext'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col # Import col for DataFrame expressions\n",
        "\n",
        "rdd = sc.parallelize([\n",
        "    Row(name=\"Alice\", age=25,),\n",
        "    Row(name=\"Bob\", age=30, )])\n",
        "data = spark.createDataFrame(rdd)\n",
        "\n",
        "# Filter using DataFrame syntax (using col)\n",
        "filtered_data = data.filter(col(\"age\") > 25)\n",
        "\n",
        "# Show the filtered data\n",
        "filtered_data.select(\"name\")\n",
        "selected_df.show()"
      ],
      "metadata": {
        "id": "dGlXiIniCPtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQLBasics\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"Sales\", 4500),\n",
        "    (5, \"Eva\", \"IT\", 4200)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLCYzWCCCRRw",
        "outputId": "a28c8dae-a8c0-4f86-943f-da36a98784e3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "strSQL=\"select * from employees\"\n",
        "sql_result=spark.sql(strSQL)\n",
        "sql_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K442BWZ-DP2S",
        "outputId": "92571298-85fd-41e5-f93c-9469f0c8097e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=df.rdd\n",
        "print(\"RDD Example:\", rdd.map(lambda x: (x.Name, x.Salary)).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU4bNHz5EOAy",
        "outputId": "d06e485e-9402-44b8-e8dd-5389ce18bb44"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Example: [('Alice', 3000), ('Bob', 4000), ('Cathy', 3500), ('David', 4500), ('Eva', 4200)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "caVztFuXExje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA FORMAT"
      ],
      "metadata": {
        "id": "Yb9dUWXoEzBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Removed explicit SparkContext creation and stopping logic\n",
        "# from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# try:\n",
        "#     sc.stop()\n",
        "# except:\n",
        "#     pass\n",
        "\n",
        "# conf = SparkConf().setAppName(\"MySparkApp\").setMaster(\"local[*]\")\n",
        "# sc = SparkContext(conf=conf)\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "# Use the existing 'spark' SparkSession\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "9k3kdoJyEyMf",
        "outputId": "81d5d7bd-9c29-481e-8a62-fb6261ff90c8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-83-1788076555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"EmpID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Salary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Use the existing 'spark' SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    950\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             )\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_jconf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"JavaObject\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;34m\"\"\"Accessor for the JVM SQL-specific configurations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"SparkSession\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", 3000),\n",
        "    (2, \"Bob\", 4000)\n",
        "]\n",
        "columns = [\"id\", \"name\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "V6ARaGFOFSn2",
        "outputId": "c61bdb6c-ec2a-4df8-a6d3-802979c1a293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-84-3119855628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    950\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             )\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0minfer_dict_as_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferDictAsStruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_jconf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"JavaObject\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;34m\"\"\"Accessor for the JVM SQL-specific configurations\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"SparkSession\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2117.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Recreate Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Restarted Session\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define data and columns again\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "UuKXTMOeF2y6",
        "outputId": "ce4d3b4b-8e47-4186-f6df-e3a3f304b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").json(\"/content/json_data\")"
      ],
      "metadata": {
        "id": "nexR7IP7F_Kq"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/json_data"
      ],
      "metadata": {
        "id": "-DfTer62GQyi",
        "outputId": "6611b9a0-1064-4a93-c73d-15fa286b04e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-6a73a00e-a06a-469b-94d5-6b3df5d67c04-c000.json  _SUCCESS\n",
            "part-00001-6a73a00e-a06a-469b-94d5-6b3df5d67c04-c000.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/json_data/part-00000-8c9ad97f-903c-48d4-8f08-4cf65a638dc3-c000.json"
      ],
      "metadata": {
        "id": "r71-RvZKI6MV",
        "outputId": "2a5fa0a2-76f7-413d-f2b9-093f4af1a926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"EmpID\":1,\"Name\":\"Alice\",\"Department\":\"Sales\",\"Salary\":3000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/json_data/part-00001-8c9ad97f-903c-48d4-8f08-4cf65a638dc3-c000.json"
      ],
      "metadata": {
        "id": "60iCtAiiJFAz",
        "outputId": "f98883f8-af21-4ecb-cf7f-f63c1df9bbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"EmpID\":2,\"Name\":\"Bob\",\"Department\":\"IT\",\"Salary\":4000}\n",
            "{\"EmpID\":3,\"Name\":\"Cathy\",\"Department\":\"HR\",\"Salary\":3500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_df= spark.read.json(\"/content/json_data/\")\n",
        "json_df.show()"
      ],
      "metadata": {
        "id": "uuAd6hfDGVeu",
        "outputId": "0316c0fe-086e-4234-fb6f-289f42f4227b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+\n",
            "|Department|EmpID| Name|Salary|\n",
            "+----------+-----+-----+------+\n",
            "|        IT|    2|  Bob|  4000|\n",
            "|        HR|    3|Cathy|  3500|\n",
            "|     Sales|    1|Alice|  3000|\n",
            "+----------+-----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strpath=\"/content/parquet_data/\"\n",
        "df.write.mode(\"overwrite\").option(\"hearder\", \"true\").csv(strpath)"
      ],
      "metadata": {
        "id": "ZF_OMtNuGjLa"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df = spark.read.option(\"header\", \"true\").csv(strpath)\n",
        "csv_df.show()"
      ],
      "metadata": {
        "id": "Wl__MFEVG2iy",
        "outputId": "f3396cbe-e6fe-402e-cbe4-e649796e6d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+----+\n",
            "|  2|  Bob| IT|4000|\n",
            "+---+-----+---+----+\n",
            "|  3|Cathy| HR|3500|\n",
            "+---+-----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "7iInkKQeHq45",
        "outputId": "76a2bc0b-23c6-4dc3-ab99-e719c8da16d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").json(\"/content/json_data\")"
      ],
      "metadata": {
        "id": "Wp5xXeocHuwR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/json_data"
      ],
      "metadata": {
        "id": "TwnZ8gPhH0PX",
        "outputId": "6ea4f690-a047-4a70-b791-cdd9f67306ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-8c9ad97f-903c-48d4-8f08-4cf65a638dc3-c000.json  _SUCCESS\n",
            "part-00001-8c9ad97f-903c-48d4-8f08-4cf65a638dc3-c000.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_df= spark.read.json(\"/content/json_data/\")\n",
        "json_df.show()"
      ],
      "metadata": {
        "id": "zONd8yM8H04F",
        "outputId": "c59c7507-213a-4507-f3cc-49922f14945f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+\n",
            "|Department|EmpID| Name|Salary|\n",
            "+----------+-----+-----+------+\n",
            "|        IT|    2|  Bob|  4000|\n",
            "|        HR|    3|Cathy|  3500|\n",
            "|     Sales|    1|Alice|  3000|\n",
            "+----------+-----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strpath=\"/content/parquet_data\"\n",
        "df.write.mode(\"overwrite\").option(\"hearder\", \"true\").csv(strpath)"
      ],
      "metadata": {
        "id": "DoJU5fyOH33w"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df = spark.read.option(\"header\", \"true\").csv(strpath)\n",
        "csv_df.show()"
      ],
      "metadata": {
        "id": "A-TE-H43H7RY",
        "outputId": "937080b5-ed05-4389-b287-8745f920a7f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+----+\n",
            "|  2|  Bob| IT|4000|\n",
            "+---+-----+---+----+\n",
            "|  3|Cathy| HR|3500|\n",
            "+---+-----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQXf6xtBH-sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}