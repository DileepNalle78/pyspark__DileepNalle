{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6l06o0P0q_sy",
        "Pdks7cnOtJ0N",
        "G3HGEg7nN5CN",
        "pJHJcDwioVYb"
      ],
      "authorship_tag": "ABX9TyNpEEBwYXGz/E9QbYUugbBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/sales.csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINUX BASIC\n",
        "\n"
      ],
      "metadata": {
        "id": "6l06o0P0q_sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tuaBrsBpsJu",
        "outputId": "7dadcd60-fcf5-4aeb-b4e7-6a0ca973685b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgDvLNuXqFAf",
        "outputId": "04a08de4-55d2-4854-db0b-d51314718e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 6d3647f0c437 6.1.123+ #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frGDZ2CAqP11",
        "outputId": "939fbe66-dba8-45fe-925e-5ba4d99b3ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3wwzlOUqgLT",
        "outputId": "ce870f83-16e8-412e-e007-2fd243ec2e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Basics"
      ],
      "metadata": {
        "id": "Pdks7cnOtJ0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHmBGEZqjHB",
        "outputId": "4add7874-0fd3-47ea-c7f9-b08f0897760e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2k8unOuFqE",
        "outputId": "b37bf375-cb4d-4d53-b6fa-21e9b3cda924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "KcvOfQ2yuJw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
      ],
      "metadata": {
        "id": "kU96jmpouaog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create DataFrame\n",
        "data = [(\"HEllo\",\"world\")]\n",
        "columns = [\"world1\",\"world2\"]\n",
        "\n",
        "df = spark.createDataFrame"
      ],
      "metadata": {
        "id": "kRQKHS3OvGMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark"
      ],
      "metadata": {
        "id": "_C91JB-JwEbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data frame"
      ],
      "metadata": {
        "id": "TQ8A_cqzw0ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Name\",\"Department\",\"Salary\"]\n",
        "data = [\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Jane\", \"Finance\", 4000),\n",
        "    (\"Mike\", \"Sales\", 3500),\n",
        "    (\"Alice\", \"Finance\", 3800),\n",
        "    (\"Bob\", \"IT\", 4500)\n",
        "]\n",
        "\n",
        "df=spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDVItNmMwG7p",
        "outputId": "8965c74c-a356-48c1-b3a9-53bb8931128c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter: employees with salary > 3500\n",
        "df_filtered = df.filter(df.Salary > 3500)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2qJQet_xtL2",
        "outputId": "db0a44ee-0834-4ba6-a744-6877e331d78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| Jane|   Finance|  4000|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped = df.groupBy(\"Department\").avg(\"salary\")\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4u6sPCiyDdF",
        "outputId": "fd802356-95e1-44d6-a2ba-4e331fa39f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new comumn: salary with new bones (10%)\n",
        "from pyspark.sql.functions import  col\n",
        "exp=col(\"Salary\") * 1.1\n",
        "df_with_bonus = df.withColumn(\"Salary_10%_Bonus\",exp)\n",
        "df_with_bonus.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL8hFjFOzpEq",
        "outputId": "dce3c884-6352-4eb0-90e2-ff86b1654525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+------------------+\n",
            "| Name|Department|Salary|  Salary_10%_Bonus|\n",
            "+-----+----------+------+------------------+\n",
            "| John|     Sales|  3000|3300.0000000000005|\n",
            "| Jane|   Finance|  4000|            4400.0|\n",
            "| Mike|     Sales|  3500|3850.0000000000005|\n",
            "|Alice|   Finance|  3800|            4180.0|\n",
            "|  Bob|        IT|  4500|            4950.0|\n",
            "+-----+----------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,upper, lower, concat_ws,length,when\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnFuUJPV1I07",
        "outputId": "26bb23cb-11a5-4866-a190-ff25c8c2f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_upper = df.withColumn(\"Name_upper\",upper(col(\"Name\")))\n",
        "df_lower = df.withColumn(\"Name_lower\", lower(col(\"Name\")))\n",
        "df_upper.show()\n",
        "df_lower.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbRaESgY2Uzk",
        "outputId": "d4d8aa80-c1e9-440d-fe5a-7174e73c2b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_upper|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      JOHN|\n",
            "| Jane|   Finance|  4000|      JANE|\n",
            "| Mike|     Sales|  3500|      MIKE|\n",
            "|Alice|   Finance|  3800|     ALICE|\n",
            "|  Bob|        IT|  4500|       BOB|\n",
            "+-----+----------+------+----------+\n",
            "\n",
            "+-----+----------+------+----------+\n",
            "| Name|Department|Salary|Name_lower|\n",
            "+-----+----------+------+----------+\n",
            "| John|     Sales|  3000|      john|\n",
            "| Jane|   Finance|  4000|      jane|\n",
            "| Mike|     Sales|  3500|      mike|\n",
            "|Alice|   Finance|  3800|     alice|\n",
            "|  Bob|        IT|  4500|       bob|\n",
            "+-----+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_concat = df.withColumn(\"Full_length_Name\",concat_ws(\" \",col(\"Name\"),col(\"Department\")))\n",
        "df_concat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQeJZZeT3Q0S",
        "outputId": "f7276e27-dca8-4a71-b026-0198df27ec03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+----------------+\n",
            "| Name|Department|Salary|Full_length_Name|\n",
            "+-----+----------+------+----------------+\n",
            "| John|     Sales|  3000|      John Sales|\n",
            "| Jane|   Finance|  4000|    Jane Finance|\n",
            "| Mike|     Sales|  3500|      Mike Sales|\n",
            "|Alice|   Finance|  3800|   Alice Finance|\n",
            "|  Bob|        IT|  4500|          Bob IT|\n",
            "+-----+----------+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# String leangth of Names in New DF\n",
        "df_length = df.withColumn(\"Name_length\",length(col(\"Name\")))\n",
        "df_length.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Idva0z_3XOK",
        "outputId": "b429794e-7e7f-4abc-e6c2-485c9875d73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+-----------+\n",
            "| Name|Department|Salary|Name_length|\n",
            "+-----+----------+------+-----------+\n",
            "| John|     Sales|  3000|          4|\n",
            "| Jane|   Finance|  4000|          4|\n",
            "| Mike|     Sales|  3500|          4|\n",
            "|Alice|   Finance|  3800|          5|\n",
            "|  Bob|        IT|  4500|          3|\n",
            "+-----+----------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conditional column (salary Category)\n",
        "df_conditional = df.withColumn(\n",
        "    \"Salary_Category\",\n",
        "    when(col(\"Salary\") >= 4000, \"High\")\n",
        "    .when(col(\"Salary\") >= 3500, \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "df_conditional.show()\n"
      ],
      "metadata": {
        "id": "g3SiP6aY33ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the column (Salary --> Base_Salary)\n",
        "df_renamed = df.withColumnRenamed(\"Salary\",\"Base_salary\")\n",
        "df_renamed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eggHzJgk5Amy",
        "outputId": "eb6cc9c1-97f9-42a2-a151-a2bc16630a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+-----------+\n",
            "| Name|Department|Base_salary|\n",
            "+-----+----------+-----------+\n",
            "| John|     Sales|       3000|\n",
            "| Jane|   Finance|       4000|\n",
            "| Mike|     Sales|       3500|\n",
            "|Alice|   Finance|       3800|\n",
            "|  Bob|        IT|       4500|\n",
            "+-----+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlVZodkHLKh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advance"
      ],
      "metadata": {
        "id": "G3HGEg7nN5CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark =SparkSession.builder.appName(\"Basics\").getOrCreate()\n",
        "columns = [\"Name\",\"Department\",\"Salary\"]\n",
        "data = [\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Jane\", \"Finance\", 4000),\n",
        "    (\"Mike\", \"Sales\", 3500),\n",
        "    (\"Alice\", \"Finance\", 3800),\n",
        "    (\"Bob\", \"IT\", 4500)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iH22Se56Jug",
        "outputId": "36dd02ce-c751-4d66-9395-89728fc9df8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------+\n",
            "| Name|Department|Salary|\n",
            "+-----+----------+------+\n",
            "| John|     Sales|  3000|\n",
            "| Jane|   Finance|  4000|\n",
            "| Mike|     Sales|  3500|\n",
            "|Alice|   Finance|  3800|\n",
            "|  Bob|        IT|  4500|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg({\"Salary\":\"avg\"}).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFvOY1PIOPHI",
        "outputId": "60470295-9716-47ce-ee2b-e11d695cb67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").avg(\"Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAPqHIMwPLCn",
        "outputId": "39d422f5-1a1f-4258-9345-a6bff005af58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3250.0|\n",
            "|   Finance|     3900.0|\n",
            "|        IT|     4500.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").agg({\"Salary\":\"avg\", \"Salary\":\"min\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9DCIGdZPla6",
        "outputId": "084faa4d-ee13-46f6-bdce-61d9572fe63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|min(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       3000|\n",
            "|   Finance|       3800|\n",
            "|        IT|       4500|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.groupBy(\"Department\") \\\n",
        "   .agg(F.avg(\"Salary\").alias(\"Average_Salary\"),\n",
        "        F.max(\"Salary\").alias(\"Max_Salary\"),\n",
        "        F.min(\"Salary\").alias(\"Min_Salary\")) \\\n",
        "   .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wag3ZpT7P-qW",
        "outputId": "b0f8aaab-351c-454f-d37a-1346879867a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+----------+----------+\n",
            "|Department|Average_Salary|Max_Salary|Min_Salary|\n",
            "+----------+--------------+----------+----------+\n",
            "|     Sales|        3250.0|      3500|      3000|\n",
            "|   Finance|        3900.0|      4000|      3800|\n",
            "|        IT|        4500.0|      4500|      4500|\n",
            "+----------+--------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for department info\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\")\n",
        "]\n",
        "dept_columns = [\"Department\", \"Location\"]\n",
        "\n",
        "dept_df=spark.createDataFrame(dept_data, dept_columns)\n",
        "dept_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-ax8hAJR3NS",
        "outputId": "6de2ceb9-fffb-45bc-c5bb-7adb0676a213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for department info\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\")\n",
        "]\n",
        "dept_columns = [\"Department\", \"Location\"]\n",
        "\n",
        "dept_df=spark.createDataFrame(dept_data, dept_columns)\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGMD86-cTwdS",
        "outputId": "cea8dd98-8a3d-453c-ab81-d088ff1ba314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "joined_df = df.join(dept_df, on=\"Department\", how=\"inner\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKa960E3TGTL",
        "outputId": "a1ea3c2a-ad26-4d12-f2c2-eae9b945cee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------+----------+\n",
            "|Department| Name|Salary|  Location|\n",
            "+----------+-----+------+----------+\n",
            "|   Finance| Jane|  4000|Building B|\n",
            "|   Finance|Alice|  3800|Building B|\n",
            "|        IT|  Bob|  4500|Building C|\n",
            "|     Sales| John|  3000|Building A|\n",
            "|     Sales| Mike|  3500|Building A|\n",
            "+----------+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXLBxOjjT8ti",
        "outputId": "af0261dc-dc6a-4cb9-8795-b39a4e0cc699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Employee DataFrame\n",
        "emp_data = [\n",
        "    (1, \"John\", \"Sales\", 3000),\n",
        "    (2, \"Jane\", \"Finance\", 4000),\n",
        "    (3, \"Mike\", \"Sales\", 3500),\n",
        "    (4, \"Alice\", \"HR\", 3800),\n",
        "    (5, \"Bob\", \"IT\", 4500),\n",
        "    (6, \"Sam\", \"Support\", 3200)\n",
        "]\n",
        "emp_cols = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_cols)\n",
        "\n",
        "# Department DataFrame\n",
        "dept_data = [\n",
        "    (\"Sales\", \"Building A\"),\n",
        "    (\"Finance\", \"Building B\"),\n",
        "    (\"IT\", \"Building C\"),\n",
        "    (\"Admin\", \"Building D\")\n",
        "]\n",
        "dept_cols = [\"Department\", \"Location\"]\n",
        "dept_df = spark.createDataFrame(dept_data, dept_cols)\n",
        "\n",
        "# Display both\n",
        "emp_df.show()\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KT6v2E4UGzU",
        "outputId": "95671345-a97b-460f-b7c2-ea29cec0c943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1| John|     Sales|  3000|\n",
            "|    2| Jane|   Finance|  4000|\n",
            "|    3| Mike|     Sales|  3500|\n",
            "|    4|Alice|        HR|  3800|\n",
            "|    5|  Bob|        IT|  4500|\n",
            "|    6|  Sam|   Support|  3200|\n",
            "+-----+-----+----------+------+\n",
            "\n",
            "+----------+----------+\n",
            "|Department|  Location|\n",
            "+----------+----------+\n",
            "|     Sales|Building A|\n",
            "|   Finance|Building B|\n",
            "|        IT|Building C|\n",
            "|     Admin|Building D|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMURZs9oVSA-",
        "outputId": "64f734a0-285f-4775-9152-a0da3a7c9982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+------+----------+\n",
            "|Department|EmpID|Name|Salary|  Location|\n",
            "+----------+-----+----+------+----------+\n",
            "|   Finance|    2|Jane|  4000|Building B|\n",
            "|        IT|    5| Bob|  4500|Building C|\n",
            "|     Sales|    1|John|  3000|Building A|\n",
            "|     Sales|    3|Mike|  3500|Building A|\n",
            "+----------+-----+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# left join\n",
        "emp_df.join(dept_df, on=\"Department\", how=\"left\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyWA9eNGVfUk",
        "outputId": "f2a75c4a-aa58-49cb-a104-667f8c2a12bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+----------+\n",
            "|Department|EmpID| Name|Salary|  Location|\n",
            "+----------+-----+-----+------+----------+\n",
            "|     Sales|    1| John|  3000|Building A|\n",
            "|     Sales|    3| Mike|  3500|Building A|\n",
            "|   Finance|    2| Jane|  4000|Building B|\n",
            "|        HR|    4|Alice|  3800|      NULL|\n",
            "|        IT|    5|  Bob|  4500|Building C|\n",
            "|   Support|    6|  Sam|  3200|      NULL|\n",
            "+----------+-----+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd5tYkmkVrs6",
        "outputId": "63678785-9822-4d6a-966b-f0700a486ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+------+----------+\n",
            "|Department|EmpID|Name|Salary|  Location|\n",
            "+----------+-----+----+------+----------+\n",
            "|     Sales|    3|Mike|  3500|Building A|\n",
            "|     Sales|    1|John|  3000|Building A|\n",
            "|   Finance|    2|Jane|  4000|Building B|\n",
            "|     Admin| NULL|NULL|  NULL|Building D|\n",
            "|        IT|    5| Bob|  4500|Building C|\n",
            "+----------+-----+----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df.join(dept_df, on=\"Department\", how=\"full\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd6lHzGIVzd-",
        "outputId": "95409f65-ca23-490d-c0fa-508a1b4f78de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------+----------+\n",
            "|Department|EmpID| Name|Salary|  Location|\n",
            "+----------+-----+-----+------+----------+\n",
            "|     Admin| NULL| NULL|  NULL|Building D|\n",
            "|   Finance|    2| Jane|  4000|Building B|\n",
            "|        HR|    4|Alice|  3800|      NULL|\n",
            "|        IT|    5|  Bob|  4500|Building C|\n",
            "|     Sales|    1| John|  3000|Building A|\n",
            "|     Sales|    3| Mike|  3500|Building A|\n",
            "|   Support|    6|  Sam|  3200|      NULL|\n",
            "+----------+-----+-----+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SALES.csv"
      ],
      "metadata": {
        "id": "pJHJcDwioVYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"sale_date\", StringType(), True),  # or TimestampType()\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Create data\n",
        "data = [\n",
        "    (1001, 101, 501, \"2025-07-10 08:23:00\", 3, 25.5),\n",
        "    (1002, 102, 502, \"2025-07-11 09:45:00\", 2, 15.0),\n",
        "    (1003, 103, 503, \"2025-07-12 10:15:00\", 1, 30.0),\n",
        "    (1004, 101, 504, \"2025-07-13 12:20:00\", 5, 25.5),\n",
        "    (1005, 105, 505, \"2025-07-14 14:35:00\", 10, 45.0),\n",
        "    (1006, 102, 506, \"2025-07-15 16:00:00\", 4, 15.0),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df = df.withColumn(\"sale_date\", F.to_timestamp(\"sale_date\"))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa1ZzS7cWIpR",
        "outputId": "8f9f43b5-3621-4e07-8cd1-fb66638c4a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|customer_id|          sale_date|quantity|price|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|   1001|       101|        501|2025-07-10 08:23:00|       3| 25.5|\n",
            "|   1002|       102|        502|2025-07-11 09:45:00|       2| 15.0|\n",
            "|   1003|       103|        503|2025-07-12 10:15:00|       1| 30.0|\n",
            "|   1004|       101|        504|2025-07-13 12:20:00|       5| 25.5|\n",
            "|   1005|       105|        505|2025-07-14 14:35:00|      10| 45.0|\n",
            "|   1006|       102|        506|2025-07-15 16:00:00|       4| 15.0|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Define schema for products\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Create product data\n",
        "products_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_products = spark.createDataFrame(products_data, schema=product_schema)\n",
        "df_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2UKYhzqZtn0",
        "outputId": "4169b7f1-95da-43d6-9dca-ccd8de97833d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|     category|\n",
            "+----------+------------+-------------+\n",
            "|       101|    Widget A|      Gadgets|\n",
            "|       102|    Widget B|      Gadgets|\n",
            "|       103|    Widget C|  Electronics|\n",
            "|       104|    Widget D|  Electronics|\n",
            "|       105|    Widget E|Home & Living|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "\n",
        "# Define schema\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"join_date\", StringType(), True),  # or use TimestampType\n",
        "])\n",
        "\n",
        "# Create data\n",
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 09:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\"),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_customers = spark.createDataFrame(customer_data, schema=customer_schema) \\\n",
        "    .withColumn(\"join_date\", F.to_timestamp(\"join_date\"))\n",
        "\n",
        "df_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE4dVCTBaJ45",
        "outputId": "1f1a501e-073b-48df-f1ab-3b3344baeb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------------+-------------------+\n",
            "|customer_id|customer_name|              email|          join_date|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "|        501|        Alice|  alice@example.com|2025-05-20 10:10:00|\n",
            "|        502|          Bob|    bob@example.com|2025-06-15 14:00:00|\n",
            "|        503|      Charlie|charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|        David|  david@example.com|2025-07-01 12:25:00|\n",
            "|        505|         Emma|   emma@example.com|2025-07-10 15:30:00|\n",
            "|        506|        Frank|  frank@example.com|2025-03-23 17:00:00|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df contains sales data with: product_id, quantity, price\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\")) \\\n",
        "  .groupBy(\"product_id\") \\\n",
        "  .agg(F.sum(\"revenue\").alias(\"total_revenue\")) \\\n",
        "  .orderBy(F.desc(\"total_revenue\")) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCQFKQG-ao5T",
        "outputId": "62ec255e-0f63-4888-ac6d-19be2c447e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|total_revenue|\n",
            "+----------+-------------+\n",
            "|       105|        450.0|\n",
            "|       101|        204.0|\n",
            "|       102|         90.0|\n",
            "|       103|         30.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Option 1: Use customer_id only\n",
        "df.groupBy(\"customer_id\") \\\n",
        "  .agg(F.sum(\"quantity\").alias(\"total_quantity\")) \\\n",
        "  .orderBy(F.desc(\"total_quantity\")) \\\n",
        "  .show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn2Qay-YbHdk",
        "outputId": "5f542244-7f44-41e3-ee92-2a2e058c5c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|total_quantity|\n",
            "+-----------+--------------+\n",
            "|        505|            10|\n",
            "|        504|             5|\n",
            "|        506|             4|\n",
            "|        501|             3|\n",
            "|        502|             2|\n",
            "|        503|             1|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join sales with products\n",
        "df_joined = df.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Then join with customers\n",
        "df_full = df_joined.join(df_customers, on=\"customer_id\", how=\"inner\")\n"
      ],
      "metadata": {
        "id": "_Wvk9iZLcXny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_full.groupBy(\"customer_name\") \\\n",
        "       .agg(F.sum(\"quantity\").alias(\"total_quantity\")) \\\n",
        "       .orderBy(F.desc(\"total_quantity\")) \\\n",
        "       .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SYG2RI0cgbV",
        "outputId": "7abae3dc-0981-4723-ff7f-1cdc9fcd2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|customer_name|total_quantity|\n",
            "+-------------+--------------+\n",
            "|         Emma|            10|\n",
            "|        David|             5|\n",
            "|        Frank|             4|\n",
            "|        Alice|             3|\n",
            "|          Bob|             2|\n",
            "|      Charlie|             1|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Make sure df_full includes: customer_id, customer_name, quantity, price\n",
        "df_full.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\")) \\\n",
        "       .groupBy(\"customer_name\") \\\n",
        "       .agg(F.avg(\"revenue\").alias(\"average_revenue\")) \\\n",
        "       .orderBy(F.desc(\"average_revenue\")) \\\n",
        "       .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hvkrKsrb2xb",
        "outputId": "5a878710-4584-4bb4-a76b-8a505a3ddf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+\n",
            "|customer_name|average_revenue|\n",
            "+-------------+---------------+\n",
            "|         Emma|          450.0|\n",
            "|        David|          127.5|\n",
            "|        Alice|           76.5|\n",
            "|        Frank|           60.0|\n",
            "|      Charlie|           30.0|\n",
            "|          Bob|           30.0|\n",
            "+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Add a 'revenue' column\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Extract year and month from 'sale_date'\n",
        "df_monthly = df_with_revenue.withColumn(\"year_month\", F.date_format(\"sale_date\", \"yyyy-MM\"))\n",
        "\n",
        "# Step 3: Group by 'year_month' and sum revenue\n",
        "df_monthly.groupBy(\"year_month\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_revenue\")) \\\n",
        "    .orderBy(\"year_month\") \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOvUu8becu7H",
        "outputId": "cb545e6c-b979-4837-c8aa-71c82e23aab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|year_month|total_revenue|\n",
            "+----------+-------------+\n",
            "|   2025-07|        774.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_sales_by_category = df.join(df_products, on=\"product_id\", how=\"inner\") \\\n",
        "    .groupBy(\"category\") \\\n",
        "    .agg(F.count(\"*\").alias(\"total_sales\")) \\\n",
        "    .orderBy(F.desc(\"total_sales\"))\n",
        "\n",
        "df_sales_by_category.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUgfpq_rdPB5",
        "outputId": "903ceefd-86b7-4f0a-ed8b-70eaa602c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|     category|total_sales|\n",
            "+-------------+-----------+\n",
            "|      Gadgets|          4|\n",
            "|  Electronics|          1|\n",
            "|Home & Living|          1|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Calculate revenue per row\n",
        "df_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with products to get product names\n",
        "df_with_names = df_revenue.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Group by product and sum revenue\n",
        "df_top_products = df_with_names.groupBy(\"product_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_revenue\")) \\\n",
        "    .orderBy(F.desc(\"total_revenue\")) \\\n",
        "    .limit(3)\n",
        "\n",
        "# Step 4: Show result\n",
        "df_top_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2J-vJURdY4H",
        "outputId": "42c16366-80c1-4f07-f420-f81e9df33fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|product_name|total_revenue|\n",
            "+------------+-------------+\n",
            "|    Widget E|        450.0|\n",
            "|    Widget A|        204.0|\n",
            "|    Widget B|         90.0|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joins\n",
        "\n",
        "# Join sales_data with product_data on product_id\n",
        "df_sales_with_products = df.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Select the relevant columns\n",
        "df_sales_with_products.select(\n",
        "    \"sale_id\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJLuvvA0dzJI",
        "outputId": "a4e24265-82f9-4ad3-cbde-30087d27bf3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category     |customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets      |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1004   |101       |Widget A    |Gadgets      |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1002   |102       |Widget B    |Gadgets      |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1006   |102       |Widget B    |Gadgets      |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|1003   |103       |Widget C    |Electronics  |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|1005   |105       |Widget E    |Home & Living|505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join sales with customer data on customer_id\n",
        "df_sales_with_customers = df.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Select columns for detailed sales info with customer name and email\n",
        "df_sales_with_customers.select(\n",
        "    \"sale_id\",\n",
        "    \"customer_id\",\n",
        "    \"customer_name\",\n",
        "    \"email\",\n",
        "    \"sale_date\",\n",
        "    \"product_id\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvZ70a8OlJo9",
        "outputId": "ee3918c1-006c-42ee-bdd3-b36db499e475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "|sale_id|customer_id|customer_name|email              |sale_date          |product_id|quantity|price|\n",
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "|1001   |501        |Alice        |alice@example.com  |2025-07-10 08:23:00|101       |3       |25.5 |\n",
            "|1002   |502        |Bob          |bob@example.com    |2025-07-11 09:45:00|102       |2       |15.0 |\n",
            "|1003   |503        |Charlie      |charlie@example.com|2025-07-12 10:15:00|103       |1       |30.0 |\n",
            "|1004   |504        |David        |david@example.com  |2025-07-13 12:20:00|101       |5       |25.5 |\n",
            "|1005   |505        |Emma         |emma@example.com   |2025-07-14 14:35:00|105       |10      |45.0 |\n",
            "|1006   |506        |Frank        |frank@example.com  |2025-07-15 16:00:00|102       |4       |15.0 |\n",
            "+-------+-----------+-------------+-------------------+-------------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Join sales and product data\n",
        "df_sales_with_category = df.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 2: Filter for category 'Gadgets'\n",
        "df_gadgets_sales = df_sales_with_category.filter(F.col(\"category\") == \"Gadgets\")\n",
        "\n",
        "# Step 3: Show selected fields\n",
        "df_gadgets_sales.select(\n",
        "    \"sale_id\", \"product_id\", \"product_name\", \"category\",\n",
        "    \"customer_id\", \"sale_date\", \"quantity\", \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ec9cypllXat",
        "outputId": "0465921b-4694-4fea-c366-38131e6750e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category|customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1004   |101       |Widget A    |Gadgets |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1002   |102       |Widget B    |Gadgets |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1006   |102       |Widget B    |Gadgets |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "+-------+----------+------------+--------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LEFT JOIN between sales and products\n",
        "df_sales_left_join = df.join(df_products, on=\"product_id\", how=\"left\")\n",
        "\n",
        "# Show key fields, including nulls if product info is missing\n",
        "df_sales_left_join.select(\n",
        "    \"sale_id\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96aU-KIflrrr",
        "outputId": "490bd3f2-c530-4a5e-acd2-943f503d6e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|product_name|category     |customer_id|sale_date          |quantity|price|\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "|1001   |101       |Widget A    |Gadgets      |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|1003   |103       |Widget C    |Electronics  |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|1002   |102       |Widget B    |Gadgets      |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|1004   |101       |Widget A    |Gadgets      |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|1006   |102       |Widget B    |Gadgets      |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|1005   |105       |Widget E    |Home & Living|505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+-------+----------+------------+-------------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Alias the sales data for self-join\n",
        "sales1 = df.alias(\"s1\")\n",
        "sales2 = df.alias(\"s2\")\n",
        "\n",
        "# Perform the self-join on product_id where sale_dates are different\n",
        "df_self_joined = sales1.join(\n",
        "    sales2,\n",
        "    (col(\"s1.product_id\") == col(\"s2.product_id\")) &\n",
        "    (col(\"s1.sale_date\") < col(\"s2.sale_date\")),  # avoid same and duplicate pairs\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Select relevant comparison fields\n",
        "df_self_joined.select(\n",
        "    col(\"s1.product_id\"),\n",
        "    col(\"s1.sale_id\").alias(\"sale_id_1\"),\n",
        "    col(\"s1.sale_date\").alias(\"sale_date_1\"),\n",
        "    col(\"s1.quantity\").alias(\"quantity_1\"),\n",
        "    col(\"s2.sale_id\").alias(\"sale_id_2\"),\n",
        "    col(\"s2.sale_date\").alias(\"sale_date_2\"),\n",
        "    col(\"s2.quantity\").alias(\"quantity_2\")\n",
        ").orderBy(\"product_id\", \"sale_date_1\").show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tin10qcRl0jW",
        "outputId": "b8234231-2b97-460a-d581-03f3e0f9953d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "|product_id|sale_id_1|sale_date_1        |quantity_1|sale_id_2|sale_date_2        |quantity_2|\n",
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "|101       |1001     |2025-07-10 08:23:00|3         |1004     |2025-07-13 12:20:00|5         |\n",
            "|102       |1002     |2025-07-11 09:45:00|2         |1006     |2025-07-15 16:00:00|4         |\n",
            "+----------+---------+-------------------+----------+---------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform full outer join on product_id\n",
        "df_full_outer = df.join(df_products, on=\"product_id\", how=\"outer\")\n",
        "\n",
        "# Select relevant fields\n",
        "df_full_outer.select(\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"sale_id\",\n",
        "    \"customer_id\",\n",
        "    \"sale_date\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").orderBy(\"product_id\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thZtbCgMmXY2",
        "outputId": "e41855db-ba36-41db-815e-4512fc26b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "|product_id|product_name|category     |sale_id|customer_id|sale_date          |quantity|price|\n",
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "|101       |Widget A    |Gadgets      |1001   |501        |2025-07-10 08:23:00|3       |25.5 |\n",
            "|101       |Widget A    |Gadgets      |1004   |504        |2025-07-13 12:20:00|5       |25.5 |\n",
            "|102       |Widget B    |Gadgets      |1002   |502        |2025-07-11 09:45:00|2       |15.0 |\n",
            "|102       |Widget B    |Gadgets      |1006   |506        |2025-07-15 16:00:00|4       |15.0 |\n",
            "|103       |Widget C    |Electronics  |1003   |503        |2025-07-12 10:15:00|1       |30.0 |\n",
            "|104       |Widget D    |Electronics  |NULL   |NULL       |NULL               |NULL    |NULL |\n",
            "|105       |Widget E    |Home & Living|1005   |505        |2025-07-14 14:35:00|10      |45.0 |\n",
            "+----------+------------+-------------+-------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Join sales with customers\n",
        "df_sales_customers = df.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 2: Join the above result with products\n",
        "df_complete_sales = df_sales_customers.join(df_products, on=\"product_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Select and display relevant columns\n",
        "df_complete_sales.select(\n",
        "    \"sale_id\",\n",
        "    \"sale_date\",\n",
        "    \"product_id\",\n",
        "    \"product_name\",\n",
        "    \"category\",\n",
        "    \"customer_id\",\n",
        "    \"customer_name\",\n",
        "    \"email\",\n",
        "    \"quantity\",\n",
        "    \"price\"\n",
        ").orderBy(\"sale_date\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPFVpO3LmfPH",
        "outputId": "3dfbeb7c-1293-41b5-9a62-6f79153dd7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "|sale_id|sale_date          |product_id|product_name|category     |customer_id|customer_name|email              |quantity|price|\n",
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "|1001   |2025-07-10 08:23:00|101       |Widget A    |Gadgets      |501        |Alice        |alice@example.com  |3       |25.5 |\n",
            "|1002   |2025-07-11 09:45:00|102       |Widget B    |Gadgets      |502        |Bob          |bob@example.com    |2       |15.0 |\n",
            "|1003   |2025-07-12 10:15:00|103       |Widget C    |Electronics  |503        |Charlie      |charlie@example.com|1       |30.0 |\n",
            "|1004   |2025-07-13 12:20:00|101       |Widget A    |Gadgets      |504        |David        |david@example.com  |5       |25.5 |\n",
            "|1005   |2025-07-14 14:35:00|105       |Widget E    |Home & Living|505        |Emma         |emma@example.com   |10      |45.0 |\n",
            "|1006   |2025-07-15 16:00:00|102       |Widget B    |Gadgets      |506        |Frank        |frank@example.com  |4       |15.0 |\n",
            "+-------+-------------------+----------+------------+-------------+-----------+-------------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Additional Transformations:\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Ensure sale_date is in timestamp format (if it's a string)\n",
        "df_filtered = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Filter rows between July 10 and July 15, 2025 (inclusive)\n",
        "df_sales_range = df_filtered.filter(\n",
        "    col(\"sale_date\").between(\"2025-07-10\", \"2025-07-15\")\n",
        ")\n",
        "\n",
        "# Show result\n",
        "df_sales_range.select(\"sale_id\", \"sale_date\", \"product_id\", \"quantity\", \"price\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKtViCPimwOJ",
        "outputId": "4a332c6f-4a8e-4160-a346-8e221e021c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+----------+--------+-----+\n",
            "|sale_id|sale_date          |product_id|quantity|price|\n",
            "+-------+-------------------+----------+--------+-----+\n",
            "|1001   |2025-07-10 08:23:00|101       |3       |25.5 |\n",
            "|1002   |2025-07-11 09:45:00|102       |2       |15.0 |\n",
            "|1003   |2025-07-12 10:15:00|103       |1       |30.0 |\n",
            "|1004   |2025-07-13 12:20:00|101       |5       |25.5 |\n",
            "|1005   |2025-07-14 14:35:00|105       |10      |45.0 |\n",
            "+-------+-------------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Step 1: Calculate revenue per transaction\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with customer data to get names\n",
        "df_with_customers = df_with_revenue.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Aggregate total revenue per customer\n",
        "df_customer_spending = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_spent\"))\n",
        "\n",
        "# Step 4: Order and limit to top 5\n",
        "df_top_5_customers = df_customer_spending.orderBy(F.desc(\"total_spent\")).limit(5)\n",
        "\n",
        "# Step 5: Show result\n",
        "df_top_5_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aomJECxunKLt",
        "outputId": "f5603e2a-f0cf-4cba-a27d-657c008a2fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+\n",
            "|customer_id|customer_name|total_spent|\n",
            "+-----------+-------------+-----------+\n",
            "|        505|         Emma|      450.0|\n",
            "|        504|        David|      127.5|\n",
            "|        501|        Alice|       76.5|\n",
            "|        506|        Frank|       60.0|\n",
            "|        502|          Bob|       30.0|\n",
            "+-----------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Step 1: Compute revenue per transaction\n",
        "df_with_revenue = df.withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Step 2: Join with customer data\n",
        "df_with_customers = df_with_revenue.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Total spending per customer\n",
        "df_total_spending = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(F.round(F.sum(\"revenue\"), 2).alias(\"total_spent\"))\n",
        "\n",
        "# Step 4: Categorize into spending groups\n",
        "df_segmented = df_total_spending.withColumn(\n",
        "    \"spending_category\",\n",
        "    when(F.col(\"total_spent\") <= 100, \"Low Spender\")\n",
        "    .when((F.col(\"total_spent\") > 100) & (F.col(\"total_spent\") <= 300), \"Medium Spender\")\n",
        "    .otherwise(\"High Spender\")\n",
        ")\n",
        "\n",
        "# Step 5: Show the result\n",
        "df_segmented.orderBy(F.desc(\"total_spent\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0DV_Qu8nWzV",
        "outputId": "22c7ea9c-47b4-4e44-b5cf-8b35da9bc74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+-----------------+\n",
            "|customer_id|customer_name|total_spent|spending_category|\n",
            "+-----------+-------------+-----------+-----------------+\n",
            "|        505|         Emma|      450.0|     High Spender|\n",
            "|        504|        David|      127.5|   Medium Spender|\n",
            "|        501|        Alice|       76.5|      Low Spender|\n",
            "|        506|        Frank|       60.0|      Low Spender|\n",
            "|        502|          Bob|       30.0|      Low Spender|\n",
            "|        503|      Charlie|       30.0|      Low Spender|\n",
            "+-----------+-------------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, to_timestamp\n",
        "\n",
        "# Ensure sale_date is timestamp\n",
        "df_sales = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Join with customer names (optional, for readability)\n",
        "df_with_customers = df_sales.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Group by customer and find first and last purchase dates\n",
        "df_purchase_dates = df_with_customers.groupBy(\"customer_id\", \"customer_name\") \\\n",
        "    .agg(\n",
        "        min(\"sale_date\").alias(\"first_purchase\"),\n",
        "        max(\"sale_date\").alias(\"last_purchase\")\n",
        "    ) \\\n",
        "    .orderBy(\"customer_id\")\n",
        "\n",
        "# Show result\n",
        "df_purchase_dates.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzN8WuuwnZKc",
        "outputId": "4fdd05e7-e8dd-49ab-dbb1-3c6ccd5be82b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------------+-------------------+\n",
            "|customer_id|customer_name|first_purchase     |last_purchase      |\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "|501        |Alice        |2025-07-10 08:23:00|2025-07-10 08:23:00|\n",
            "|502        |Bob          |2025-07-11 09:45:00|2025-07-11 09:45:00|\n",
            "|503        |Charlie      |2025-07-12 10:15:00|2025-07-12 10:15:00|\n",
            "|504        |David        |2025-07-13 12:20:00|2025-07-13 12:20:00|\n",
            "|505        |Emma         |2025-07-14 14:35:00|2025-07-14 14:35:00|\n",
            "|506        |Frank        |2025-07-15 16:00:00|2025-07-15 16:00:00|\n",
            "+-----------+-------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, to_timestamp, current_date, datediff\n",
        "\n",
        "# Ensure sale_date is timestamp\n",
        "df_sales = df.withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Step 1: Get last purchase date per customer\n",
        "df_last_purchase = df_sales.groupBy(\"customer_id\") \\\n",
        "    .agg(max(\"sale_date\").alias(\"last_purchase\"))\n",
        "\n",
        "# Step 2: Join with customer info\n",
        "df_customer_last = df_last_purchase.join(df_customers, on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "# Step 3: Filter customers where last purchase was more than 30 days ago\n",
        "df_inactive_customers = df_customer_last.filter(\n",
        "    datediff(current_date(), \"last_purchase\") > 30\n",
        ")\n",
        "\n",
        "# Step 4: Show result\n",
        "df_inactive_customers.select(\"customer_id\", \"customer_name\", \"email\", \"last_purchase\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bNuzSONnmTQ",
        "outputId": "b5fea272-e19b-4808-810f-8e25a7d23196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----+-------------+\n",
            "|customer_id|customer_name|email|last_purchase|\n",
            "+-----------+-------------+-----+-------------+\n",
            "+-----------+-------------+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA SETS"
      ],
      "metadata": {
        "id": "UOosWDy_r5VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"FullCustomerDataset\").getOrCreate()\n",
        "\n",
        "# Define schema for customer data\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"customer_name\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"join_date\", StringType(), True),\n",
        "])\n",
        "\n",
        "# Full customer dataset\n",
        "customer_data = [\n",
        "    (501, \"Alice\", \"alice@example.com\", \"2025-05-20 10:10:00\"),\n",
        "    (502, \"Bob\", \"bob@example.com\", \"2025-06-15 14:00:00\"),\n",
        "    (503, \"Charlie\", \"charlie@example.com\", \"2025-04-05 9:50:00\"),\n",
        "    (504, \"David\", \"david@example.com\", \"2025-07-01 12:25:00\"),\n",
        "    (505, \"Emma\", \"emma@example.com\", \"2025-07-10 15:30:00\"),\n",
        "    (506, \"Frank\", \"frank@example.com\", \"2025-03-23 17:00:00\"),\n",
        "    (507, \"Grace\", \"grace@example.com\", \"2025-05-01 13:20:00\"),\n",
        "    (508, \"Henry\", \"henry@example.com\", \"2025-06-07 10:10:00\"),\n",
        "    (509, \"Isabel\", \"isabel@example.com\", \"2025-05-25 16:30:00\"),\n",
        "    (510, \"Jack\", \"jack@example.com\", \"2025-04-12 11:55:00\"),\n",
        "    (511, \"Kate\", \"kate@example.com\", \"2025-06-18 14:10:00\"),\n",
        "    (512, \"Liam\", \"liam@example.com\", \"2025-07-05 8:45:00\"),\n",
        "    (513, \"Mona\", \"mona@example.com\", \"2025-03-30 15:25:00\"),\n",
        "    (514, \"Nina\", \"nina@example.com\", \"2025-04-19 10:35:00\"),\n",
        "    (515, \"Oscar\", \"oscar@example.com\", \"2025-05-14 9:15:00\"),\n",
        "    (516, \"Paul\", \"paul@example.com\", \"2025-06-03 12:50:00\"),\n",
        "    (517, \"Quinn\", \"quinn@example.com\", \"2025-07-13 13:10:00\"),\n",
        "    (518, \"Rita\", \"rita@example.com\", \"2025-07-20 11:00:00\"),\n",
        "    (519, \"Sara\", \"sara@example.com\", \"2025-06-30 17:40:00\"),\n",
        "    (520, \"Tom\", \"tom@example.com\", \"2025-05-18 14:20:00\"),\n",
        "    (521, \"Ursula\", \"ursula@example.com\", \"2025-04-01 10:50:00\"),\n",
        "    (522, \"Victor\", \"victor@example.com\", \"2025-07-22 16:30:00\"),\n",
        "    (523, \"Wendy\", \"wendy@example.com\", \"2025-06-09 13:00:00\"),\n",
        "    (524, \"Xander\", \"xander@example.com\", \"2025-05-30 18:05:00\"),\n",
        "    (525, \"Yvonne\", \"yvonne@example.com\", \"2025-03-28 14:15:00\"),\n",
        "    (526, \"Zach\", \"zach@example.com\", \"2025-04-23 11:30:00\"),\n",
        "    (527, \"Amos\", \"amos@example.com\", \"2025-07-07 9:25:00\"),\n",
        "    (528, \"Bella\", \"bella@example.com\", \"2025-05-02 12:00:00\"),\n",
        "    (529, \"Clara\", \"clara@example.com\", \"2025-06-25 15:10:00\"),\n",
        "    (530, \"Dan\", \"d@danmail.com\", \"2025-07-14 14:15:00\"),\n",
        "    (531, \"Ellie\", \"ellie@example.com\", \"2025-06-01 8:30:00\"),\n",
        "    (532, \"Freddy\", \"freddy@example.com\", \"2025-07-03 12:45:00\"),\n",
        "    (533, \"Grace\", \"grace@anothermail.com\", \"2025-05-15 13:30:00\"),\n",
        "    (534, \"Harry\", \"harry@example.com\", \"2025-04-10 17:25:00\"),\n",
        "    (535, \"Ivy\", \"ivy@example.com\", \"2025-03-18 16:00:00\"),\n",
        "    (536, \"John\", \"john@example.com\", \"2025-07-19 10:50:00\"),\n",
        "    (537, \"Ken\", \"ken@example.com\", \"2025-05-12 9:35:00\"),\n",
        "    (538, \"Lena\", \"lena@example.com\", \"2025-06-21 13:55:00\"),\n",
        "    (539, \"Monica\", \"monica@example.com\", \"2025-03-27 18:40:00\"),\n",
        "    (540, \"Nash\", \"nash@example.com\", \"2025-04-15 14:25:00\"),\n",
        "    (541, \"Olivia\", \"olivia@example.com\", \"2025-07-06 17:10:00\"),\n",
        "    (542, \"Peter\", \"peter@example.com\", \"2025-05-23 14:00:00\"),\n",
        "    (543, \"Quincy\", \"quincy@example.com\", \"2025-06-05 12:10:00\"),\n",
        "    (544, \"Rob\", \"rob@example.com\", \"2025-03-22 13:35:00\"),\n",
        "    (545, \"Sophia\", \"sophia@example.com\", \"2025-07-09 10:20:00\"),\n",
        "    (546, \"Theo\", \"theo@example.com\", \"2025-04-02 12:30:00\"),\n",
        "    (547, \"Una\", \"una@example.com\", \"2025-07-15 16:55:00\"),\n",
        "    (548, \"Victor\", \"victor1@example.com\", \"2025-05-06 15:40:00\"),\n",
        "    (549, \"Wesley\", \"wesley@example.com\", \"2025-06-12 11:50:00\"),\n",
        "    (550, \"Xena\", \"xena@example.com\", \"2025-07-17 8:55:00\"),\n",
        "    (551, \"Yara\", \"yara@example.com\", \"2025-03-29 10:45:00\"),\n",
        "    (552, \"Zoe\", \"zoe@example.com\", \"2025-05-05 9:20:00\"),\n",
        "    (553, \"Alice2\", \"alice2@example.com\", \"2025-07-23 14:30:00\"),\n",
        "    (554, \"Bob2\", \"bob2@example.com\", \"2025-06-13 11:25:00\"),\n",
        "    (555, \"Charlie2\", \"charlie2@example.com\", \"2025-04-20 9:55:00\"),\n",
        "    (556, \"David2\", \"david2@example.com\", \"2025-07-11 10:00:00\")\n",
        "]\n",
        "\n",
        "# Create DataFrame and convert join_date to timestamp\n",
        "df_all_customers = spark.createDataFrame(customer_data, schema=customer_schema) \\\n",
        "    .withColumn(\"join_date\", to_timestamp(\"join_date\"))\n",
        "\n",
        "# Display first few records\n",
        "df_all_customers.show(60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1yjGEEur4gv",
        "outputId": "799a1311-f908-4f8d-ed8d-331508ccce8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+--------------------+-------------------+\n",
            "|customer_id|customer_name|               email|          join_date|\n",
            "+-----------+-------------+--------------------+-------------------+\n",
            "|        501|        Alice|   alice@example.com|2025-05-20 10:10:00|\n",
            "|        502|          Bob|     bob@example.com|2025-06-15 14:00:00|\n",
            "|        503|      Charlie| charlie@example.com|2025-04-05 09:50:00|\n",
            "|        504|        David|   david@example.com|2025-07-01 12:25:00|\n",
            "|        505|         Emma|    emma@example.com|2025-07-10 15:30:00|\n",
            "|        506|        Frank|   frank@example.com|2025-03-23 17:00:00|\n",
            "|        507|        Grace|   grace@example.com|2025-05-01 13:20:00|\n",
            "|        508|        Henry|   henry@example.com|2025-06-07 10:10:00|\n",
            "|        509|       Isabel|  isabel@example.com|2025-05-25 16:30:00|\n",
            "|        510|         Jack|    jack@example.com|2025-04-12 11:55:00|\n",
            "|        511|         Kate|    kate@example.com|2025-06-18 14:10:00|\n",
            "|        512|         Liam|    liam@example.com|2025-07-05 08:45:00|\n",
            "|        513|         Mona|    mona@example.com|2025-03-30 15:25:00|\n",
            "|        514|         Nina|    nina@example.com|2025-04-19 10:35:00|\n",
            "|        515|        Oscar|   oscar@example.com|2025-05-14 09:15:00|\n",
            "|        516|         Paul|    paul@example.com|2025-06-03 12:50:00|\n",
            "|        517|        Quinn|   quinn@example.com|2025-07-13 13:10:00|\n",
            "|        518|         Rita|    rita@example.com|2025-07-20 11:00:00|\n",
            "|        519|         Sara|    sara@example.com|2025-06-30 17:40:00|\n",
            "|        520|          Tom|     tom@example.com|2025-05-18 14:20:00|\n",
            "|        521|       Ursula|  ursula@example.com|2025-04-01 10:50:00|\n",
            "|        522|       Victor|  victor@example.com|2025-07-22 16:30:00|\n",
            "|        523|        Wendy|   wendy@example.com|2025-06-09 13:00:00|\n",
            "|        524|       Xander|  xander@example.com|2025-05-30 18:05:00|\n",
            "|        525|       Yvonne|  yvonne@example.com|2025-03-28 14:15:00|\n",
            "|        526|         Zach|    zach@example.com|2025-04-23 11:30:00|\n",
            "|        527|         Amos|    amos@example.com|2025-07-07 09:25:00|\n",
            "|        528|        Bella|   bella@example.com|2025-05-02 12:00:00|\n",
            "|        529|        Clara|   clara@example.com|2025-06-25 15:10:00|\n",
            "|        530|          Dan|       d@danmail.com|2025-07-14 14:15:00|\n",
            "|        531|        Ellie|   ellie@example.com|2025-06-01 08:30:00|\n",
            "|        532|       Freddy|  freddy@example.com|2025-07-03 12:45:00|\n",
            "|        533|        Grace|grace@anothermail...|2025-05-15 13:30:00|\n",
            "|        534|        Harry|   harry@example.com|2025-04-10 17:25:00|\n",
            "|        535|          Ivy|     ivy@example.com|2025-03-18 16:00:00|\n",
            "|        536|         John|    john@example.com|2025-07-19 10:50:00|\n",
            "|        537|          Ken|     ken@example.com|2025-05-12 09:35:00|\n",
            "|        538|         Lena|    lena@example.com|2025-06-21 13:55:00|\n",
            "|        539|       Monica|  monica@example.com|2025-03-27 18:40:00|\n",
            "|        540|         Nash|    nash@example.com|2025-04-15 14:25:00|\n",
            "|        541|       Olivia|  olivia@example.com|2025-07-06 17:10:00|\n",
            "|        542|        Peter|   peter@example.com|2025-05-23 14:00:00|\n",
            "|        543|       Quincy|  quincy@example.com|2025-06-05 12:10:00|\n",
            "|        544|          Rob|     rob@example.com|2025-03-22 13:35:00|\n",
            "|        545|       Sophia|  sophia@example.com|2025-07-09 10:20:00|\n",
            "|        546|         Theo|    theo@example.com|2025-04-02 12:30:00|\n",
            "|        547|          Una|     una@example.com|2025-07-15 16:55:00|\n",
            "|        548|       Victor| victor1@example.com|2025-05-06 15:40:00|\n",
            "|        549|       Wesley|  wesley@example.com|2025-06-12 11:50:00|\n",
            "|        550|         Xena|    xena@example.com|2025-07-17 08:55:00|\n",
            "|        551|         Yara|    yara@example.com|2025-03-29 10:45:00|\n",
            "|        552|          Zoe|     zoe@example.com|2025-05-05 09:20:00|\n",
            "|        553|       Alice2|  alice2@example.com|2025-07-23 14:30:00|\n",
            "|        554|         Bob2|    bob2@example.com|2025-06-13 11:25:00|\n",
            "|        555|     Charlie2|charlie2@example.com|2025-04-20 09:55:00|\n",
            "|        556|       David2|  david2@example.com|2025-07-11 10:00:00|\n",
            "+-----------+-------------+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start SparkSession (if not already running)\n",
        "spark = SparkSession.builder.appName(\"ProductDataset\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Product data (truncated here  use full list in your environment)\n",
        "product_data = [\n",
        "    (101, \"Widget A\", \"Gadgets\"),\n",
        "    (102, \"Widget B\", \"Gadgets\"),\n",
        "    (103, \"Widget C\", \"Electronics\"),\n",
        "    (104, \"Widget D\", \"Electronics\"),\n",
        "    (105, \"Widget E\", \"Home & Living\"),\n",
        "    (106, \"Widget F\", \"Home & Living\"),\n",
        "    (107, \"Widget G\", \"Furniture\"),\n",
        "    (108, \"Widget H\", \"Gadgets\"),\n",
        "    (109, \"Widget I\", \"Furniture\"),\n",
        "    (110, \"Widget J\", \"Electronics\"),\n",
        "    (111, \"Widget K\", \"Home & Living\"),\n",
        "    (112, \"Widget L\", \"Electronics\"),\n",
        "    (113, \"Widget M\", \"Gadgets\"),\n",
        "    (114, \"Widget N\", \"Furniture\"),\n",
        "    (115, \"Widget O\", \"Gadgets\"),\n",
        "    (116, \"Widget P\", \"Home & Living\"),\n",
        "    (117, \"Widget Q\", \"Electronics\"),\n",
        "    (118, \"Widget R\", \"Furniture\"),\n",
        "    (119, \"Widget S\", \"Gadgets\"),\n",
        "    (120, \"Widget T\", \"Furniture\"),\n",
        "    (121, \"Widget U\", \"Home & Living\"),\n",
        "    (122, \"Widget V\", \"Electronics\"),\n",
        "    (123, \"Widget W\", \"Furniture\"),\n",
        "    (124, \"Widget X\", \"Home & Living\"),\n",
        "    (125, \"Widget Y\", \"Gadgets\"),\n",
        "    (126, \"Widget Z\", \"Electronics\"),\n",
        "    (127, \"Widget AA\", \"Furniture\"),\n",
        "    (128, \"Widget AB\", \"Gadgets\"),\n",
        "    (129, \"Widget AC\", \"Home & Living\"),\n",
        "    (130, \"Widget AD\", \"Furniture\"),\n",
        "    (131, \"Widget AE\", \"Gadgets\"),\n",
        "    (132, \"Widget AF\", \"Electronics\"),\n",
        "    (133, \"Widget AG\", \"Furniture\"),\n",
        "    (134, \"Widget AH\", \"Home & Living\"),\n",
        "    (135, \"Widget AI\", \"Electronics\"),\n",
        "    (136, \"Widget AJ\", \"Furniture\"),\n",
        "    (137, \"Widget AK\", \"Gadgets\"),\n",
        "    (138, \"Widget AL\", \"Home & Living\"),\n",
        "    (139, \"Widget AM\", \"Electronics\"),\n",
        "    (140, \"Widget AN\", \"Furniture\"),\n",
        "    (141, \"Widget AO\", \"Gadgets\"),\n",
        "    (142, \"Widget AP\", \"Electronics\"),\n",
        "    (143, \"Widget AQ\", \"Home & Living\"),\n",
        "    (144, \"Widget AR\", \"Gadgets\"),\n",
        "    (145, \"Widget AS\", \"Furniture\"),\n",
        "    (146, \"Widget AT\", \"Home & Living\"),\n",
        "    (147, \"Widget AU\", \"Electronics\"),\n",
        "    (148, \"Widget AV\", \"Furniture\"),\n",
        "    (149, \"Widget AW\", \"Gadgets\"),\n",
        "    (150, \"Widget AX\", \"Home & Living\"),\n",
        "    (151, \"Widget AY\", \"Electronics\"),\n",
        "    (152, \"Widget AZ\", \"Furniture\"),\n",
        "    (153, \"Widget BA\", \"Gadgets\"),\n",
        "    (154, \"Widget BB\", \"Electronics\"),\n",
        "    (155, \"Widget BC\", \"Home & Living\"),\n",
        "    (156, \"Widget BD\", \"Furniture\"),\n",
        "    (157, \"Widget BE\", \"Gadgets\"),\n",
        "    (158, \"Widget BF\", \"Electronics\"),\n",
        "    (159, \"Widget BG\", \"Home & Living\"),\n",
        "    (160, \"Widget BH\", \"Furniture\"),\n",
        "    (161, \"Widget BI\", \"Gadgets\"),\n",
        "    (162, \"Widget BJ\", \"Electronics\"),\n",
        "    (163, \"Widget BK\", \"Home & Living\"),\n",
        "    (164, \"Widget BL\", \"Furniture\"),\n",
        "    (165, \"Widget BM\", \"Gadgets\"),\n",
        "    (166, \"Widget BN\", \"Electronics\"),\n",
        "    (167, \"Widget BO\", \"Furniture\"),\n",
        "    (168, \"Widget BP\", \"Home & Living\"),\n",
        "    (169, \"Widget BQ\", \"Gadgets\"),\n",
        "    (170, \"Widget BR\", \"Electronics\")\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_products = spark.createDataFrame(product_data, schema=product_schema)\n",
        "\n",
        "# Show some records\n",
        "df_products.show(70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffk_Qg5ytdLv",
        "outputId": "5dd0a067-e1c0-4829-8073-b82608510117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|     category|\n",
            "+----------+------------+-------------+\n",
            "|       101|    Widget A|      Gadgets|\n",
            "|       102|    Widget B|      Gadgets|\n",
            "|       103|    Widget C|  Electronics|\n",
            "|       104|    Widget D|  Electronics|\n",
            "|       105|    Widget E|Home & Living|\n",
            "|       106|    Widget F|Home & Living|\n",
            "|       107|    Widget G|    Furniture|\n",
            "|       108|    Widget H|      Gadgets|\n",
            "|       109|    Widget I|    Furniture|\n",
            "|       110|    Widget J|  Electronics|\n",
            "|       111|    Widget K|Home & Living|\n",
            "|       112|    Widget L|  Electronics|\n",
            "|       113|    Widget M|      Gadgets|\n",
            "|       114|    Widget N|    Furniture|\n",
            "|       115|    Widget O|      Gadgets|\n",
            "|       116|    Widget P|Home & Living|\n",
            "|       117|    Widget Q|  Electronics|\n",
            "|       118|    Widget R|    Furniture|\n",
            "|       119|    Widget S|      Gadgets|\n",
            "|       120|    Widget T|    Furniture|\n",
            "|       121|    Widget U|Home & Living|\n",
            "|       122|    Widget V|  Electronics|\n",
            "|       123|    Widget W|    Furniture|\n",
            "|       124|    Widget X|Home & Living|\n",
            "|       125|    Widget Y|      Gadgets|\n",
            "|       126|    Widget Z|  Electronics|\n",
            "|       127|   Widget AA|    Furniture|\n",
            "|       128|   Widget AB|      Gadgets|\n",
            "|       129|   Widget AC|Home & Living|\n",
            "|       130|   Widget AD|    Furniture|\n",
            "|       131|   Widget AE|      Gadgets|\n",
            "|       132|   Widget AF|  Electronics|\n",
            "|       133|   Widget AG|    Furniture|\n",
            "|       134|   Widget AH|Home & Living|\n",
            "|       135|   Widget AI|  Electronics|\n",
            "|       136|   Widget AJ|    Furniture|\n",
            "|       137|   Widget AK|      Gadgets|\n",
            "|       138|   Widget AL|Home & Living|\n",
            "|       139|   Widget AM|  Electronics|\n",
            "|       140|   Widget AN|    Furniture|\n",
            "|       141|   Widget AO|      Gadgets|\n",
            "|       142|   Widget AP|  Electronics|\n",
            "|       143|   Widget AQ|Home & Living|\n",
            "|       144|   Widget AR|      Gadgets|\n",
            "|       145|   Widget AS|    Furniture|\n",
            "|       146|   Widget AT|Home & Living|\n",
            "|       147|   Widget AU|  Electronics|\n",
            "|       148|   Widget AV|    Furniture|\n",
            "|       149|   Widget AW|      Gadgets|\n",
            "|       150|   Widget AX|Home & Living|\n",
            "|       151|   Widget AY|  Electronics|\n",
            "|       152|   Widget AZ|    Furniture|\n",
            "|       153|   Widget BA|      Gadgets|\n",
            "|       154|   Widget BB|  Electronics|\n",
            "|       155|   Widget BC|Home & Living|\n",
            "|       156|   Widget BD|    Furniture|\n",
            "|       157|   Widget BE|      Gadgets|\n",
            "|       158|   Widget BF|  Electronics|\n",
            "|       159|   Widget BG|Home & Living|\n",
            "|       160|   Widget BH|    Furniture|\n",
            "|       161|   Widget BI|      Gadgets|\n",
            "|       162|   Widget BJ|  Electronics|\n",
            "|       163|   Widget BK|Home & Living|\n",
            "|       164|   Widget BL|    Furniture|\n",
            "|       165|   Widget BM|      Gadgets|\n",
            "|       166|   Widget BN|  Electronics|\n",
            "|       167|   Widget BO|    Furniture|\n",
            "|       168|   Widget BP|Home & Living|\n",
            "|       169|   Widget BQ|      Gadgets|\n",
            "|       170|   Widget BR|  Electronics|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import necessary components\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "\n",
        "# Define schema for full sales data\n",
        "sales_schema = StructType([\n",
        "    StructField(\"sale_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"sale_date\", StringType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"price\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Full 70-entry sales dataset\n",
        "sales_data_full = [\n",
        "    (1001, 101, 501, \"2025-07-10 8:23:00\", 3, 25.5), (1002, 102, 502, \"2025-07-11 9:45:00\", 2, 15.0),\n",
        "    (1003, 103, 503, \"2025-07-12 10:15:00\", 1, 30.0), (1004, 101, 504, \"2025-07-13 12:20:00\", 5, 25.5),\n",
        "    (1005, 105, 505, \"2025-07-14 14:35:00\", 10, 45.0), (1006, 102, 506, \"2025-07-15 16:00:00\", 4, 15.0),\n",
        "    (1007, 106, 507, \"2025-07-16 17:10:00\", 3, 40.0), (1008, 107, 508, \"2025-07-17 18:25:00\", 2, 60.0),\n",
        "    (1009, 108, 509, \"2025-07-18 19:30:00\", 7, 25.0), (1010, 109, 510, \"2025-07-19 20:45:00\", 6, 50.0),\n",
        "    (1011, 110, 511, \"2025-07-20 21:55:00\", 4, 12.5), (1012, 111, 512, \"2025-07-21 22:30:00\", 9, 10.0),\n",
        "    (1013, 112, 513, \"2025-07-22 23:40:00\", 8, 22.5), (1014, 113, 514, \"2025-07-23 9:15:00\", 3, 17.0),\n",
        "    (1015, 114, 515, \"2025-07-24 10:35:00\", 2, 45.5), (1016, 115, 516, \"2025-07-25 11:45:00\", 4, 32.0),\n",
        "    (1017, 116, 517, \"2025-07-26 13:00:00\", 5, 28.0), (1018, 117, 518, \"2025-07-27 14:15:00\", 1, 50.0),\n",
        "    (1019, 118, 519, \"2025-07-28 15:30:00\", 6, 18.5), (1020, 119, 520, \"2025-07-29 16:45:00\", 7, 40.0),\n",
        "    (1021, 120, 521, \"2025-07-30 17:50:00\", 3, 30.5), (1022, 121, 522, \"2025-07-31 18:20:00\", 2, 22.0),\n",
        "    (1023, 122, 523, \"2025-08-01 19:00:00\", 4, 25.0), (1024, 123, 524, \"2025-08-02 20:10:00\", 9, 15.5),\n",
        "    (1025, 124, 525, \"2025-08-03 21:00:00\", 5, 27.5), (1026, 125, 526, \"2025-08-04 22:25:00\", 8, 33.0),\n",
        "    (1027, 126, 527, \"2025-08-05 23:30:00\", 6, 40.0), (1028, 127, 528, \"2025-08-06 9:50:00\", 7, 10.0),\n",
        "    (1029, 128, 529, \"2025-08-07 10:10:00\", 4, 22.5), (1030, 129, 530, \"2025-08-08 11:00:00\", 2, 38.0),\n",
        "    (1031, 130, 531, \"2025-08-09 12:20:00\", 5, 50.0), (1032, 131, 532, \"2025-08-10 13:30:00\", 8, 29.5),\n",
        "    (1033, 132, 533, \"2025-08-11 14:40:00\", 3, 44.0), (1034, 133, 534, \"2025-08-12 15:00:00\", 7, 16.5),\n",
        "    (1035, 134, 535, \"2025-08-13 16:00:00\", 9, 20.0), (1036, 135, 536, \"2025-08-14 17:10:00\", 2, 28.5),\n",
        "    (1037, 136, 537, \"2025-08-15 18:30:00\", 4, 32.0), (1038, 137, 538, \"2025-08-16 19:00:00\", 5, 23.0),\n",
        "    (1039, 138, 539, \"2025-08-17 20:15:00\", 6, 35.0), (1040, 139, 540, \"2025-08-18 21:25:00\", 3, 18.5),\n",
        "    (1041, 140, 541, \"2025-08-19 22:00:00\", 2, 40.0), (1042, 141, 542, \"2025-08-20 23:10:00\", 8, 21.5),\n",
        "    (1043, 142, 543, \"2025-08-21 9:15:00\", 4, 47.5), (1044, 143, 544, \"2025-08-22 10:30:00\", 7, 12.0),\n",
        "    (1045, 144, 545, \"2025-08-23 11:40:00\", 3, 30.5), (1046, 145, 546, \"2025-08-24 12:55:00\", 6, 15.0),\n",
        "    (1047, 146, 547, \"2025-08-25 13:10:00\", 5, 36.0), (1048, 147, 548, \"2025-08-26 14:25:00\", 9, 22.0),\n",
        "    (1049, 148, 549, \"2025-08-27 15:40:00\", 2, 29.0), (1050, 149, 550, \"2025-08-28 16:50:00\", 1, 50.0),\n",
        "    (1051, 101, 551, \"2025-08-29 17:55:00\", 3, 25.5), (1052, 102, 552, \"2025-08-30 18:45:00\", 4, 15.0),\n",
        "    (1053, 103, 553, \"2025-08-31 19:30:00\", 7, 30.0), (1054, 104, 554, \"2025-09-01 20:40:00\", 5, 40.0),\n",
        "    (1055, 105, 555, \"2025-09-02 21:10:00\", 2, 45.0), (1056, 106, 556, \"2025-09-03 22:30:00\", 8, 50.0),\n",
        "    (1057, 107, 557, \"2025-09-04 23:50:00\", 3, 20.5), (1058, 108, 558, \"2025-09-05 9:30:00\", 1, 60.0),\n",
        "    (1059, 109, 559, \"2025-09-06 10:40:00\", 6, 18.0), (1060, 110, 560, \"2025-09-07 11:50:00\", 5, 32.0),\n",
        "    (1061, 111, 561, \"2025-09-08 13:00:00\", 9, 22.5), (1062, 112, 562, \"2025-09-09 14:10:00\", 2, 25.5),\n",
        "    (1063, 113, 563, \"2025-09-10 15:25:00\", 7, 38.0), (1064, 114, 564, \"2025-09-11 16:35:00\", 3, 27.5),\n",
        "    (1065, 115, 565, \"2025-09-12 17:45:00\", 8, 44.5), (1066, 116, 566, \"2025-09-13 18:55:00\", 6, 20.0),\n",
        "    (1067, 117, 567, \"2025-09-14 20:00:00\", 2, 32.0), (1068, 118, 568, \"2025-09-15 21:10:00\", 5, 27.5),\n",
        "    (1069, 119, 569, \"2025-09-16 22:15:00\", 4, 50.0), (1070, 120, 570, \"2025-09-17 23:00:00\", 3, 40.0)\n",
        "]\n",
        "\n",
        "# Create DataFrame and convert sale_date to timestamp\n",
        "df_sales_full = spark.createDataFrame(sales_data_full, schema=sales_schema) \\\n",
        "    .withColumn(\"sale_date\", to_timestamp(\"sale_date\"))\n",
        "\n",
        "# Display first 10 records to confirm\n",
        "df_sales_full.show(70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l_BwomdxTYG",
        "outputId": "0c679c33-5a32-46c8-c8b7-2493cfe14f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|sale_id|product_id|customer_id|          sale_date|quantity|price|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "|   1001|       101|        501|2025-07-10 08:23:00|       3| 25.5|\n",
            "|   1002|       102|        502|2025-07-11 09:45:00|       2| 15.0|\n",
            "|   1003|       103|        503|2025-07-12 10:15:00|       1| 30.0|\n",
            "|   1004|       101|        504|2025-07-13 12:20:00|       5| 25.5|\n",
            "|   1005|       105|        505|2025-07-14 14:35:00|      10| 45.0|\n",
            "|   1006|       102|        506|2025-07-15 16:00:00|       4| 15.0|\n",
            "|   1007|       106|        507|2025-07-16 17:10:00|       3| 40.0|\n",
            "|   1008|       107|        508|2025-07-17 18:25:00|       2| 60.0|\n",
            "|   1009|       108|        509|2025-07-18 19:30:00|       7| 25.0|\n",
            "|   1010|       109|        510|2025-07-19 20:45:00|       6| 50.0|\n",
            "|   1011|       110|        511|2025-07-20 21:55:00|       4| 12.5|\n",
            "|   1012|       111|        512|2025-07-21 22:30:00|       9| 10.0|\n",
            "|   1013|       112|        513|2025-07-22 23:40:00|       8| 22.5|\n",
            "|   1014|       113|        514|2025-07-23 09:15:00|       3| 17.0|\n",
            "|   1015|       114|        515|2025-07-24 10:35:00|       2| 45.5|\n",
            "|   1016|       115|        516|2025-07-25 11:45:00|       4| 32.0|\n",
            "|   1017|       116|        517|2025-07-26 13:00:00|       5| 28.0|\n",
            "|   1018|       117|        518|2025-07-27 14:15:00|       1| 50.0|\n",
            "|   1019|       118|        519|2025-07-28 15:30:00|       6| 18.5|\n",
            "|   1020|       119|        520|2025-07-29 16:45:00|       7| 40.0|\n",
            "|   1021|       120|        521|2025-07-30 17:50:00|       3| 30.5|\n",
            "|   1022|       121|        522|2025-07-31 18:20:00|       2| 22.0|\n",
            "|   1023|       122|        523|2025-08-01 19:00:00|       4| 25.0|\n",
            "|   1024|       123|        524|2025-08-02 20:10:00|       9| 15.5|\n",
            "|   1025|       124|        525|2025-08-03 21:00:00|       5| 27.5|\n",
            "|   1026|       125|        526|2025-08-04 22:25:00|       8| 33.0|\n",
            "|   1027|       126|        527|2025-08-05 23:30:00|       6| 40.0|\n",
            "|   1028|       127|        528|2025-08-06 09:50:00|       7| 10.0|\n",
            "|   1029|       128|        529|2025-08-07 10:10:00|       4| 22.5|\n",
            "|   1030|       129|        530|2025-08-08 11:00:00|       2| 38.0|\n",
            "|   1031|       130|        531|2025-08-09 12:20:00|       5| 50.0|\n",
            "|   1032|       131|        532|2025-08-10 13:30:00|       8| 29.5|\n",
            "|   1033|       132|        533|2025-08-11 14:40:00|       3| 44.0|\n",
            "|   1034|       133|        534|2025-08-12 15:00:00|       7| 16.5|\n",
            "|   1035|       134|        535|2025-08-13 16:00:00|       9| 20.0|\n",
            "|   1036|       135|        536|2025-08-14 17:10:00|       2| 28.5|\n",
            "|   1037|       136|        537|2025-08-15 18:30:00|       4| 32.0|\n",
            "|   1038|       137|        538|2025-08-16 19:00:00|       5| 23.0|\n",
            "|   1039|       138|        539|2025-08-17 20:15:00|       6| 35.0|\n",
            "|   1040|       139|        540|2025-08-18 21:25:00|       3| 18.5|\n",
            "|   1041|       140|        541|2025-08-19 22:00:00|       2| 40.0|\n",
            "|   1042|       141|        542|2025-08-20 23:10:00|       8| 21.5|\n",
            "|   1043|       142|        543|2025-08-21 09:15:00|       4| 47.5|\n",
            "|   1044|       143|        544|2025-08-22 10:30:00|       7| 12.0|\n",
            "|   1045|       144|        545|2025-08-23 11:40:00|       3| 30.5|\n",
            "|   1046|       145|        546|2025-08-24 12:55:00|       6| 15.0|\n",
            "|   1047|       146|        547|2025-08-25 13:10:00|       5| 36.0|\n",
            "|   1048|       147|        548|2025-08-26 14:25:00|       9| 22.0|\n",
            "|   1049|       148|        549|2025-08-27 15:40:00|       2| 29.0|\n",
            "|   1050|       149|        550|2025-08-28 16:50:00|       1| 50.0|\n",
            "|   1051|       101|        551|2025-08-29 17:55:00|       3| 25.5|\n",
            "|   1052|       102|        552|2025-08-30 18:45:00|       4| 15.0|\n",
            "|   1053|       103|        553|2025-08-31 19:30:00|       7| 30.0|\n",
            "|   1054|       104|        554|2025-09-01 20:40:00|       5| 40.0|\n",
            "|   1055|       105|        555|2025-09-02 21:10:00|       2| 45.0|\n",
            "|   1056|       106|        556|2025-09-03 22:30:00|       8| 50.0|\n",
            "|   1057|       107|        557|2025-09-04 23:50:00|       3| 20.5|\n",
            "|   1058|       108|        558|2025-09-05 09:30:00|       1| 60.0|\n",
            "|   1059|       109|        559|2025-09-06 10:40:00|       6| 18.0|\n",
            "|   1060|       110|        560|2025-09-07 11:50:00|       5| 32.0|\n",
            "|   1061|       111|        561|2025-09-08 13:00:00|       9| 22.5|\n",
            "|   1062|       112|        562|2025-09-09 14:10:00|       2| 25.5|\n",
            "|   1063|       113|        563|2025-09-10 15:25:00|       7| 38.0|\n",
            "|   1064|       114|        564|2025-09-11 16:35:00|       3| 27.5|\n",
            "|   1065|       115|        565|2025-09-12 17:45:00|       8| 44.5|\n",
            "|   1066|       116|        566|2025-09-13 18:55:00|       6| 20.0|\n",
            "|   1067|       117|        567|2025-09-14 20:00:00|       2| 32.0|\n",
            "|   1068|       118|        568|2025-09-15 21:10:00|       5| 27.5|\n",
            "|   1069|       119|        569|2025-09-16 22:15:00|       4| 50.0|\n",
            "|   1070|       120|        570|2025-09-17 23:00:00|       3| 40.0|\n",
            "+-------+----------+-----------+-------------------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WINDOWS FUNCTION  "
      ],
      "metadata": {
        "id": "X3j0tvmybGrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedOps\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, [\"math\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (2, \"Bob\", 1500, [\"english\"], {\"city\": \"SF\", \"zip\": \"94105\"}),\n",
        "    (3, \"Charlie\", 2200, [\"math\", \"history\", \"science\"], {\"city\": \"NYC\", \"zip\": \"10001\"}),\n",
        "    (4, \"David\", 1200, [\"art\"], {\"city\": \"LA\", \"zip\": \"90001\"}),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=[\"id\", \"name\", \"salary\", \"subjects\", \"address\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "HsYh5QhnbFgg",
        "outputId": "8e2d0af1-1509-45e8-adc0-941ebdec12fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+\n",
            "|id |name   |salary|subjects                |address                    |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |\n",
            "+---+-------+------+------------------------+---------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window specification\n",
        "window_spec = Window.partitionBy(\"address.city\").orderBy(\"salary\")\n",
        "df.withColumn(\"rank\", rank().over(window_spec)).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "RDYhkCxsypye",
        "outputId": "35262962-99ed-47d5-ed28-f1cb455003b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "|id |name   |salary|subjects                |address                    |rank|\n",
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "|4  |David  |1200  |[art]                   |{zip -> 90001, city -> LA} |1   |\n",
            "|1  |Alice  |2000  |[math, science]         |{zip -> 10001, city -> NYC}|1   |\n",
            "|3  |Charlie|2200  |[math, history, science]|{zip -> 10001, city -> NYC}|2   |\n",
            "|2  |Bob    |1500  |[english]               |{zip -> 94105, city -> SF} |1   |\n",
            "+---+-------+------+------------------------+---------------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0D97ukKbp6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}