{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkjogfMZd1T4lFFQwoN2sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/6th_aug_casestudy.java%2Bspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpf5SaxdlHD5",
        "outputId": "4a82ac6b-b1ab-4079-ef74-22b45171f39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8o-K0vqlJOt",
        "outputId": "4266c3d8-0e9c-49a5-9849-dac613667e3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-MIKR5latt",
        "outputId": "2c8641f8-0070-41e0-95a6-3adeaedd55fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbs1SAHjliy1",
        "outputId": "7f1a759a-15c5-49a8-c956-a3ca353416e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "javac 11.0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkApp.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkApp {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Java Spark App\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        Dataset<Row> df = spark.read().option(\"header\", true).csv(\"input.csv\");\n",
        "        df.show();\n",
        "\n",
        "        df.write().mode(\"overwrite\").parquet(\"output_parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1oG-hlpllb5",
        "outputId": "48921413-02eb-4855-a349-937b12d97a71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SparkApp.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkApp.java"
      ],
      "metadata": {
        "id": "EmAd7qVMlrIN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkApp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9Cql-JwlrsE",
        "outputId": "42aa2ced-05b6-4b9c-c77a-500e7e210d6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:54:37 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:54:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:54:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:54:38 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:54:38 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:54:38 INFO SparkContext: Submitted application: Java Spark App\n",
            "25/08/06 08:54:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:54:38 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:54:38 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:54:38 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:54:38 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:54:38 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:54:38 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:54:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:54:38 INFO Utils: Successfully started service 'sparkDriver' on port 36659.\n",
            "25/08/06 08:54:38 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:54:38 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:54:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:54:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:54:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:54:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-28e92f8b-feee-44eb-bc62-7393681beb6c\n",
            "25/08/06 08:54:39 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:54:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:54:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:54:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:54:39 INFO Executor: Starting executor ID driver on host 25844f9bc21b\n",
            "25/08/06 08:54:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:54:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46339.\n",
            "25/08/06 08:54:39 INFO NettyBlockTransferService: Server created on 25844f9bc21b:46339\n",
            "25/08/06 08:54:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:54:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25844f9bc21b, 46339, None)\n",
            "25/08/06 08:54:39 INFO BlockManagerMasterEndpoint: Registering block manager 25844f9bc21b:46339 with 1767.6 MiB RAM, BlockManagerId(driver, 25844f9bc21b, 46339, None)\n",
            "25/08/06 08:54:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25844f9bc21b, 46339, None)\n",
            "25/08/06 08:54:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25844f9bc21b, 46339, None)\n",
            "25/08/06 08:54:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:54:40 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/input.csv.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 08:54:43 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 08:54:43 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:54:43 INFO SparkUI: Stopped Spark web UI at http://25844f9bc21b:4040\n",
            "25/08/06 08:54:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:54:43 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:54:43 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:54:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:54:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:54:43 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:54:43 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:54:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0576dec-e088-4215-b9e0-4bc6ca703159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Schema.java\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "\n",
        "public class Schema {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Data Ingestion Assignment\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Define schemas manually\n",
        "        StructType productLinesSchema = new StructType()\n",
        "                .add(\"productLine\", DataTypes.StringType)\n",
        "                .add(\"textDescription\", DataTypes.StringType);\n",
        "\n",
        "        StructType productsSchema = new StructType()\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"productName\", DataTypes.StringType)\n",
        "                .add(\"productLine\", DataTypes.StringType);\n",
        "\n",
        "        StructType officesSchema = new StructType()\n",
        "                .add(\"officeCode\", DataTypes.StringType)\n",
        "                .add(\"city\", DataTypes.StringType)\n",
        "                .add(\"country\", DataTypes.StringType);\n",
        "\n",
        "        StructType employeesSchema = new StructType()\n",
        "                .add(\"employeeNumber\", DataTypes.IntegerType)\n",
        "                .add(\"lastName\", DataTypes.StringType)\n",
        "                .add(\"officeCode\", DataTypes.StringType);\n",
        "\n",
        "        StructType customersSchema = new StructType()\n",
        "    .add(\"customerNumber\", DataTypes.IntegerType, true)\n",
        "    .add(\"customerName\", DataTypes.StringType, true)\n",
        "    .add(\"contactLastName\", DataTypes.StringType, true)\n",
        "    .add(\"contactFirstName\", DataTypes.StringType, true)\n",
        "    .add(\"phone\", DataTypes.StringType, true)\n",
        "    .add(\"addressLine1\", DataTypes.StringType, true)\n",
        "    .add(\"addressLine2\", DataTypes.StringType, true)\n",
        "    .add(\"city\", DataTypes.StringType, true)\n",
        "    .add(\"state\", DataTypes.StringType, true)\n",
        "    .add(\"postalCode\", DataTypes.StringType, true)\n",
        "    .add(\"country\", DataTypes.StringType, true)\n",
        "    .add(\"salesRepEmployeeNumber\", DataTypes.IntegerType, true)\n",
        "    .add(\"creditLimit\", DataTypes.DoubleType, true);\n",
        "\n",
        "        StructType paymentsSchema = new StructType()\n",
        "              .add(\"customerNumber\", DataTypes.IntegerType)\n",
        "              .add(\"checkNumber\", DataTypes.StringType)\n",
        "              .add(\"paymentDate\", DataTypes.DateType)\n",
        "              .add(\"amount\", DataTypes.DoubleType);\n",
        "\n",
        "        StructType ordersSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"orderDate\", DataTypes.StringType)\n",
        "                .add(\"customerNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        StructType orderDetailsSchema = new StructType()\n",
        "                .add(\"orderNumber\", DataTypes.IntegerType)\n",
        "                .add(\"productCode\", DataTypes.StringType)\n",
        "                .add(\"quantityOrdered\", DataTypes.IntegerType)\n",
        "                .add(\"priceEach\", DataTypes.DoubleType)\n",
        "                .add(\"orderLineNumber\", DataTypes.IntegerType);\n",
        "\n",
        "        // Base paths\n",
        "        String inputPath = \"\";\n",
        "        String outputPath = \"data/parquet/\";\n",
        "\n",
        "        // Read and write all tables\n",
        "        readAndSave(spark, inputPath + \"productlines.csv\", outputPath + \"productlines\", productLinesSchema);\n",
        "        readAndSave(spark, inputPath + \"products.csv\", outputPath + \"products\", productsSchema);\n",
        "        readAndSave(spark, inputPath + \"offices.csv\", outputPath + \"offices\", officesSchema);\n",
        "        readAndSave(spark, inputPath + \"employees.csv\", outputPath + \"employees\", employeesSchema);\n",
        "        readAndSave(spark, inputPath + \"customers.csv\", outputPath + \"customers\", customersSchema);\n",
        "        readAndSave(spark, inputPath + \"payments.csv\", outputPath + \"payments\", paymentsSchema);\n",
        "        readAndSave(spark, inputPath + \"orders.csv\", outputPath + \"orders\", ordersSchema);\n",
        "        readAndSave(spark, inputPath + \"orderdetails.csv\", outputPath + \"orderdetails\", orderDetailsSchema);\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "\n",
        "    private static void readAndSave(SparkSession spark, String inputCsvPath, String outputParquetPath, StructType schema) {\n",
        "        Dataset<Row> df = spark.read()\n",
        "                .option(\"header\", \"true\")\n",
        "                .schema(schema)\n",
        "                .csv(inputCsvPath);\n",
        "\n",
        "        df.write()\n",
        "                .mode(SaveMode.Overwrite)\n",
        "                .parquet(outputParquetPath);\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ3zuseXluN1",
        "outputId": "b660ec36-95ff-4e25-df4f-6fa1b4eb5311"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Schema.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Schema.java"
      ],
      "metadata": {
        "id": "Mt-pm3Qgl0A5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$SPARK_HOME/jars/*\" Schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXNiGhcVl28X",
        "outputId": "179632a2-5488-4b05-b4b3-636d8138ffb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 08:55:23 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 08:55:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 08:55:24 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:55:24 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 08:55:24 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 08:55:24 INFO SparkContext: Submitted application: Data Ingestion Assignment\n",
            "25/08/06 08:55:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 08:55:24 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 08:55:24 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 08:55:24 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 08:55:24 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 08:55:24 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 08:55:24 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 08:55:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 08:55:25 INFO Utils: Successfully started service 'sparkDriver' on port 44807.\n",
            "25/08/06 08:55:25 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 08:55:25 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 08:55:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 08:55:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 08:55:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 08:55:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ebcb4d66-09f3-4ac8-8b07-14b62a4d73f7\n",
            "25/08/06 08:55:25 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 08:55:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 08:55:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 08:55:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 08:55:26 INFO Executor: Starting executor ID driver on host 25844f9bc21b\n",
            "25/08/06 08:55:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 08:55:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46457.\n",
            "25/08/06 08:55:26 INFO NettyBlockTransferService: Server created on 25844f9bc21b:46457\n",
            "25/08/06 08:55:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 08:55:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25844f9bc21b, 46457, None)\n",
            "25/08/06 08:55:26 INFO BlockManagerMasterEndpoint: Registering block manager 25844f9bc21b:46457 with 1767.6 MiB RAM, BlockManagerId(driver, 25844f9bc21b, 46457, None)\n",
            "25/08/06 08:55:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25844f9bc21b, 46457, None)\n",
            "25/08/06 08:55:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25844f9bc21b, 46457, None)\n",
            "25/08/06 08:55:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 08:55:27 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/productlines.csv.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 08:55:29 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 08:55:29 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 08:55:29 INFO SparkUI: Stopped Spark web UI at http://25844f9bc21b:4040\n",
            "25/08/06 08:55:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 08:55:29 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 08:55:29 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 08:55:29 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 08:55:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 08:55:29 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 08:55:29 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 08:55:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-8acd1c1e-8169-4244-b8cb-cb8ecaf6d61e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "drNH60nsl5M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Zip the data folder\n",
        "!zip -r data_folder.zip /content/drive/MyDrive/4th_aug_2025/csv\n",
        "\n",
        "# Step 2: Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('data_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "szEsEj3jqQkR",
        "outputId": "8a9e1fd2-dd0a-4110-f893-a6f5a420e5d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/ (stored 0%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/productlines.csv (deflated 62%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/products.csv (deflated 71%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/offices.csv (deflated 31%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/employees.csv (deflated 65%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/customers.csv (deflated 50%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/payments.csv (deflated 53%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/orders.csv (deflated 77%)\n",
            "  adding: content/drive/MyDrive/4th_aug_2025/csv/orderdetails.csv (deflated 69%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aea1b884-b9d6-4b47-93fa-ae56a4802d42\", \"data_folder.zip\", 107092)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SparkAnalysis.java\n",
        "import org.apache.spark.sql.*;\n",
        "\n",
        "public class SparkAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "            .appName(\"Product and Order Analysis\")\n",
        "            .master(\"local[*]\")\n",
        "            .getOrCreate();\n",
        "\n",
        "        Dataset<Row> orderDetails = spark.read().parquet(\"data/parquet/orderdetails\");\n",
        "        Dataset<Row> orders = spark.read().parquet(\"data/parquet/orders\");\n",
        "        Dataset<Row> products = spark.read().parquet(\"data/parquet/products\");\n",
        "\n",
        "        // Top 10 products by quantity sold\n",
        "        Dataset<Row> topProducts = orderDetails.groupBy(\"productCode\")\n",
        "            .sum(\"quantityOrdered\")\n",
        "            .orderBy(functions.desc(\"sum(quantityOrdered)\"))\n",
        "            .limit(10);\n",
        "\n",
        "        topProducts.show();\n",
        "\n",
        "        // Join for product-wise revenue\n",
        "        Dataset<Row> revenueData = orderDetails\n",
        "            .join(products, \"productCode\")\n",
        "            .join(orders, \"orderNumber\")\n",
        "            .withColumn(\"revenue\", functions.expr(\"quantityOrdered * priceEach\"));\n",
        "\n",
        "        Dataset<Row> productRevenue = revenueData.groupBy(\"productCode\", \"productName\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalRevenue\"))\n",
        "            .orderBy(functions.desc(\"totalRevenue\"));\n",
        "\n",
        "        productRevenue.show();\n",
        "\n",
        "        // Average order value per customer\n",
        "        Dataset<Row> customerAOV = revenueData.groupBy(\"customerNumber\")\n",
        "            .agg(functions.sum(\"revenue\").alias(\"totalSpent\"),\n",
        "                 functions.countDistinct(\"orderNumber\").alias(\"orderCount\"))\n",
        "            .withColumn(\"averageOrderValue\", functions.expr(\"totalSpent / orderCount\"));\n",
        "\n",
        "        customerAOV.show();\n",
        "\n",
        "        // Save results\n",
        "        topProducts.write().mode(\"overwrite\").parquet(\"data/results/top_products\");\n",
        "        productRevenue.write().mode(\"overwrite\").parquet(\"data/results/product_revenue\");\n",
        "        customerAOV.write().mode(\"overwrite\").parquet(\"data/results/customer_aov\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOFgNxyol9VA",
        "outputId": "550e0a9c-b43b-40bc-c0ed-2a9c337b5c4f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SparkAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" SparkAnalysis.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" SparkAnalysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFdCtChvnAhT",
        "outputId": "cd6f76f6-d513-414e-e222-d5c127286b34"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:17:18 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:17:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:17:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:17:19 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:17:19 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:17:19 INFO SparkContext: Submitted application: Product and Order Analysis\n",
            "25/08/06 09:17:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:17:19 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:17:19 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:17:19 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:17:19 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:17:19 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:17:19 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:17:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:17:19 INFO Utils: Successfully started service 'sparkDriver' on port 36471.\n",
            "25/08/06 09:17:19 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:17:20 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:17:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:17:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:17:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:17:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-93e71e59-bde1-4688-ab3a-09d64de9d0a2\n",
            "25/08/06 09:17:20 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:17:20 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:17:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:17:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 09:17:21 INFO Executor: Starting executor ID driver on host 25844f9bc21b\n",
            "25/08/06 09:17:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:17:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36797.\n",
            "25/08/06 09:17:21 INFO NettyBlockTransferService: Server created on 25844f9bc21b:36797\n",
            "25/08/06 09:17:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:17:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25844f9bc21b, 36797, None)\n",
            "25/08/06 09:17:21 INFO BlockManagerMasterEndpoint: Registering block manager 25844f9bc21b:36797 with 1767.6 MiB RAM, BlockManagerId(driver, 25844f9bc21b, 36797, None)\n",
            "25/08/06 09:17:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25844f9bc21b, 36797, None)\n",
            "25/08/06 09:17:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25844f9bc21b, 36797, None)\n",
            "25/08/06 09:17:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:17:21 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/data/parquet/orderdetails.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 09:17:25 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 09:17:25 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:17:25 INFO SparkUI: Stopped Spark web UI at http://25844f9bc21b:4040\n",
            "25/08/06 09:17:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:17:25 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:17:25 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:17:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:17:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:17:25 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:17:25 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:17:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7b30552-d376-4534-a9bc-56466b8ed0af\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 3"
      ],
      "metadata": {
        "id": "VnSG9prCopSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task3.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "\n",
        "public class Task3 {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 3 - Regional Sales Insights\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Read input Parquet data\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "        Dataset<Row> employees = spark.read().parquet(\"data/parquet/employees\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "\n",
        "        // Register views\n",
        "        offices.createOrReplaceTempView(\"offices\");\n",
        "        employees.createOrReplaceTempView(\"employees\");\n",
        "        customers.createOrReplaceTempView(\"customers\");\n",
        "        payments.createOrReplaceTempView(\"payments\");\n",
        "\n",
        "        // Task 3.1: Sales per region (country, city)\n",
        "        Dataset<Row> salesPerRegion = spark.sql(\n",
        "            \"SELECT o.country, o.city, \" +\n",
        "            \"COUNT(DISTINCT e.employeeNumber) AS totalEmployees, \" +\n",
        "            \"COUNT(DISTINCT c.customerNumber) AS totalCustomers \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"GROUP BY o.country, o.city\"\n",
        "        );\n",
        "\n",
        "        salesPerRegion.show();\n",
        "        salesPerRegion.write().mode(\"overwrite\").parquet(\"data/output/task3/sales_per_region\");\n",
        "\n",
        "        // Task 3.2: Total revenue by country\n",
        "        Dataset<Row> revenueByCountry = spark.sql(\n",
        "            \"SELECT c.country, SUM(payments.amount) AS totalRevenue \" +\n",
        "            \"FROM customers c \" +\n",
        "            \"JOIN payments ON c.customerNumber = payments.customerNumber \" +\n",
        "            \"GROUP BY c.country \" +\n",
        "            \"ORDER BY totalRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        revenueByCountry.show();\n",
        "        revenueByCountry.write().mode(\"overwrite\").parquet(\"data/output/task3/revenue_by_country\");\n",
        "\n",
        "        // Task 3.3: Top-performing offices by revenue\n",
        "        Dataset<Row> topOffices = spark.sql(\n",
        "            \"SELECT o.officeCode, o.city, o.country, SUM(payments.amount) AS officeRevenue \" +\n",
        "            \"FROM offices o \" +\n",
        "            \"JOIN employees e ON o.officeCode = e.officeCode \" +\n",
        "            \"JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber \" +\n",
        "            \"JOIN payments ON c.customerNumber = payments.customerNumber \" +\n",
        "            \"GROUP BY o.officeCode, o.city, o.country \" +\n",
        "            \"ORDER BY officeRevenue DESC\"\n",
        "        );\n",
        "\n",
        "        topOffices.show();\n",
        "        topOffices.write().mode(\"overwrite\").parquet(\"data/output/task3/top_offices\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHM5GQPQoky_",
        "outputId": "6114f85a-4074-4733-dc49-c1363dcf5a52"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Task3.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Task3.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" Task3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdpSqPp2ovsw",
        "outputId": "87360dbc-c136-4c53-82c2-8148e368dfff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:08:04 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:08:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:08:05 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:08:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:08:05 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:08:05 INFO SparkContext: Submitted application: Task 3 - Regional Sales Insights\n",
            "25/08/06 09:08:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:08:05 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:08:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:08:05 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:08:05 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:08:05 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:08:05 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:08:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:08:05 INFO Utils: Successfully started service 'sparkDriver' on port 37887.\n",
            "25/08/06 09:08:05 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:08:05 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:08:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:08:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:08:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:08:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4b41d47c-a9b0-4664-8f4d-6b24d5a370ac\n",
            "25/08/06 09:08:06 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:08:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:08:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:08:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 09:08:06 INFO Executor: Starting executor ID driver on host 25844f9bc21b\n",
            "25/08/06 09:08:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:08:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33233.\n",
            "25/08/06 09:08:06 INFO NettyBlockTransferService: Server created on 25844f9bc21b:33233\n",
            "25/08/06 09:08:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:08:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25844f9bc21b, 33233, None)\n",
            "25/08/06 09:08:07 INFO BlockManagerMasterEndpoint: Registering block manager 25844f9bc21b:33233 with 1767.6 MiB RAM, BlockManagerId(driver, 25844f9bc21b, 33233, None)\n",
            "25/08/06 09:08:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25844f9bc21b, 33233, None)\n",
            "25/08/06 09:08:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25844f9bc21b, 33233, None)\n",
            "25/08/06 09:08:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:08:07 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/data/parquet/offices.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 09:08:10 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 09:08:10 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:08:10 INFO SparkUI: Stopped Spark web UI at http://25844f9bc21b:4040\n",
            "25/08/06 09:08:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:08:10 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:08:10 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:08:10 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:08:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:08:10 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:08:10 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:08:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-106ded1f-c61f-4965-8bbc-11db16c1e4ef\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK4"
      ],
      "metadata": {
        "id": "_UurFMBDo0Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Task4.java\n",
        "import org.apache.spark.api.java.JavaPairRDD;\n",
        "import org.apache.spark.api.java.JavaRDD;\n",
        "import org.apache.spark.api.java.JavaSparkContext;\n",
        "import org.apache.spark.broadcast.Broadcast;\n",
        "import org.apache.spark.sql.*;\n",
        "import org.apache.spark.sql.types.*;\n",
        "import org.apache.spark.storage.StorageLevel;\n",
        "import scala.Tuple2;\n",
        "\n",
        "import java.util.Iterator;\n",
        "\n",
        "public class Task4 {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"Task 4: Performance Optimization\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n",
        "\n",
        "        Dataset<Row> customers = spark.read().parquet(\"data/parquet/customers\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"data/parquet/payments\");\n",
        "        Dataset<Row> offices = spark.read().parquet(\"data/parquet/offices\");\n",
        "\n",
        "        // Broadcast smaller dataset\n",
        "        Broadcast<Dataset<Row>> broadcastOffices = jsc.broadcast(offices);\n",
        "\n",
        "        // Cache payments since we’ll use it multiple times\n",
        "        payments.persist(StorageLevel.MEMORY_AND_DISK());\n",
        "\n",
        "        // 1. Aggregate revenue per country using mapPartitions\n",
        "        JavaRDD<Row> revenueByCountryRDD = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapPartitions(iterator -> {\n",
        "                    java.util.Map<String, Double> map = new java.util.HashMap<>();\n",
        "                    while (iterator.hasNext()) {\n",
        "                        Row row = iterator.next();\n",
        "                        String country = row.getString(0);\n",
        "                        double amount = row.getDouble(1);\n",
        "                        map.put(country, map.getOrDefault(country, 0.0) + amount);\n",
        "                    }\n",
        "                    java.util.List<Row> rows = new java.util.ArrayList<>();\n",
        "                    for (java.util.Map.Entry<String, Double> entry : map.entrySet()) {\n",
        "                        rows.add(RowFactory.create(entry.getKey(), entry.getValue()));\n",
        "                    }\n",
        "                    return rows.iterator();\n",
        "                });\n",
        "\n",
        "        // Define schema\n",
        "        StructType schema = new StructType()\n",
        "                .add(\"country\", DataTypes.StringType)\n",
        "                .add(\"totalRevenue\", DataTypes.DoubleType);\n",
        "\n",
        "        Dataset<Row> revenueByCountry = spark.createDataFrame(revenueByCountryRDD, schema);\n",
        "        revenueByCountry.show();\n",
        "\n",
        "        // 2. Aggregate using aggregateByKey\n",
        "        JavaPairRDD<String, Double> countryRevenuePair = payments\n",
        "                .join(customers, \"customerNumber\")\n",
        "                .select(\"country\", \"amount\")\n",
        "                .javaRDD()\n",
        "                .mapToPair(row -> new Tuple2<>(row.getString(0), row.getDouble(1)));\n",
        "\n",
        "        JavaPairRDD<String, Double> aggregatedRevenue = countryRevenuePair.aggregateByKey(\n",
        "                0.0,\n",
        "                Double::sum,\n",
        "                Double::sum\n",
        "        );\n",
        "\n",
        "        Dataset<Row> aggregatedDF = spark.createDataFrame(\n",
        "                aggregatedRevenue.map(tuple -> RowFactory.create(tuple._1, tuple._2)),\n",
        "                schema\n",
        "        );\n",
        "        aggregatedDF.show();\n",
        "\n",
        "        // 3. Lazy evaluation example\n",
        "        Dataset<Row> lazyEval = payments.filter(\"amount > 1000\");\n",
        "        System.out.println(\"Lazy evaluation example defined. Not triggered yet.\");\n",
        "        lazyEval.show(); // Action triggers execution\n",
        "\n",
        "        // Save output\n",
        "        revenueByCountry.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/revenueByCountry\");\n",
        "        aggregatedDF.write().mode(SaveMode.Overwrite).parquet(\"data/output/task4/aggregatedRevenue\");\n",
        "\n",
        "        // Unpersist after use\n",
        "        payments.unpersist();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syJr2gxFoyOE",
        "outputId": "f4f4f045-d5f1-447a-f2a3-f869059861c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Task4.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFWjV7OQrLkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!javac -cp \"$SPARK_HOME/jars/*\" Task4.java\n",
        "!java -cp \".:$SPARK_HOME/jars/*\" Task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLkKG7lUo5Xg",
        "outputId": "aa4424c6-f013-4398-ea6d-13d4dfa7db75"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 09:17:52 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 09:17:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 09:17:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:17:53 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 09:17:53 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 09:17:53 INFO SparkContext: Submitted application: Task 4: Performance Optimization\n",
            "25/08/06 09:17:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 09:17:53 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 09:17:53 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 09:17:53 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 09:17:53 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 09:17:53 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 09:17:53 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 09:17:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 09:17:54 INFO Utils: Successfully started service 'sparkDriver' on port 44069.\n",
            "25/08/06 09:17:54 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 09:17:54 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 09:17:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 09:17:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 09:17:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 09:17:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b047d401-48ca-4145-833a-f85728d8deb9\n",
            "25/08/06 09:17:54 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 09:17:54 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 09:17:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 09:17:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 09:17:55 INFO Executor: Starting executor ID driver on host 25844f9bc21b\n",
            "25/08/06 09:17:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 09:17:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40559.\n",
            "25/08/06 09:17:55 INFO NettyBlockTransferService: Server created on 25844f9bc21b:40559\n",
            "25/08/06 09:17:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 09:17:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 25844f9bc21b, 40559, None)\n",
            "25/08/06 09:17:55 INFO BlockManagerMasterEndpoint: Registering block manager 25844f9bc21b:40559 with 1767.6 MiB RAM, BlockManagerId(driver, 25844f9bc21b, 40559, None)\n",
            "25/08/06 09:17:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 25844f9bc21b, 40559, None)\n",
            "25/08/06 09:17:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 25844f9bc21b, 40559, None)\n",
            "25/08/06 09:17:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 09:17:56 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/content/data/parquet/customers.\n",
            "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1419)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)\n",
            "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n",
            "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
            "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
            "\tat scala.util.Success.map(Try.scala:213)\n",
            "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
            "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
            "25/08/06 09:17:58 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "25/08/06 09:17:58 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 09:17:58 INFO SparkUI: Stopped Spark web UI at http://25844f9bc21b:4040\n",
            "25/08/06 09:17:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 09:17:58 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 09:17:58 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 09:17:58 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 09:17:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 09:17:58 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 09:17:58 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 09:17:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-51142d20-6f54-4bc0-a515-b76200f21a9a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Zip the data folder\n",
        "!zip -r data_folder.zip /content/drive/MyDrive/4th_aug_2025/csv\n",
        "\n",
        "# Step 2: Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download('data_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "6N8huyW9rM4c",
        "outputId": "ff4f2021-c481-4123-e3bb-900954436fbd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/drive/MyDrive/4th_aug_2025/csv/ (stored 0%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/productlines.csv (deflated 62%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/products.csv (deflated 71%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/offices.csv (deflated 31%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/employees.csv (deflated 65%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/customers.csv (deflated 50%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/payments.csv (deflated 53%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/orders.csv (deflated 77%)\n",
            "updating: content/drive/MyDrive/4th_aug_2025/csv/orderdetails.csv (deflated 69%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fcd7a2f8-b826-4df4-90eb-4fb162b31219\", \"data_folder.zip\", 107092)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}