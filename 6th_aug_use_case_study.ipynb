{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DileepNalle78/pyspark__DileepNalle/blob/main/6th_aug_use_case_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Spark Scala Env."
      ],
      "metadata": {
        "id": "c6Z58K9N1_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78Oxr-PY1pA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3521c349-59f4-4404-96bd-f778c8e73791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to store cached files\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set variables\n",
        "strBasePath=\"/content/drive/MyDrive/IBM-DE-Spark-Scala\"\n",
        "scala_deb_path = strBasePath+\"/scala-2.12.18.deb\"\n",
        "spark_tgz_path = strBasePath+\"/spark-3.4.1-bin-hadoop3.tgz\"\n",
        "\n",
        "!mkdir -p /content/tmp\n",
        "import os\n",
        "# Download Scala .deb if not cached\n",
        "if not os.path.exists(scala_deb_path):\n",
        "    !wget -O \"{scala_deb_path}\" https://github.com/scala/scala/releases/download/v2.12.18/scala-2.12.18.deb\n",
        "\n",
        "# Download Spark tgz if not cached\n",
        "if not os.path.exists(spark_tgz_path):\n",
        "    !wget -O \"{spark_tgz_path}\" https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Copy cached files to working dir\n",
        "!cp \"{scala_deb_path}\" /content/tmp/scala-2.12.18.deb\n",
        "!cp \"{spark_tgz_path}\" /content/tmp/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install Java if not already present\n",
        "!java -version || apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install Scala\n",
        "!dpkg -i /content/tmp/scala-2.12.18.deb\n",
        "\n",
        "# Extract Spark\n",
        "!tar xf /content/tmp/spark-3.4.1-bin-hadoop3.tgz -C /content\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "# Confirm installation\n",
        "!java -version\n",
        "!scala -version\n",
        "!scalac -version\n",
        "!echo \"Spark path: $SPARK_HOME\"\n",
        "!ls $SPARK_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xyJiL9W-2lXu",
        "outputId": "4d789a06-44d5-404b-b51b-0dbdd331591d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Selecting previously unselected package scala.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack /content/tmp/scala-2.12.18.deb ...\n",
            "Unpacking scala (2.12.18-400) ...\n",
            "Setting up scala (2.12.18-400) ...\n",
            "Creating system group: scala\n",
            "Creating system user: scala in scala with scala daemon-user and shell /bin/false\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Scala code runner version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Scala compiler version 2.12.18 -- Copyright 2002-2023, LAMP/EPFL and Lightbend, Inc.\n",
            "Spark path: /content/spark-3.4.1-bin-hadoop3\n",
            "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\n",
            "conf  examples\tkubernetes  licenses  python  README.md  sbin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Hello World in JAVA"
      ],
      "metadata": {
        "id": "P86TPUCo6-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CreateDataFrame.java\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Encoders;\n",
        "import java.util.Arrays;\n",
        "import java.util.List;\n",
        "\n",
        "public class CreateDataFrame {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"CreateDataFrameExample\")\n",
        "                .master(\"local[*]\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Sample data\n",
        "        List<String> data = Arrays.asList(\"Java\", \"Python\", \"Scala\");\n",
        "\n",
        "        // Create DataFrame from a list\n",
        "        Dataset<String> df = spark.createDataset(data, Encoders.STRING());\n",
        "\n",
        "        // Show the DataFrame content\n",
        "        df.show();\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_aQP7Q28De",
        "outputId": "3273e0e9-68e6-40de-d98e-0e767ec6a069"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CreateDataFrame.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!javac -cp \"$spark_home/jars/*\" CreateDataFrame.java"
      ],
      "metadata": {
        "id": "sIaNIoC56kLN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "spark_home = os.environ.get(\"SPARK_HOME\")\n",
        "!java -cp \"$spark_home/jars/*:.\" CreateDataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS5Z3wOblVCw",
        "outputId": "0d7a9c83-6b1f-47b5-e28d-6beffa8d151a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 04:31:24 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 04:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 04:31:25 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:25 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 04:31:25 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:25 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 04:31:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 04:31:25 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 04:31:25 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 04:31:25 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 04:31:25 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 04:31:25 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 04:31:25 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 04:31:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 04:31:26 INFO Utils: Successfully started service 'sparkDriver' on port 46115.\n",
            "25/08/06 04:31:26 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 04:31:26 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 04:31:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 04:31:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 04:31:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 04:31:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5c7f7987-5b36-424d-b913-71f406a9a3a6\n",
            "25/08/06 04:31:26 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 04:31:26 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 04:31:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 04:31:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 04:31:27 INFO Executor: Starting executor ID driver on host 921d066169b7\n",
            "25/08/06 04:31:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 04:31:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42105.\n",
            "25/08/06 04:31:27 INFO NettyBlockTransferService: Server created on 921d066169b7:42105\n",
            "25/08/06 04:31:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 04:31:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 921d066169b7, 42105, None)\n",
            "25/08/06 04:31:27 INFO BlockManagerMasterEndpoint: Registering block manager 921d066169b7:42105 with 1767.6 MiB RAM, BlockManagerId(driver, 921d066169b7, 42105, None)\n",
            "25/08/06 04:31:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 921d066169b7, 42105, None)\n",
            "25/08/06 04:31:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 921d066169b7, 42105, None)\n",
            "25/08/06 04:31:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 04:31:30 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 04:31:32 INFO CodeGenerator: Code generated in 517.959746 ms\n",
            "25/08/06 04:31:35 INFO CodeGenerator: Code generated in 61.068321 ms\n",
            "25/08/06 04:31:35 INFO CodeGenerator: Code generated in 24.058674 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 04:31:35 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 04:31:35 INFO SparkUI: Stopped Spark web UI at http://921d066169b7:4040\n",
            "25/08/06 04:31:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 04:31:35 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 04:31:35 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 04:31:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 04:31:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 04:31:35 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 04:31:35 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 04:31:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2f63d76-17bc-441a-a3de-ba4201a6f4f9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set varible\n",
        "\n",
        "!SPARK_HOME=/content/spark-3.3.2-bin-hadoop3\n",
        "!JARS=$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\n"
      ],
      "metadata": {
        "id": "M4ClcALbla3J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $SPARK_HOME\n",
        "!echo $JARS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-PmEhD51yxA",
        "outputId": "4a3c71aa-8631-4e99-faf9-0ee72c81cfaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.4.1-bin-hadoop3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \"$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame.java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9urYc1e1_CI",
        "outputId": "7e95181d-d84b-4a52-e655-44e19daeacb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 04:31:41 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 04:31:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 04:31:42 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:42 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 04:31:42 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:42 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 04:31:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 04:31:42 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 04:31:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 04:31:42 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 04:31:42 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 04:31:42 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 04:31:42 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 04:31:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 04:31:43 INFO Utils: Successfully started service 'sparkDriver' on port 35335.\n",
            "25/08/06 04:31:43 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 04:31:43 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 04:31:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 04:31:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 04:31:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 04:31:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0c8b62ec-4459-4d74-ba26-a6d2612ba8f1\n",
            "25/08/06 04:31:43 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 04:31:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 04:31:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 04:31:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 04:31:44 INFO Executor: Starting executor ID driver on host 921d066169b7\n",
            "25/08/06 04:31:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 04:31:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37535.\n",
            "25/08/06 04:31:44 INFO NettyBlockTransferService: Server created on 921d066169b7:37535\n",
            "25/08/06 04:31:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 04:31:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 921d066169b7, 37535, None)\n",
            "25/08/06 04:31:44 INFO BlockManagerMasterEndpoint: Registering block manager 921d066169b7:37535 with 1767.6 MiB RAM, BlockManagerId(driver, 921d066169b7, 37535, None)\n",
            "25/08/06 04:31:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 921d066169b7, 37535, None)\n",
            "25/08/06 04:31:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 921d066169b7, 37535, None)\n",
            "25/08/06 04:31:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 04:31:47 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 04:31:51 INFO CodeGenerator: Code generated in 831.839908 ms\n",
            "25/08/06 04:31:53 INFO CodeGenerator: Code generated in 23.369389 ms\n",
            "25/08/06 04:31:53 INFO CodeGenerator: Code generated in 47.010003 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 04:31:53 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 04:31:53 INFO SparkUI: Stopped Spark web UI at http://921d066169b7:4040\n",
            "25/08/06 04:31:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 04:31:53 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 04:31:53 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 04:31:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 04:31:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 04:31:53 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 04:31:53 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 04:31:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-01e8df31-f719-48f1-bfb7-c2419a83a417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -cp \".:$(echo $SPARK_HOME/jars/*.jar | tr ' ' ':')\" CreateDataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y69uEWm92J3S",
        "outputId": "8f994ff6-ca8a-411b-8396-69e5c2ba5944"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 04:31:56 INFO SparkContext: Running Spark version 3.4.1\n",
            "25/08/06 04:31:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 04:31:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 04:31:57 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 04:31:57 INFO SparkContext: Submitted application: CreateDataFrameExample\n",
            "25/08/06 04:31:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 04:31:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 04:31:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 04:31:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 04:31:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 04:31:57 INFO Utils: Successfully started service 'sparkDriver' on port 44803.\n",
            "25/08/06 04:31:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/content/spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 04:31:57 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 04:31:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 04:31:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 04:31:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 04:31:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-438e347e-679a-44cc-885a-6b9f32e390fc\n",
            "25/08/06 04:31:58 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 04:31:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 04:31:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 04:31:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/08/06 04:31:58 INFO Executor: Starting executor ID driver on host 921d066169b7\n",
            "25/08/06 04:31:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 04:31:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34639.\n",
            "25/08/06 04:31:59 INFO NettyBlockTransferService: Server created on 921d066169b7:34639\n",
            "25/08/06 04:31:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 04:31:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManagerMasterEndpoint: Registering block manager 921d066169b7:34639 with 1767.6 MiB RAM, BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:31:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 921d066169b7, 34639, None)\n",
            "25/08/06 04:32:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 04:32:02 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 04:32:06 INFO CodeGenerator: Code generated in 654.355278 ms\n",
            "25/08/06 04:32:08 INFO CodeGenerator: Code generated in 36.013936 ms\n",
            "25/08/06 04:32:08 INFO CodeGenerator: Code generated in 25.784043 ms\n",
            "+------+\n",
            "| value|\n",
            "+------+\n",
            "|  Java|\n",
            "|Python|\n",
            "| Scala|\n",
            "+------+\n",
            "\n",
            "25/08/06 04:32:08 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 04:32:08 INFO SparkUI: Stopped Spark web UI at http://921d066169b7:4040\n",
            "25/08/06 04:32:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 04:32:08 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 04:32:08 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 04:32:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 04:32:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 04:32:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 04:32:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 04:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-7b934dbf-5cd3-49e0-8cf9-d98a3ae8bc0c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0QvYqpTo2s1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e1a386a"
      },
      "source": [
        "# Task\n",
        "Perform data ingestion, setup, and product/order analysis using Apache Spark with Java based on the provided dataset tables and instructions. The analysis should include finding the top 10 products by quantity sold, calculating product-wise revenue, and calculating the average order value. The data should be read from CSV files, converted to Parquet, and stored in \"/data/parquet/\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3a4c9a"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "### Subtask:\n",
        "Install Java and set up Apache Spark as described. Link the project with a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01438b51"
      },
      "source": [
        "## Data ingestion & setup\n",
        "\n",
        "### Subtask:\n",
        "Read the CSV files for all the dataset tables into Spark DataFrames, define schemas, convert them to Parquet format, and save them to the specified location.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d61dfd"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a SparkSession object to start the Spark application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7703e7e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to Parquet Conversion\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ba00c7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define schemas for each table and read the CSV files into DataFrames using the defined schemas. Then convert and save the DataFrames to Parquet format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "0150d21d",
        "outputId": "3709a943-7644-425e-f74b-cc883dd55b38"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "# Define schemas for each table\n",
        "schemas = {\n",
        "    \"customers\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"customerName\", StringType()),\n",
        "        StructField(\"contactLastName\", StringType()),\n",
        "        StructField(\"contactFirstName\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"salesRepEmployeeNumber\", IntegerType()),\n",
        "        StructField(\"creditLimit\", DoubleType())\n",
        "    ]),\n",
        "    \"employees\": StructType([\n",
        "        StructField(\"employeeNumber\", IntegerType()),\n",
        "        StructField(\"lastName\", StringType()),\n",
        "        StructField(\"firstName\", StringType()),\n",
        "        StructField(\"extension\", StringType()),\n",
        "        StructField(\"email\", StringType()),\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"reportsTo\", IntegerType()),\n",
        "        StructField(\"jobTitle\", StringType())\n",
        "    ]),\n",
        "    \"offices\": StructType([\n",
        "        StructField(\"officeCode\", StringType()),\n",
        "        StructField(\"city\", StringType()),\n",
        "        StructField(\"phone\", StringType()),\n",
        "        StructField(\"addressLine1\", StringType()),\n",
        "        StructField(\"addressLine2\", StringType()),\n",
        "        StructField(\"state\", StringType()),\n",
        "        StructField(\"country\", StringType()),\n",
        "        StructField(\"postalCode\", StringType()),\n",
        "        StructField(\"territory\", StringType())\n",
        "    ]),\n",
        "    \"orderdetails\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"quantityOrdered\", IntegerType()),\n",
        "        StructField(\"priceEach\", DoubleType()),\n",
        "        StructField(\"orderLineNumber\", IntegerType())\n",
        "    ]),\n",
        "    \"orders\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType()),\n",
        "        StructField(\"orderDate\", DateType()),\n",
        "        StructField(\"requiredDate\", DateType()),\n",
        "        StructField(\"shippedDate\", DateType()),\n",
        "        StructField(\"status\", StringType()),\n",
        "        StructField(\"comments\", StringType()),\n",
        "        StructField(\"customerNumber\", IntegerType())\n",
        "    ]),\n",
        "    \"payments\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType()),\n",
        "        StructField(\"checkNumber\", StringType()),\n",
        "        StructField(\"paymentDate\", DateType()),\n",
        "        StructField(\"amount\", DoubleType())\n",
        "    ]),\n",
        "    \"productlines\": StructType([\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"textDescription\", StringType()),\n",
        "        StructField(\"htmlDescription\", StringType()),\n",
        "        StructField(\"image\", StringType())\n",
        "    ]),\n",
        "    \"products\": StructType([\n",
        "        StructField(\"productCode\", StringType()),\n",
        "        StructField(\"productName\", StringType()),\n",
        "        StructField(\"productLine\", StringType()),\n",
        "        StructField(\"productScale\", StringType()),\n",
        "        StructField(\"productVendor\", StringType()),\n",
        "        StructField(\"productDescription\", StringType()),\n",
        "        StructField(\"quantityInStock\", IntegerType()),\n",
        "        StructField(\"buyPrice\", DoubleType()),\n",
        "        StructField(\"MSRP\", DoubleType())\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Read CSVs, convert to Parquet, and save\n",
        "base_input_path = \"/content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/\"\n",
        "base_output_path = \"/data/parquet/\"\n",
        "\n",
        "import os\n",
        "\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"Processing {table_name}...\")\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "    print(f\"Saved {table_name} to {parquet_path}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing customers...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/customers.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2181162266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing {table_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Create output directory if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/customers.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfdb6a49"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the input file path was incorrect. I need to correct the path to where the data is located.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9RXGYHdrZFb",
        "outputId": "16be55bb-1e8a-42a5-e0f3-7757041ea048"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save\n",
        "base_input_path = \"/content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/\"\n",
        "base_output_path = \"/data/parquet/\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Verify the input directory exists and list its contents for debugging\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"Input path exists. Contents: {os.listdir(base_input_path)}\")\n",
        "else:\n",
        "    print(f\"Input path does NOT exist.\")\n",
        "\n",
        "\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"Processing {table_name}...\")\n",
        "\n",
        "    # Add a check to see if the individual CSV file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: Input file not found: {csv_path}\")\n",
        "        continue # Skip to the next table if the file is not found\n",
        "\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "    print(f\"Saved {table_name} to {parquet_path}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/\n",
            "Input path does NOT exist.\n",
            "Processing customers...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/customers.csv\n",
            "Processing employees...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/employees.csv\n",
            "Processing offices...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/offices.csv\n",
            "Processing orderdetails...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/orderdetails.csv\n",
            "Processing orders...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/orders.csv\n",
            "Processing payments...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/payments.csv\n",
            "Processing productlines...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/productlines.csv\n",
            "Processing products...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03ffa76"
      },
      "source": [
        "**Reasoning**:\n",
        "The input path is incorrect. I need to determine the correct path to the data and try reading the files again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb621e5a",
        "outputId": "6e4e0ca0-485e-4d13-8b90-9eb2563100cf"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save\n",
        "# Assuming the data is in the home directory of the notebook\n",
        "base_input_path = \"./data/csv/\"\n",
        "base_output_path = \"/data/parquet/\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Verify the input directory exists and list its contents for debugging\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"Input path exists. Contents: {os.listdir(base_input_path)}\")\n",
        "else:\n",
        "    print(f\"Input path does NOT exist.\")\n",
        "\n",
        "\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"Processing {table_name}...\")\n",
        "\n",
        "    # Add a check to see if the individual CSV file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: Input file not found: {csv_path}\")\n",
        "        continue # Skip to the next table if the file is not found\n",
        "\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "    print(f\"Saved {table_name} to {parquet_path}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: ./data/csv/\n",
            "Input path does NOT exist.\n",
            "Processing customers...\n",
            "Error: Input file not found: ./data/csv/customers.csv\n",
            "Processing employees...\n",
            "Error: Input file not found: ./data/csv/employees.csv\n",
            "Processing offices...\n",
            "Error: Input file not found: ./data/csv/offices.csv\n",
            "Processing orderdetails...\n",
            "Error: Input file not found: ./data/csv/orderdetails.csv\n",
            "Processing orders...\n",
            "Error: Input file not found: ./data/csv/orders.csv\n",
            "Processing payments...\n",
            "Error: Input file not found: ./data/csv/payments.csv\n",
            "Processing productlines...\n",
            "Error: Input file not found: ./data/csv/productlines.csv\n",
            "Processing products...\n",
            "Error: Input file not found: ./data/csv/products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae6448ed"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to locate the data failed. I need to determine the correct location of the CSV files to proceed with reading and converting them to Parquet format. I will try listing files in the root directory to find the correct path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54a529a6",
        "outputId": "9f15b857-c027-40ac-c84d-78345f5793de"
      },
      "source": [
        "!ls /content/\n",
        "!ls /content/drive/MyDrive/IBM-DE-Spark-Scala/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CreateDataFrame.class  drive\t    spark-3.4.1-bin-hadoop3\n",
            "CreateDataFrame.java   sample_data  tmp\n",
            "scala-2.12.18.deb  spark-3.4.1-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62d2ad19"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the output of the previous command, the data directory 'data' is not directly under `/content/drive/MyDrive/IBM-DE-Spark-Scala/`. I need to check the parent directory to see if the data is located there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ad9b8c",
        "outputId": "3170fee3-f0d1-43a3-80b1-de8f3a9cfbca"
      },
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'005OC1744_Nalle  Dileep kumar_IBM Onboarding - Confidentiality Agreement (1)_copy.pdf'\n",
            "'005OC1744_Nallekumar Dileep_OL_cdOutPdf__copy.pdf'\n",
            "'27th event 2025'\n",
            " 4th_aug_2025\n",
            "'Colab Notebooks'\n",
            "'Copy of UseCaseStudy-Spark-Scala-ProductAnalyticsWorkflowAutomation (1).docx'\n",
            "'Copy of UseCaseStudy-Spark-Scala-ProductAnalyticsWorkflowAutomation.docx'\n",
            " empdata\n",
            " IBM-DE-Spark-Scala\n",
            " passbook.pdf\n",
            " passport.pdf\n",
            "'pc .pdf'\n",
            " py_spark_for_demo_data\n",
            "'Untitled form.gform'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27ed5d43"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'data' directory is not directly in '/content/drive/MyDrive/IBM-DE-Spark-Scala/' or '/content/drive/MyDrive/'. It appears to be in a different location or might not be mounted correctly. I will try to list the contents of the root directory of the mounted drive to see if the data is located there.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c33e188",
        "outputId": "efe1b713-fc70-4cb9-9436-ca7b82081747"
      },
      "source": [
        "!ls /content/drive/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d89bff93"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to find the data directory failed. It seems the data is not directly in the common locations within the mounted drive. Since the initial setup code mounted the drive to '/content/drive', and the 'MyDrive' directory is listed there, the data should be within 'MyDrive'. The previous attempts at path were incorrect. I will try the original path again, assuming there might have been a temporary issue in the previous runs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1750c99c",
        "outputId": "ed238c10-d752-456c-c999-e396fc1d88c1"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save\n",
        "base_input_path = \"/content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/\"\n",
        "base_output_path = \"/data/parquet/\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Verify the input directory exists and list its contents for debugging\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"Input path exists. Contents: {os.listdir(base_input_path)}\")\n",
        "else:\n",
        "    print(f\"Input path does NOT exist.\")\n",
        "\n",
        "\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"Processing {table_name}...\")\n",
        "\n",
        "    # Add a check to see if the individual CSV file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: Input file not found: {csv_path}\")\n",
        "        continue # Skip to the next table if the file is not found\n",
        "\n",
        "    df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "    df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "    print(f\"Saved {table_name} to {parquet_path}\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/\n",
            "Input path does NOT exist.\n",
            "Processing customers...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/customers.csv\n",
            "Processing employees...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/employees.csv\n",
            "Processing offices...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/offices.csv\n",
            "Processing orderdetails...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/orderdetails.csv\n",
            "Processing orders...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/orders.csv\n",
            "Processing payments...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/payments.csv\n",
            "Processing productlines...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/productlines.csv\n",
            "Processing products...\n",
            "Error: Input file not found: /content/drive/MyDrive/IBM-DE-Spark-Scala/data/csv/products.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699d930c"
      },
      "source": [
        "## Product & order analysis (part 1)\n",
        "\n",
        "### Subtask:\n",
        "Find the top 10 products based on the total quantity sold by joining the relevant tables and performing aggregations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "009fc819"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the 'orderdetails' and 'products' Parquet files into Spark DataFrames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "e00c1f62",
        "outputId": "4d6b2bd8-b436-4f1f-e2e0-a533c7f8d0bc"
      },
      "source": [
        "orderdetails_df = spark.read.parquet(\"/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/data/parquet/products\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/data/parquet/orderdetails.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2915887684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morderdetails_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/parquet/orderdetails\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mproducts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/parquet/products\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/data/parquet/orderdetails."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7506781"
      },
      "source": [
        "## Product & order analysis (part 1) - retry 1\n",
        "\n",
        "### Subtask:\n",
        "Find the top 10 products based on the total quantity sold by joining the relevant tables and performing aggregations. Retry this step as the input files were not found in the previous attempt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378274d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the orderdetails and products Parquet files from the specified path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "50e6a65b",
        "outputId": "a84f12a3-d7f8-4167-b416-d3c10e9ffe89"
      },
      "source": [
        "orderdetails_df = spark.read.parquet(\"/data/parquet/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/data/parquet/products\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/data/parquet/orderdetails.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2915887684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morderdetails_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/parquet/orderdetails\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mproducts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/parquet/products\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    542\u001b[0m         )\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/data/parquet/orderdetails."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c87c21"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to read the Parquet files failed because the path was not found. I will now check the contents of the `/data/parquet/` directory to see if the files were created in the previous data ingestion step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsYdeTHTr6-_",
        "outputId": "9653d084-77c5-40c8-ddc4-774bfff7bbfb"
      },
      "source": [
        "!ls /data/parquet/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/data/parquet/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dafdba9"
      },
      "source": [
        "**Reasoning**:\n",
        "The `/data/parquet/` directory does not exist, which is why the previous read operations failed. The data ingestion step in the previous subtask also failed to create these files because it could not find the source CSV files. I need to re-run the data ingestion step to create the Parquet files before attempting to read them for analysis. Since the previous attempt to find the CSVs failed, I will try a different path that was shown to contain data in a previous `ls` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c967594",
        "outputId": "9e1d444d-5fd4-4bf6-e852-8d0133daaf7a"
      },
      "source": [
        "# Read CSVs, convert to Parquet, and save\n",
        "base_input_path = \"/content/drive/MyDrive/4th_aug_2025/csv\"\n",
        "base_output_path = \"/content/drive/MyDrive/4th_aug_2025/out_put\"\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to Parquet Conversion\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schemas for each table (as defined in the previous step)\n",
        "schemas = {\n",
        "    \"customers\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType(), True),\n",
        "        StructField(\"customerName\", StringType(), True),\n",
        "        StructField(\"contactLastName\", StringType(), True),\n",
        "        StructField(\"contactFirstName\", StringType(), True),\n",
        "        StructField(\"phone\", StringType(), True),\n",
        "        StructField(\"addressLine1\", StringType(), True),\n",
        "        StructField(\"addressLine2\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"state\", StringType(), True),\n",
        "        StructField(\"postalCode\", StringType(), True),\n",
        "        StructField(\"country\", StringType(), True),\n",
        "        StructField(\"salesRepEmployeeNumber\", IntegerType(), True),\n",
        "        StructField(\"creditLimit\", DoubleType(), True)\n",
        "    ]),\n",
        "    \"employees\": StructType([\n",
        "        StructField(\"employeeNumber\", IntegerType(), True),\n",
        "        StructField(\"lastName\", StringType(), True),\n",
        "        StructField(\"firstName\", StringType(), True),\n",
        "        StructField(\"extension\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"officeCode\", StringType(), True),\n",
        "        StructField(\"reportsTo\", IntegerType(), True),\n",
        "        StructField(\"jobTitle\", StringType(), True)\n",
        "    ]),\n",
        "    \"offices\": StructType([\n",
        "        StructField(\"officeCode\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"phone\", StringType(), True),\n",
        "        StructField(\"addressLine1\", StringType(), True),\n",
        "        StructField(\"addressLine2\", StringType(), True),\n",
        "        StructField(\"state\", StringType(), True),\n",
        "        StructField(\"country\", StringType(), True),\n",
        "        StructField(\"postalCode\", StringType(), True),\n",
        "        StructField(\"territory\", StringType(), True)\n",
        "    ]),\n",
        "    \"orderdetails\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType(), True),\n",
        "        StructField(\"productCode\", StringType(), True),\n",
        "        StructField(\"quantityOrdered\", IntegerType(), True),\n",
        "        StructField(\"priceEach\", DoubleType(), True),\n",
        "        StructField(\"orderLineNumber\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"orders\": StructType([\n",
        "        StructField(\"orderNumber\", IntegerType(), True),\n",
        "        StructField(\"orderDate\", DateType(), True),\n",
        "        StructField(\"requiredDate\", DateType(), True),\n",
        "        StructField(\"shippedDate\", DateType(), True),\n",
        "        StructField(\"status\", StringType(), True),\n",
        "        StructField(\"comments\", StringType(), True),\n",
        "        StructField(\"customerNumber\", IntegerType(), True)\n",
        "    ]),\n",
        "    \"payments\": StructType([\n",
        "        StructField(\"customerNumber\", IntegerType(), True),\n",
        "        StructField(\"checkNumber\", StringType(), True),\n",
        "        StructField(\"paymentDate\", DateType(), True),\n",
        "        StructField(\"amount\", DoubleType(), True)\n",
        "    ]),\n",
        "    \"productlines\": StructType([\n",
        "        StructField(\"productLine\", StringType(), True),\n",
        "        StructField(\"textDescription\", StringType(), True),\n",
        "        StructField(\"htmlDescription\", StringType(), True),\n",
        "        StructField(\"image\", StringType(), True)\n",
        "    ]),\n",
        "    \"products\": StructType([\n",
        "        StructField(\"productCode\", StringType(), True),\n",
        "        StructField(\"productName\", StringType(), True),\n",
        "        StructField(\"productLine\", StringType(), True),\n",
        "        StructField(\"productScale\", StringType(), True),\n",
        "        StructField(\"productVendor\", StringType(), True),\n",
        "        StructField(\"productDescription\", StringType(), True),\n",
        "        StructField(\"quantityInStock\", IntegerType(), True),\n",
        "        StructField(\"buyPrice\", DoubleType(), True),\n",
        "        StructField(\"MSRP\", DoubleType(), True)\n",
        "    ])\n",
        "}\n",
        "\n",
        "\n",
        "# Verify the input directory exists and list its contents for debugging\n",
        "print(f\"Checking input path: {base_input_path}\")\n",
        "if os.path.exists(base_input_path):\n",
        "    print(f\"Input path exists. Contents: {os.listdir(base_input_path)}\")\n",
        "else:\n",
        "    print(f\"Input path does NOT exist.\")\n",
        "\n",
        "\n",
        "for table_name, schema in schemas.items():\n",
        "    csv_path = os.path.join(base_input_path, f\"{table_name}.csv\")\n",
        "    parquet_path = os.path.join(base_output_path, table_name)\n",
        "\n",
        "    print(f\"Processing {table_name} from {csv_path}...\")\n",
        "\n",
        "    # Add a check to see if the individual CSV file exists\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"Error: Input file not found: {csv_path}\")\n",
        "        continue # Skip to the next table if the file is not found\n",
        "\n",
        "    try:\n",
        "        df = spark.read.csv(csv_path, header=True, schema=schema)\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(parquet_path, exist_ok=True)\n",
        "\n",
        "        df.write.parquet(parquet_path, mode=\"overwrite\")\n",
        "        print(f\"Saved {table_name} to {parquet_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {table_name}: {e}\")\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking input path: /content/drive/MyDrive/4th_aug_2025/csv\n",
            "Input path exists. Contents: ['productlines.csv', 'products.csv', 'offices.csv', 'employees.csv', 'customers.csv', 'payments.csv', 'orders.csv', 'orderdetails.csv']\n",
            "Processing customers from /content/drive/MyDrive/4th_aug_2025/csv/customers.csv...\n",
            "Saved customers to /content/drive/MyDrive/4th_aug_2025/out_put/customers\n",
            "Processing employees from /content/drive/MyDrive/4th_aug_2025/csv/employees.csv...\n",
            "Saved employees to /content/drive/MyDrive/4th_aug_2025/out_put/employees\n",
            "Processing offices from /content/drive/MyDrive/4th_aug_2025/csv/offices.csv...\n",
            "Saved offices to /content/drive/MyDrive/4th_aug_2025/out_put/offices\n",
            "Processing orderdetails from /content/drive/MyDrive/4th_aug_2025/csv/orderdetails.csv...\n",
            "Saved orderdetails to /content/drive/MyDrive/4th_aug_2025/out_put/orderdetails\n",
            "Processing orders from /content/drive/MyDrive/4th_aug_2025/csv/orders.csv...\n",
            "Saved orders to /content/drive/MyDrive/4th_aug_2025/out_put/orders\n",
            "Processing payments from /content/drive/MyDrive/4th_aug_2025/csv/payments.csv...\n",
            "Saved payments to /content/drive/MyDrive/4th_aug_2025/out_put/payments\n",
            "Processing productlines from /content/drive/MyDrive/4th_aug_2025/csv/productlines.csv...\n",
            "Saved productlines to /content/drive/MyDrive/4th_aug_2025/out_put/productlines\n",
            "Processing products from /content/drive/MyDrive/4th_aug_2025/csv/products.csv...\n",
            "Saved products to /content/drive/MyDrive/4th_aug_2025/out_put/products\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea6d591"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully ingested the data and saved the Parquet files to `/content/drive/MyDrive/4th_aug_2025/out_put/`. Now I can proceed with the analysis by reading the `orderdetails` and `products` Parquet files from this correct path, joining them, grouping by product name, summing the quantity ordered, ordering the results, and showing the top 10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f20e581c",
        "outputId": "225dde74-59af-4220-e998-7af006db3903"
      },
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Read the Parquet files from the correct location\n",
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orderdetails\")\n",
        "products_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/products\")\n",
        "\n",
        "# Join the dataframes\n",
        "joined_df = orderdetails_df.join(products_df, \"productCode\")\n",
        "\n",
        "# Group by product name and sum the quantity ordered\n",
        "top_products_df = joined_df.groupBy(\"productName\") \\\n",
        "                           .agg(sum(\"quantityOrdered\").alias(\"totalQuantitySold\"))\n",
        "\n",
        "# Order by total quantity sold in descending order and show top 10\n",
        "top_10_products_df = top_products_df.orderBy(top_products_df[\"totalQuantitySold\"].desc()).limit(10)\n",
        "\n",
        "# Display the result\n",
        "top_10_products_df.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+\n",
            "|         productName|totalQuantitySold|\n",
            "+--------------------+-----------------+\n",
            "|1992 Ferrari 360 ...|             1808|\n",
            "|1937 Lincoln Berline|             1111|\n",
            "|American Airlines...|             1085|\n",
            "|1941 Chevrolet Sp...|             1076|\n",
            "|1930 Buick Marque...|             1074|\n",
            "|    1940s Ford truck|             1061|\n",
            "|1969 Harley David...|             1057|\n",
            "|   1957 Chevy Pickup|             1056|\n",
            "|1964 Mercedes Tou...|             1053|\n",
            "|1956 Porsche 356A...|             1052|\n",
            "+--------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a187577"
      },
      "source": [
        "## Product & order analysis (part 2)\n",
        "\n",
        "### Subtask:\n",
        "Join `orders`, `orderdetails`, and `products` tables to calculate and display the revenue for each product.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dd5090b"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the orders Parquet file, join with orderdetails and products, calculate revenue, group by product, and display the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f63647b6",
        "outputId": "f926d7a6-60fc-4102-c5c3-9519f67de9c1"
      },
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Read the orders Parquet file\n",
        "orders_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orders\")\n",
        "\n",
        "# Join orders_df with orderdetails_df\n",
        "joined_orders_details_df = orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Join the result with products_df\n",
        "joined_df = joined_orders_details_df.join(products_df, \"productCode\")\n",
        "\n",
        "# Calculate revenue for each product\n",
        "product_revenue_df = joined_df.withColumn(\"revenue\", col(\"quantityOrdered\") * col(\"priceEach\"))\n",
        "\n",
        "# Group by productName and sum the revenue\n",
        "total_product_revenue_df = product_revenue_df.groupBy(\"productName\") \\\n",
        "                                           .agg(sum(\"revenue\").alias(\"totalRevenue\"))\n",
        "\n",
        "# Display the product revenue\n",
        "total_product_revenue_df.show(truncate=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------+------------------+\n",
            "|productName                                |totalRevenue      |\n",
            "+-------------------------------------------+------------------+\n",
            "|1996 Moto Guzzi 1100i                      |109998.81999999998|\n",
            "|1936 Chrysler Airflow                      |88434.45999999999 |\n",
            "|18th Century Vintage Horse Carriage        |85328.56999999998 |\n",
            "|The Titanic                                |84992.24999999999 |\n",
            "|1958 Setra Bus                             |119085.24999999999|\n",
            "|Diamond T620 Semi-Skirted Tanker           |101137.54999999999|\n",
            "|2001 Ferrari Enzo                          |190755.86         |\n",
            "|The Queen Mary                             |78919.05999999998 |\n",
            "|1930 Buick Marquette Phaeton               |41599.24          |\n",
            "|The Mayflower                              |69531.61          |\n",
            "|1958 Chevy Corvette Limited Edition        |31627.960000000003|\n",
            "|1966 Shelby Cobra 427 S/C                  |42015.53999999999 |\n",
            "|1968 Dodge Charger                         |98718.76000000001 |\n",
            "|1948 Porsche 356-A Roadster                |66455.62          |\n",
            "|The USS Constitution Ship                  |66697.13          |\n",
            "|1982 Camaro Z28                            |89272.65000000001 |\n",
            "|1936 Mercedes Benz 500k Roadster           |29763.39          |\n",
            "|1993 Mazda RX-7                            |114648.12000000002|\n",
            "|1996 Peterbilt 379 Stake Bed with Outrigger|57282.49          |\n",
            "|1972 Alfa Romeo GTA                        |127924.31999999999|\n",
            "+-------------------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344f0404"
      },
      "source": [
        "## Product & order analysis (part 3)\n",
        "\n",
        "### Subtask:\n",
        "Calculate the average order value by joining `orders` and `orderdetails` and performing necessary aggregations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb3ad9c"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the necessary Parquet files, join them, calculate the total value for each order, and then compute the average of these order values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c996b81d",
        "outputId": "1bc35c28-cc72-4ffd-ab06-acd8bc5d9901"
      },
      "source": [
        "from pyspark.sql.functions import sum, avg, col\n",
        "\n",
        "# Read the Parquet files\n",
        "orders_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orders\")\n",
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orderdetails\")\n",
        "\n",
        "# Join the orders and orderdetails DataFrames\n",
        "joined_orders_details_df = orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Calculate the total value for each order\n",
        "order_total_value_df = joined_orders_details_df.withColumn(\"orderLineValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "                                              .groupBy(\"orderNumber\") \\\n",
        "                                              .agg(sum(\"orderLineValue\").alias(\"totalOrderValue\"))\n",
        "\n",
        "# Calculate the average order value\n",
        "average_order_value_df = order_total_value_df.agg(avg(\"totalOrderValue\").alias(\"averageOrderValue\"))\n",
        "\n",
        "# Display the average order value\n",
        "average_order_value_df.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|averageOrderValue|\n",
            "+-----------------+\n",
            "|29460.70739263803|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8fe9d05"
      },
      "source": [
        "## Sales & customer analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze sales and customer data according to the remaining subtasks (which are not fully provided in the prompt).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4979bea"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify and formulate remaining sales and customer analysis objectives and plan the Spark steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5cb1e85",
        "outputId": "315866e3-313d-4722-d713-564f641f7384"
      },
      "source": [
        "# Identify remaining sales and customer analysis tasks:\n",
        "# Based on common retail analysis, remaining tasks related to sales and customers could include:\n",
        "# 1. Total sales amount per customer.\n",
        "# 2. Top N customers by total spending.\n",
        "# 3. Sales distribution by customer location (city, country).\n",
        "# 4. Number of orders per customer.\n",
        "\n",
        "# Formulate specific analysis objectives and outline Spark steps:\n",
        "\n",
        "# Objective 1: Total sales amount per customer.\n",
        "# Steps:\n",
        "# - Join 'customers_df', 'orders_df', and 'orderdetails_df'.\n",
        "# - Calculate the value of each order line (quantityOrdered * priceEach).\n",
        "# - Group by customer and sum the order line values to get total sales per customer.\n",
        "# - Select customer name and total sales.\n",
        "\n",
        "# Objective 2: Top N customers by total spending (e.g., top 10).\n",
        "# Steps:\n",
        "# - Use the result from Objective 1 (total sales per customer).\n",
        "# - Order the customers by total sales in descending order.\n",
        "# - Limit the result to the top N customers.\n",
        "# - Select customer name and total sales.\n",
        "\n",
        "# Objective 3: Sales distribution by customer location (city, country).\n",
        "# Steps:\n",
        "# - Join 'customers_df', 'orders_df', and 'orderdetails_df'.\n",
        "# - Calculate the value of each order line (quantityOrdered * priceEach).\n",
        "# - Group by customer city and country and sum the order line values to get total sales per location.\n",
        "# - Select city, country, and total sales.\n",
        "\n",
        "# Objective 4: Number of orders per customer.\n",
        "# Steps:\n",
        "# - Join 'customers_df' and 'orders_df'.\n",
        "# - Group by customer and count the number of orders.\n",
        "# - Select customer name and order count.\n",
        "\n",
        "# Now, proceed with executing the first analysis objective: Total sales amount per customer.\n",
        "from pyspark.sql.functions import col, sum, count\n",
        "\n",
        "# Load the customers DataFrame\n",
        "customers_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/customers\")\n",
        "\n",
        "# Join customers_df with orders_df\n",
        "customers_orders_df = customers_df.join(orders_df, \"customerNumber\")\n",
        "\n",
        "# Join the result with orderdetails_df\n",
        "customers_orders_details_df = customers_orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Calculate the value of each order line and the total sales per customer\n",
        "customer_total_sales_df = customers_orders_details_df.withColumn(\"orderLineValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "                                                   .groupBy(\"customerName\") \\\n",
        "                                                   .agg(sum(\"orderLineValue\").alias(\"totalSales\"))\n",
        "\n",
        "# Display the total sales per customer\n",
        "customer_total_sales_df.show(truncate=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+------------------+\n",
            "|customerName                     |totalSales        |\n",
            "+---------------------------------+------------------+\n",
            "|Suominen Souveniers              |103896.73999999999|\n",
            "|Amica Models & Co.               |82223.23000000001 |\n",
            "|Collectables For Less Inc.       |73533.65          |\n",
            "|CAF Imports                      |46751.14          |\n",
            "|King Kong Collectables, Co.      |45480.789999999986|\n",
            "|Rovelli Gifts                    |127529.69         |\n",
            "|giftsbymail.co.uk                |71783.75          |\n",
            "|Lyon Souveniers                  |67659.19          |\n",
            "|La Rochelle Gifts                |158573.12         |\n",
            "|Signal Collectibles Ltd.         |42570.37          |\n",
            "|L'ordine Souveniers              |125505.56999999999|\n",
            "|Vitachrome Inc.                  |72497.63999999998 |\n",
            "|Volvo Model Replicas, Co         |66694.82          |\n",
            "|Classic Legends Inc.             |69214.33000000002 |\n",
            "|Daedalus Designs Imports         |61781.700000000004|\n",
            "|Mini Wheels Co.                  |66710.56          |\n",
            "|Signal Gift Stores               |80180.97999999998 |\n",
            "|La Corne D'abondance, Co.        |86553.51999999999 |\n",
            "|Royal Canadian Collectables, Ltd.|66812.0           |\n",
            "|Online Diecast Creations Co.     |116449.29000000001|\n",
            "+---------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40988bce"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the next analysis objective: find the top N customers by total spending.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db171dbd",
        "outputId": "3014ffe5-9d31-4955-8067-5d28a33bc2c4"
      },
      "source": [
        "# Objective 2: Top N customers by total spending (e.g., top 10).\n",
        "# Steps:\n",
        "# - Use the result from Objective 1 (customer_total_sales_df).\n",
        "# - Order the customers by total sales in descending order.\n",
        "# - Limit the result to the top N customers.\n",
        "# - Select customer name and total sales.\n",
        "\n",
        "# Order by total sales in descending order and show top 10\n",
        "top_10_customers_df = customer_total_sales_df.orderBy(customer_total_sales_df[\"totalSales\"].desc()).limit(10)\n",
        "\n",
        "# Display the top 10 customers by total spending\n",
        "print(\"Top 10 Customers by Total Spending:\")\n",
        "top_10_customers_df.show(truncate=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Customers by Total Spending:\n",
            "+----------------------------+------------------+\n",
            "|customerName                |totalSales        |\n",
            "+----------------------------+------------------+\n",
            "|Euro+ Shopping Channel      |820689.5399999995 |\n",
            "|Mini Gifts Distributors Ltd.|591827.34         |\n",
            "|Australian Collectors, Co.  |180585.06999999998|\n",
            "|Muscle Machine Inc          |177913.95000000007|\n",
            "|La Rochelle Gifts           |158573.12         |\n",
            "|Dragon Souveniers, Ltd.     |156251.03         |\n",
            "|Down Under Souveniers, Inc  |154622.08000000002|\n",
            "|Land of Toys Inc.           |149085.15         |\n",
            "|AV Stores, Co.              |148410.09000000003|\n",
            "|The Sharp Gifts Warehouse   |143536.27         |\n",
            "+----------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d09c584"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the next analysis objective: analyze sales distribution by customer location (city, country).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56688ca9",
        "outputId": "fd11ca69-c45c-4cea-ad22-db585bccfdb9"
      },
      "source": [
        "# Objective 3: Sales distribution by customer location (city, country).\n",
        "# Steps:\n",
        "# - Use the joined_orders_details_df from Objective 1.\n",
        "# - Calculate the value of each order line (quantityOrdered * priceEach).\n",
        "# - Group by customer city and country and sum the order line values to get total sales per location.\n",
        "# - Select city, country, and total sales.\n",
        "\n",
        "# Calculate the value of each order line and the total sales per location\n",
        "sales_by_location_df = customers_orders_details_df.withColumn(\"orderLineValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "                                                .groupBy(\"city\", \"country\") \\\n",
        "                                                .agg(sum(\"orderLineValue\").alias(\"totalSales\"))\n",
        "\n",
        "# Display the sales distribution by customer location\n",
        "print(\"Sales Distribution by Customer Location (City, Country):\")\n",
        "sales_by_location_df.show(truncate=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Distribution by Customer Location (City, Country):\n",
            "+-----------------+-----------+------------------+\n",
            "|city             |country    |totalSales        |\n",
            "+-----------------+-----------+------------------+\n",
            "|Allentown        |USA        |104358.68999999999|\n",
            "|Toulouse         |France     |60483.36000000001 |\n",
            "|NYC              |USA        |497941.50000000023|\n",
            "|Århus            |Denmark    |89909.80000000002 |\n",
            "|Helsinki         |Finland    |95546.45999999999 |\n",
            "|Oslo             |Norway     |69059.04000000001 |\n",
            "|Manchester       |UK         |148410.09         |\n",
            "|Central Hong Kong|Hong Kong  |45480.79          |\n",
            "|Boston           |USA        |149882.06000000003|\n",
            "|Los Angeles      |USA        |41506.19          |\n",
            "|Munich           |Germany    |31310.09          |\n",
            "|Sevilla          |Spain      |50987.850000000006|\n",
            "|Pasadena         |USA        |93803.3           |\n",
            "|Nashua           |USA        |116449.29000000002|\n",
            "|Wellington       |New Zealand|90332.37999999999 |\n",
            "|Paris            |France     |240649.68         |\n",
            "|Glen Waverly     |Australia  |55866.020000000004|\n",
            "|Stavern          |Norway     |104224.79000000001|\n",
            "|Auckland         |New Zealand|94431.75999999998 |\n",
            "|Newark           |USA        |81806.54999999999 |\n",
            "+-----------------+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb06035"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the final analysis objective: calculate the number of orders per customer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bc6f383",
        "outputId": "a472ad92-6ed5-4828-ca37-2793859b7a9c"
      },
      "source": [
        "# Objective 4: Number of orders per customer.\n",
        "# Steps:\n",
        "# - Use the customers_orders_df from Objective 1.\n",
        "# - Group by customer and count the number of orders.\n",
        "# - Select customer name and order count.\n",
        "\n",
        "# Group by customer and count the number of orders\n",
        "orders_per_customer_df = customers_orders_df.groupBy(\"customerName\") \\\n",
        "                                           .agg(count(\"orderNumber\").alias(\"numberOfOrders\"))\n",
        "\n",
        "# Display the number of orders per customer\n",
        "print(\"Number of Orders per Customer:\")\n",
        "orders_per_customer_df.show(truncate=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Orders per Customer:\n",
            "+---------------------------------+--------------+\n",
            "|customerName                     |numberOfOrders|\n",
            "+---------------------------------+--------------+\n",
            "|Suominen Souveniers              |3             |\n",
            "|Amica Models & Co.               |2             |\n",
            "|Collectables For Less Inc.       |3             |\n",
            "|CAF Imports                      |2             |\n",
            "|King Kong Collectables, Co.      |2             |\n",
            "|Rovelli Gifts                    |3             |\n",
            "|giftsbymail.co.uk                |2             |\n",
            "|Lyon Souveniers                  |3             |\n",
            "|La Rochelle Gifts                |4             |\n",
            "|Signal Collectibles Ltd.         |2             |\n",
            "|L'ordine Souveniers              |3             |\n",
            "|Vitachrome Inc.                  |3             |\n",
            "|Volvo Model Replicas, Co         |4             |\n",
            "|Classic Legends Inc.             |3             |\n",
            "|Daedalus Designs Imports         |2             |\n",
            "|Mini Wheels Co.                  |3             |\n",
            "|Signal Gift Stores               |3             |\n",
            "|La Corne D'abondance, Co.        |3             |\n",
            "|Royal Canadian Collectables, Ltd.|2             |\n",
            "|Online Diecast Creations Co.     |3             |\n",
            "+---------------------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66f42a89"
      },
      "source": [
        "## Employee performance analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze employee performance as per the instructions (which are not fully provided in the prompt).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4aed85"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify potential employee performance metrics, formulate a specific analysis objective, outline the necessary Spark steps, load the necessary Parquet files, perform the Spark operations to calculate the chosen metric, and display the results. A relevant metric available in the dataset is the total sales generated by each sales representative. The objective is to calculate the total sales amount for each sales representative. This involves joining the customers, orders, and orderdetails tables, calculating the value of each order line, grouping by the sales representative, and summing the order line values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed5e4165",
        "outputId": "1079b69d-1e47-4b3f-85df-2e267c085b08"
      },
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Objective: Calculate the total sales amount for each sales representative.\n",
        "# Steps:\n",
        "# - Load the 'employees', 'customers', 'orders', and 'orderdetails' Parquet files into DataFrames.\n",
        "# - Join 'employees_df' with 'customers_df' on 'employeeNumber' (assuming salesRepEmployeeNumber in customers maps to employeeNumber in employees).\n",
        "# - Join the result with 'orders_df' on 'customerNumber'.\n",
        "# - Join the result with 'orderdetails_df' on 'orderNumber'.\n",
        "# - Calculate the value of each order line (quantityOrdered * priceEach).\n",
        "# - Group by sales representative (employee name) and sum the order line values to get total sales per representative.\n",
        "# - Select employee name and total sales.\n",
        "\n",
        "# Load the necessary DataFrames\n",
        "employees_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/employees\")\n",
        "customers_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/customers\")\n",
        "orders_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orders\")\n",
        "orderdetails_df = spark.read.parquet(\"/content/drive/MyDrive/4th_aug_2025/out_put/orderdetails\")\n",
        "\n",
        "# Join employees_df with customers_df\n",
        "# Assuming salesRepEmployeeNumber in customers corresponds to employeeNumber in employees\n",
        "employees_customers_df = employees_df.join(customers_df, employees_df.employeeNumber == customers_df.salesRepEmployeeNumber)\n",
        "\n",
        "# Join the result with orders_df\n",
        "employees_customers_orders_df = employees_customers_df.join(orders_df, \"customerNumber\")\n",
        "\n",
        "# Join the result with orderdetails_df\n",
        "employees_customers_orders_details_df = employees_customers_orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Calculate the value of each order line and the total sales per sales representative\n",
        "employee_sales_performance_df = employees_customers_orders_details_df.withColumn(\"orderLineValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "                                                                    .groupBy(\"firstName\", \"lastName\") \\\n",
        "                                                                    .agg(sum(\"orderLineValue\").alias(\"totalSales\"))\n",
        "\n",
        "# Display the employee sales performance\n",
        "print(\"Employee Sales Performance (Total Sales by Sales Representative):\")\n",
        "employee_sales_performance_df.show(truncate=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee Sales Performance (Total Sales by Sales Representative):\n",
            "+---------+--------+----------+\n",
            "|firstName|lastName|totalSales|\n",
            "+---------+--------+----------+\n",
            "+---------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0d5d36"
      },
      "source": [
        "**Reasoning**:\n",
        "The employee sales performance calculation resulted in an empty DataFrame, which indicates there might be an issue with the joins or the data itself. Before assuming a data issue, it's important to verify the intermediate joined DataFrames to understand where the records are being lost. Checking the counts of the DataFrames after each join will help pinpoint the problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efae36b1",
        "outputId": "f5ae1b1c-2d3a-4add-e6a0-5d8484689d9e"
      },
      "source": [
        "# Verify the counts of the DataFrames at each step of the join\n",
        "print(f\"Count of employees_df: {employees_df.count()}\")\n",
        "print(f\"Count of customers_df: {customers_df.count()}\")\n",
        "print(f\"Count of orders_df: {orders_df.count()}\")\n",
        "print(f\"Count of orderdetails_df: {orderdetails_df.count()}\")\n",
        "\n",
        "# Check the count after the first join (employees and customers)\n",
        "print(f\"Count of employees_customers_df: {employees_customers_df.count()}\")\n",
        "\n",
        "# Check the count after the second join (employees_customers and orders)\n",
        "print(f\"Count of employees_customers_orders_df: {employees_customers_orders_df.count()}\")\n",
        "\n",
        "# Check the count after the third join (employees_customers_orders and orderdetails)\n",
        "print(f\"Count of employees_customers_orders_details_df: {employees_customers_orders_details_df.count()}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of employees_df: 23\n",
            "Count of customers_df: 122\n",
            "Count of orders_df: 326\n",
            "Count of orderdetails_df: 2996\n",
            "Count of employees_customers_df: 0\n",
            "Count of employees_customers_orders_df: 0\n",
            "Count of employees_customers_orders_details_df: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e492bda6"
      },
      "source": [
        "**Reasoning**:\n",
        "The counts of the DataFrames show that the join between `employees_df` and `customers_df` results in an empty DataFrame (`employees_customers_df`). This indicates that there are no matching `salesRepEmployeeNumber` values in the `customers_df` that correspond to `employeeNumber` values in the `employees_df`. To address this, I will perform a left outer join from `customers_df` to `employees_df` based on the `salesRepEmployeeNumber` and `employeeNumber` columns to include all customers and their associated sales representatives, even if a sales representative is not found in the employees table (though this is unlikely to be the issue given the zero count). A more probable issue is a mismatch in the join keys or data types, or that the sales representatives in the customer table do not have corresponding entries in the employee table with the same employee number. I will also inspect the schema and a few rows of both `employees_df` and `customers_df` to check the join columns (`employeeNumber` and `salesRepEmployeeNumber`) for potential issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6740cc7",
        "outputId": "eb44cd3a-bcb0-4692-fa2e-57faf570781d"
      },
      "source": [
        "# Inspect the schema and a few rows of employees_df and customers_df\n",
        "print(\"Employees DataFrame Schema:\")\n",
        "employees_df.printSchema()\n",
        "print(\"Employees DataFrame Head:\")\n",
        "employees_df.show(5)\n",
        "\n",
        "print(\"Customers DataFrame Schema:\")\n",
        "customers_df.printSchema()\n",
        "print(\"Customers DataFrame Head:\")\n",
        "customers_df.show(5)\n",
        "\n",
        "# Attempt a left outer join to see if any customers have a salesRepEmployeeNumber that exists in employees\n",
        "employees_customers_left_join_df = customers_df.join(employees_df, customers_df.salesRepEmployeeNumber == employees_df.employeeNumber, \"left_outer\")\n",
        "\n",
        "print(f\"Count after left outer join (customers left): {employees_customers_left_join_df.count()}\")\n",
        "employees_customers_left_join_df.show(5)\n",
        "\n",
        "# Perform the join again, but use a left outer join from employees to customers\n",
        "# This will show if any employees are sales reps linked to customers\n",
        "employees_customers_left_join_employees_df = employees_df.join(customers_df, employees_df.employeeNumber == customers_df.salesRepEmployeeNumber, \"left_outer\")\n",
        "\n",
        "print(f\"Count after left outer join (employees left): {employees_customers_left_join_employees_df.count()}\")\n",
        "employees_customers_left_join_employees_df.show(5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees DataFrame Schema:\n",
            "root\n",
            " |-- employeeNumber: integer (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- firstName: string (nullable = true)\n",
            " |-- extension: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- officeCode: string (nullable = true)\n",
            " |-- reportsTo: integer (nullable = true)\n",
            " |-- jobTitle: string (nullable = true)\n",
            "\n",
            "Employees DataFrame Head:\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "|employeeNumber| lastName|firstName|extension|               email|officeCode|reportsTo|            jobTitle|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "|          1002|   Murphy|    Diane|    x5800|dmurphy@classicmo...|         1|     null|           President|\n",
            "|          1056|Patterson|     Mary|    x4611|mpatterso@classic...|         1|     null|            VP Sales|\n",
            "|          1076| Firrelli|     Jeff|    x9273|jfirrelli@classic...|         1|     null|        VP Marketing|\n",
            "|          1088|Patterson|  William|    x4871|wpatterson@classi...|         6|     null|Sales Manager (APAC)|\n",
            "|          1102|   Bondur|   Gerard|    x5408|gbondur@classicmo...|         4|     null| Sale Manager (EMEA)|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Customers DataFrame Schema:\n",
            "root\n",
            " |-- customerNumber: integer (nullable = true)\n",
            " |-- customerName: string (nullable = true)\n",
            " |-- contactLastName: string (nullable = true)\n",
            " |-- contactFirstName: string (nullable = true)\n",
            " |-- phone: string (nullable = true)\n",
            " |-- addressLine1: string (nullable = true)\n",
            " |-- addressLine2: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- postalCode: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- salesRepEmployeeNumber: integer (nullable = true)\n",
            " |-- creditLimit: double (nullable = true)\n",
            "\n",
            "Customers DataFrame Head:\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "|customerNumber|        customerName|contactLastName|contactFirstName|       phone|        addressLine1|addressLine2|     city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        null|   Nantes|    null|     44000|   France|                  null|    21000.0|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        null|Las Vegas|      NV|     83030|      USA|                  null|    71800.0|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3|Melbourne|Victoria|      3004|Australia|                  null|   117300.0|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        null|   Nantes|    null|     44000|   France|                  null|   118200.0|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|  Stavern|    null|      4110|   Norway|                  null|    81700.0|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Count after left outer join (customers left): 122\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+--------------+--------+---------+---------+-----+----------+---------+--------+\n",
            "|customerNumber|        customerName|contactLastName|contactFirstName|       phone|        addressLine1|addressLine2|     city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|employeeNumber|lastName|firstName|extension|email|officeCode|reportsTo|jobTitle|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+--------------+--------+---------+---------+-----+----------+---------+--------+\n",
            "|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        null|   Nantes|    null|     44000|   France|                  null|    21000.0|          null|    null|     null|     null| null|      null|     null|    null|\n",
            "|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        null|Las Vegas|      NV|     83030|      USA|                  null|    71800.0|          null|    null|     null|     null| null|      null|     null|    null|\n",
            "|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3|Melbourne|Victoria|      3004|Australia|                  null|   117300.0|          null|    null|     null|     null| null|      null|     null|    null|\n",
            "|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        null|   Nantes|    null|     44000|   France|                  null|   118200.0|          null|    null|     null|     null| null|      null|     null|    null|\n",
            "|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        null|  Stavern|    null|      4110|   Norway|                  null|    81700.0|          null|    null|     null|     null| null|      null|     null|    null|\n",
            "+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+--------------+--------+---------+---------+-----+----------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Count after left outer join (employees left): 23\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+--------------+------------+---------------+----------------+-----+------------+------------+----+-----+----------+-------+----------------------+-----------+\n",
            "|employeeNumber| lastName|firstName|extension|               email|officeCode|reportsTo|            jobTitle|customerNumber|customerName|contactLastName|contactFirstName|phone|addressLine1|addressLine2|city|state|postalCode|country|salesRepEmployeeNumber|creditLimit|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+--------------+------------+---------------+----------------+-----+------------+------------+----+-----+----------+-------+----------------------+-----------+\n",
            "|          1002|   Murphy|    Diane|    x5800|dmurphy@classicmo...|         1|     null|           President|          null|        null|           null|            null| null|        null|        null|null| null|      null|   null|                  null|       null|\n",
            "|          1056|Patterson|     Mary|    x4611|mpatterso@classic...|         1|     null|            VP Sales|          null|        null|           null|            null| null|        null|        null|null| null|      null|   null|                  null|       null|\n",
            "|          1076| Firrelli|     Jeff|    x9273|jfirrelli@classic...|         1|     null|        VP Marketing|          null|        null|           null|            null| null|        null|        null|null| null|      null|   null|                  null|       null|\n",
            "|          1088|Patterson|  William|    x4871|wpatterson@classi...|         6|     null|Sales Manager (APAC)|          null|        null|           null|            null| null|        null|        null|null| null|      null|   null|                  null|       null|\n",
            "|          1102|   Bondur|   Gerard|    x5408|gbondur@classicmo...|         4|     null| Sale Manager (EMEA)|          null|        null|           null|            null| null|        null|        null|null| null|      null|   null|                  null|       null|\n",
            "+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+--------------+------------+---------------+----------------+-----+------------+------------+----+-----+----------+-------+----------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c4bf0e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output shows that the `salesRepEmployeeNumber` column in the `customers_df` contains `null` values for the first few rows displayed, and the left outer joins confirm that there are no matches between `customers_df.salesRepEmployeeNumber` and `employees_df.employeeNumber`. This is the reason for the empty result in the employee performance calculation. The data suggests that the `salesRepEmployeeNumber` in the customers table is not consistently populated or linked to the `employeeNumber` in the employees table for all customers. However, some customers *do* have a sales representative, as indicated by the schema allowing non-null values. To proceed with the analysis of sales representatives who *are* linked to customers, I will filter the `customers_df` to include only rows where `salesRepEmployeeNumber` is not null before joining with the `employees_df`. Then, I will perform the inner joins with `orders_df` and `orderdetails_df` and calculate the total sales per sales representative as originally intended, but only for those sales representatives who are linked to customers with non-null `salesRepEmployeeNumber`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82de231b",
        "outputId": "96cdc15e-c968-4fee-de79-fa6f5c6c654f"
      },
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Filter customers_df to include only customers with a sales representative\n",
        "customers_with_salesrep_df = customers_df.filter(col(\"salesRepEmployeeNumber\").isNotNull())\n",
        "\n",
        "# Join employees_df with the filtered customers_df\n",
        "employees_customers_filtered_df = employees_df.join(customers_with_salesrep_df, employees_df.employeeNumber == customers_with_salesrep_df.salesRepEmployeeNumber)\n",
        "\n",
        "# Join the result with orders_df\n",
        "employees_customers_orders_df = employees_customers_filtered_df.join(orders_df, \"customerNumber\")\n",
        "\n",
        "# Join the result with orderdetails_df\n",
        "employees_customers_orders_details_df = employees_customers_orders_df.join(orderdetails_df, \"orderNumber\")\n",
        "\n",
        "# Calculate the value of each order line and the total sales per sales representative\n",
        "employee_sales_performance_df = employees_customers_orders_details_df.withColumn(\"orderLineValue\", col(\"quantityOrdered\") * col(\"priceEach\")) \\\n",
        "                                                                    .groupBy(\"firstName\", \"lastName\") \\\n",
        "                                                                    .agg(sum(\"orderLineValue\").alias(\"totalSales\"))\n",
        "\n",
        "# Display the employee sales performance\n",
        "print(\"Employee Sales Performance (Total Sales by Sales Representative - Filtered):\")\n",
        "employee_sales_performance_df.show(truncate=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee Sales Performance (Total Sales by Sales Representative - Filtered):\n",
            "+---------+--------+----------+\n",
            "|firstName|lastName|totalSales|\n",
            "+---------+--------+----------+\n",
            "+---------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "053656de"
      },
      "source": [
        "**Reasoning**:\n",
        "The employee sales performance calculation still resulted in an empty DataFrame even after filtering for customers with a sales representative number. This strongly suggests that either there are no customers with a non-null `salesRepEmployeeNumber`, or none of the `salesRepEmployeeNumber` values in the customers table match any `employeeNumber` in the employees table. To confirm this, I will check the count of the filtered customers DataFrame and then check if there are any matching employee numbers in the employees table for the `salesRepEmployeeNumber` values present in the filtered customers DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b84830b4",
        "outputId": "696321c3-7448-49ac-a012-2392d47c83fb"
      },
      "source": [
        "# Check the count of customers with a sales representative\n",
        "print(f\"Count of customers with salesRepEmployeeNumber: {customers_with_salesrep_df.count()}\")\n",
        "\n",
        "# Get the distinct salesRepEmployeeNumbers from the filtered customers DataFrame\n",
        "sales_rep_employee_numbers = customers_with_salesrep_df.select(\"salesRepEmployeeNumber\").distinct()\n",
        "print(\"Distinct salesRepEmployeeNumbers in customers_with_salesrep_df:\")\n",
        "sales_rep_employee_numbers.show(truncate=False)\n",
        "\n",
        "# Check if these salesRepEmployeeNumbers exist in the employees DataFrame's employeeNumber column\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "matching_employees_df = employees_df.join(sales_rep_employee_numbers, employees_df.employeeNumber == sales_rep_employee_numbers.salesRepEmployeeNumber)\n",
        "\n",
        "print(f\"Count of employees matching salesRepEmployeeNumber in customers: {matching_employees_df.count()}\")\n",
        "print(\"Employees matching salesRepEmployeeNumber in customers:\")\n",
        "matching_employees_df.show(truncate=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of customers with salesRepEmployeeNumber: 0\n",
            "Distinct salesRepEmployeeNumbers in customers_with_salesrep_df:\n",
            "+----------------------+\n",
            "|salesRepEmployeeNumber|\n",
            "+----------------------+\n",
            "+----------------------+\n",
            "\n",
            "Count of employees matching salesRepEmployeeNumber in customers: 0\n",
            "Employees matching salesRepEmployeeNumber in customers:\n",
            "+--------------+--------+---------+---------+-----+----------+---------+--------+----------------------+\n",
            "|employeeNumber|lastName|firstName|extension|email|officeCode|reportsTo|jobTitle|salesRepEmployeeNumber|\n",
            "+--------------+--------+---------+---------+-----+----------+---------+--------+----------------------+\n",
            "+--------------+--------+---------+---------+-----+----------+---------+--------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ca648c9"
      },
      "source": [
        "## Final output\n",
        "\n",
        "### Subtask:\n",
        "Present the results of the analyses in a clear and understandable format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dffb3b0"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the key findings and DataFrames for the \"Product & order analysis (part 1)\" subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76a64c9b",
        "outputId": "b83189fc-1587-4340-dbff-35b5649b9202"
      },
      "source": [
        "print(\"--- Product & Order Analysis: Top 10 Products by Quantity Sold ---\")\n",
        "print(\"Key Finding: The top 10 products by quantity sold have been successfully identified.\")\n",
        "print(\"DataFrame: top_10_products_df\")\n",
        "top_10_products_df.show(truncate=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Product & Order Analysis: Top 10 Products by Quantity Sold ---\n",
            "Key Finding: The top 10 products by quantity sold have been successfully identified.\n",
            "DataFrame: top_10_products_df\n",
            "+---------------------------------------+-----------------+\n",
            "|productName                            |totalQuantitySold|\n",
            "+---------------------------------------+-----------------+\n",
            "|1992 Ferrari 360 Spider red            |1808             |\n",
            "|1937 Lincoln Berline                   |1111             |\n",
            "|American Airlines: MD-11S              |1085             |\n",
            "|1941 Chevrolet Special Deluxe Cabriolet|1076             |\n",
            "|1930 Buick Marquette Phaeton           |1074             |\n",
            "|1940s Ford truck                       |1061             |\n",
            "|1969 Harley Davidson Ultimate Chopper  |1057             |\n",
            "|1957 Chevy Pickup                      |1056             |\n",
            "|1964 Mercedes Tour Bus                 |1053             |\n",
            "|1956 Porsche 356A Coupe                |1052             |\n",
            "+---------------------------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1636dbaf"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the key findings and DataFrame for the \"Product & order analysis (part 2)\" subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4a32fc7",
        "outputId": "7e8d7f6e-afbc-4bfc-c30f-088ee2fbbb9c"
      },
      "source": [
        "print(\"\\n--- Product & Order Analysis: Product-wise Revenue ---\")\n",
        "print(\"Key Finding: The total revenue for each product has been calculated.\")\n",
        "print(\"DataFrame: total_product_revenue_df\")\n",
        "total_product_revenue_df.show(truncate=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Product & Order Analysis: Product-wise Revenue ---\n",
            "Key Finding: The total revenue for each product has been calculated.\n",
            "DataFrame: total_product_revenue_df\n",
            "+-------------------------------------------+------------------+\n",
            "|productName                                |totalRevenue      |\n",
            "+-------------------------------------------+------------------+\n",
            "|1996 Moto Guzzi 1100i                      |109998.81999999998|\n",
            "|1936 Chrysler Airflow                      |88434.45999999999 |\n",
            "|18th Century Vintage Horse Carriage        |85328.56999999998 |\n",
            "|The Titanic                                |84992.24999999999 |\n",
            "|1958 Setra Bus                             |119085.24999999999|\n",
            "|Diamond T620 Semi-Skirted Tanker           |101137.54999999999|\n",
            "|2001 Ferrari Enzo                          |190755.86         |\n",
            "|The Queen Mary                             |78919.05999999998 |\n",
            "|1930 Buick Marquette Phaeton               |41599.24          |\n",
            "|The Mayflower                              |69531.61          |\n",
            "|1958 Chevy Corvette Limited Edition        |31627.960000000003|\n",
            "|1966 Shelby Cobra 427 S/C                  |42015.53999999999 |\n",
            "|1968 Dodge Charger                         |98718.76000000001 |\n",
            "|1948 Porsche 356-A Roadster                |66455.62          |\n",
            "|The USS Constitution Ship                  |66697.13          |\n",
            "|1982 Camaro Z28                            |89272.65000000001 |\n",
            "|1936 Mercedes Benz 500k Roadster           |29763.39          |\n",
            "|1993 Mazda RX-7                            |114648.12000000002|\n",
            "|1996 Peterbilt 379 Stake Bed with Outrigger|57282.49          |\n",
            "|1972 Alfa Romeo GTA                        |127924.31999999999|\n",
            "+-------------------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d68c7db"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the key findings and DataFrame for the \"Product & order analysis (part 3)\" subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72b0063c",
        "outputId": "30113a44-a6b0-428f-a5aa-31db318a3304"
      },
      "source": [
        "print(\"\\n--- Product & Order Analysis: Average Order Value ---\")\n",
        "print(\"Key Finding: The average order value has been calculated.\")\n",
        "print(\"DataFrame: average_order_value_df\")\n",
        "average_order_value_df.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Product & Order Analysis: Average Order Value ---\n",
            "Key Finding: The average order value has been calculated.\n",
            "DataFrame: average_order_value_df\n",
            "+-----------------+\n",
            "|averageOrderValue|\n",
            "+-----------------+\n",
            "|29460.70739263803|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403983b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Present the key findings and DataFrames for the \"Sales & customer analysis\" subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44f3800",
        "outputId": "50bcf215-3d3a-45d7-d150-6d688a7cd8c6"
      },
      "source": [
        "print(\"\\n--- Sales & Customer Analysis ---\")\n",
        "print(\"Key Findings:\")\n",
        "print(\"- Total sales per customer have been calculated.\")\n",
        "print(\"- Top 10 customers by total spending have been identified.\")\n",
        "print(\"- Sales distribution by customer location (city, country) has been analyzed.\")\n",
        "print(\"- The number of orders per customer has been calculated.\")\n",
        "\n",
        "print(\"\\nDataFrame: customer_total_sales_df (Total Sales per Customer)\")\n",
        "customer_total_sales_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nDataFrame: top_10_customers_df (Top 10 Customers by Spending)\")\n",
        "top_10_customers_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nDataFrame: sales_by_location_df (Sales by Location)\")\n",
        "sales_by_location_df.show(truncate=False)\n",
        "\n",
        "print(\"\\nDataFrame: orders_per_customer_df (Orders per Customer)\")\n",
        "orders_per_customer_df.show(truncate=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sales & Customer Analysis ---\n",
            "Key Findings:\n",
            "- Total sales per customer have been calculated.\n",
            "- Top 10 customers by total spending have been identified.\n",
            "- Sales distribution by customer location (city, country) has been analyzed.\n",
            "- The number of orders per customer has been calculated.\n",
            "\n",
            "DataFrame: customer_total_sales_df (Total Sales per Customer)\n",
            "+---------------------------------+------------------+\n",
            "|customerName                     |totalSales        |\n",
            "+---------------------------------+------------------+\n",
            "|Suominen Souveniers              |103896.73999999999|\n",
            "|Amica Models & Co.               |82223.23000000001 |\n",
            "|Collectables For Less Inc.       |73533.65          |\n",
            "|CAF Imports                      |46751.14          |\n",
            "|King Kong Collectables, Co.      |45480.789999999986|\n",
            "|Rovelli Gifts                    |127529.69         |\n",
            "|giftsbymail.co.uk                |71783.75          |\n",
            "|Lyon Souveniers                  |67659.19          |\n",
            "|La Rochelle Gifts                |158573.12         |\n",
            "|Signal Collectibles Ltd.         |42570.37          |\n",
            "|L'ordine Souveniers              |125505.56999999999|\n",
            "|Vitachrome Inc.                  |72497.63999999998 |\n",
            "|Volvo Model Replicas, Co         |66694.82          |\n",
            "|Classic Legends Inc.             |69214.33000000002 |\n",
            "|Daedalus Designs Imports         |61781.700000000004|\n",
            "|Mini Wheels Co.                  |66710.56          |\n",
            "|Signal Gift Stores               |80180.97999999998 |\n",
            "|La Corne D'abondance, Co.        |86553.51999999999 |\n",
            "|Royal Canadian Collectables, Ltd.|66812.0           |\n",
            "|Online Diecast Creations Co.     |116449.29000000001|\n",
            "+---------------------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "DataFrame: top_10_customers_df (Top 10 Customers by Spending)\n",
            "+----------------------------+------------------+\n",
            "|customerName                |totalSales        |\n",
            "+----------------------------+------------------+\n",
            "|Euro+ Shopping Channel      |820689.5399999995 |\n",
            "|Mini Gifts Distributors Ltd.|591827.34         |\n",
            "|Australian Collectors, Co.  |180585.06999999998|\n",
            "|Muscle Machine Inc          |177913.95000000007|\n",
            "|La Rochelle Gifts           |158573.12         |\n",
            "|Dragon Souveniers, Ltd.     |156251.03         |\n",
            "|Down Under Souveniers, Inc  |154622.08000000002|\n",
            "|Land of Toys Inc.           |149085.15         |\n",
            "|AV Stores, Co.              |148410.09000000003|\n",
            "|The Sharp Gifts Warehouse   |143536.27         |\n",
            "+----------------------------+------------------+\n",
            "\n",
            "\n",
            "DataFrame: sales_by_location_df (Sales by Location)\n",
            "+-----------------+-----------+------------------+\n",
            "|city             |country    |totalSales        |\n",
            "+-----------------+-----------+------------------+\n",
            "|Allentown        |USA        |104358.68999999999|\n",
            "|Toulouse         |France     |60483.36000000001 |\n",
            "|NYC              |USA        |497941.50000000023|\n",
            "|Århus            |Denmark    |89909.80000000002 |\n",
            "|Helsinki         |Finland    |95546.45999999999 |\n",
            "|Oslo             |Norway     |69059.04000000001 |\n",
            "|Manchester       |UK         |148410.09         |\n",
            "|Central Hong Kong|Hong Kong  |45480.79          |\n",
            "|Boston           |USA        |149882.06000000003|\n",
            "|Los Angeles      |USA        |41506.19          |\n",
            "|Munich           |Germany    |31310.09          |\n",
            "|Sevilla          |Spain      |50987.850000000006|\n",
            "|Pasadena         |USA        |93803.3           |\n",
            "|Nashua           |USA        |116449.29000000002|\n",
            "|Wellington       |New Zealand|90332.37999999999 |\n",
            "|Paris            |France     |240649.68         |\n",
            "|Glen Waverly     |Australia  |55866.020000000004|\n",
            "|Stavern          |Norway     |104224.79000000001|\n",
            "|Auckland         |New Zealand|94431.75999999998 |\n",
            "|Newark           |USA        |81806.54999999999 |\n",
            "+-----------------+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "DataFrame: orders_per_customer_df (Orders per Customer)\n",
            "+---------------------------------+--------------+\n",
            "|customerName                     |numberOfOrders|\n",
            "+---------------------------------+--------------+\n",
            "|Suominen Souveniers              |3             |\n",
            "|Amica Models & Co.               |2             |\n",
            "|Collectables For Less Inc.       |3             |\n",
            "|CAF Imports                      |2             |\n",
            "|King Kong Collectables, Co.      |2             |\n",
            "|Rovelli Gifts                    |3             |\n",
            "|giftsbymail.co.uk                |2             |\n",
            "|Lyon Souveniers                  |3             |\n",
            "|La Rochelle Gifts                |4             |\n",
            "|Signal Collectibles Ltd.         |2             |\n",
            "|L'ordine Souveniers              |3             |\n",
            "|Vitachrome Inc.                  |3             |\n",
            "|Volvo Model Replicas, Co         |4             |\n",
            "|Classic Legends Inc.             |3             |\n",
            "|Daedalus Designs Imports         |2             |\n",
            "|Mini Wheels Co.                  |3             |\n",
            "|Signal Gift Stores               |3             |\n",
            "|La Corne D'abondance, Co.        |3             |\n",
            "|Royal Canadian Collectables, Ltd.|2             |\n",
            "|Online Diecast Creations Co.     |3             |\n",
            "+---------------------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37fbf1ea"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The data ingestion and setup phase initially faced challenges in locating the input CSV files, leading to retries. The correct input path was identified as `/content/drive/MyDrive/4th_aug_2025/csv/`, and the data was successfully converted to Parquet format and stored in `/content/drive/MyDrive/4th_aug_2025/out_put/`.\n",
        "*   The analysis of the top 10 products by quantity sold was successfully completed after resolving the data path issues. The top 10 products were identified and displayed.\n",
        "*   Product-wise revenue was calculated by joining the `orders`, `orderdetails`, and `products` tables. The total revenue for each product was successfully computed and displayed.\n",
        "*   The average order value was calculated by joining the `orders` and `orderdetails` tables and aggregating the total value of each order. The average order value was found to be approximately \\$29,460.71.\n",
        "*   Sales and customer analysis revealed total sales per customer, identified the top 10 customers by spending, analyzed sales distribution by location (city and country), and calculated the number of orders per customer. These analyses were successfully completed and their results displayed.\n",
        "*   Employee performance analysis, specifically calculating sales per sales representative based on the `salesRepEmployeeNumber` column in the `customers` table, could not be completed. Investigation revealed that there were no valid `salesRepEmployeeNumber` values in the `customers` table that matched `employeeNumber` values in the `employees` table, preventing the necessary data linkage.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the data quality issue regarding the `salesRepEmployeeNumber` column in the `customers` table. This column seems to lack valid entries that link customers to employees, preventing meaningful sales performance analysis by sales representative.\n",
        "*   Explore alternative ways to analyze employee performance if the `salesRepEmployeeNumber` linkage cannot be fixed, such as analyzing sales by office location or job title, assuming those relationships are present and accurate in the dataset.\n"
      ]
    }
  ]
}